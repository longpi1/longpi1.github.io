<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>cni插件之flannel的host-gw模式与calico</title>
    <link href="/2022/10/29/cni%E6%8F%92%E4%BB%B6%E4%B9%8Bflannel%E7%9A%84host-gw%E6%A8%A1%E5%BC%8F%E4%B8%8Ecalico/"/>
    <url>/2022/10/29/cni%E6%8F%92%E4%BB%B6%E4%B9%8Bflannel%E7%9A%84host-gw%E6%A8%A1%E5%BC%8F%E4%B8%8Ecalico/</url>
    
    <content type="html"><![CDATA[<h1 id="cni插件之flannel的host-gw模式与calico"><a href="#cni插件之flannel的host-gw模式与calico" class="headerlink" title="cni插件之flannel的host-gw模式与calico"></a>cni插件之flannel的host-gw模式与calico</h1><blockquote><p>本文笔记来自：「深入剖析 Kubernetes课程」，原文链接：<a href="https://time.geekbang.org/column/article/64948">https://time.geekbang.org/column/article/64948</a></p></blockquote><p>先来看一下Flannel的host-gw模式。</p><p><img src="https://static001.geekbang.org/resource/image/3d/25/3d8b08411eeb49be2658eb4352206d25.png?wh=2880*1528" alt="img"></p><p>假设现在，Node 1上的Infra-container-1，要访问Node 2上的Infra-container-2。</p><p>当设置Flannel使用host-gw模式之后，flanneld会在宿主机上创建这样一条规则，以Node 1为例：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs accesslog">$ ip route<br>...<br><span class="hljs-number">10.244.1.0</span>/<span class="hljs-number">24</span> via <span class="hljs-number">10</span>.<span class="hljs-number">168</span>.<span class="hljs-number">0</span>.<span class="hljs-number">3</span> dev eth0<br><br></code></pre></td></tr></table></figure><p>这条路由规则的含义是：目的IP地址属于10.244.1.0&#x2F;24网段的IP包，应该经过本机的eth0设备发出去（即：dev eth0）；并且，它下一跳地址（next-hop）是10.168.0.3（即：via 10.168.0.3）。</p><p>所谓下一跳地址就是：如果IP包从主机A发到主机B，需要经过路由设备X的中转。那么X的IP地址就应该配置为主机A的下一跳地址。</p><p>而从host-gw示意图中我们可以看到，这个下一跳地址对应的，正是我们的目的宿主机Node 2。</p><p>一旦配置了下一跳地址，那么接下来，当IP包从网络层进入链路层封装成帧的时候，eth0设备就会使用下一跳地址对应的MAC地址，作为该数据帧的目的MAC地址。显然，这个MAC地址，正是Node 2的MAC地址。</p><p>这样，这个数据帧就会从Node 1通过宿主机的二层网络顺利到达Node 2上。</p><p>而Node 2的内核网络栈从二层数据帧里拿到IP包后，会“看到”这个IP包的目的IP地址是10.244.1.3，即Infra-container-2的IP地址。这时候，根据Node 2上的路由表，该目的地址会匹配到第二条路由规则（也就是10.244.1.0对应的路由规则），从而进入cni0网桥，进而进入到Infra-container-2当中。</p><p>可以看到， <strong>host-gw模式的工作原理，其实就是将每个Flannel子网（Flannel Subnet，比如：10.244.1.0&#x2F;24）的“下一跳”，设置成了该子网对应的宿主机的IP地址。</strong></p><p>也就是说，这台“主机”（Host）会充当这条容器通信路径里的“网关”（Gateway）。这也正是“host-gw”的含义。</p><p>当然，Flannel子网和主机的信息，都是保存在Etcd当中的。flanneld只需要WACTH这些数据的变化，然后实时更新路由表即可。</p><blockquote><p>注意：在Kubernetes v1.7之后，类似Flannel、Calico的CNI网络插件都是可以直接连接Kubernetes的APIServer来访问Etcd的，无需额外部署Etcd给它们使用。</p></blockquote><p>而在这种模式下，容器通信的过程就免除了额外的封包和解包带来的性能损耗。根据实际的测试，host-gw的性能损失大约在10%左右，而其他所有基于VXLAN“隧道”机制的网络方案，性能损失都在20%~30%左右。</p><p>当然，通过上面的叙述，你也应该看到，host-gw模式能够正常工作的核心，就在于IP包在封装成帧发送出去的时候，会使用路由表里的“下一跳”来设置目的MAC地址。这样，它就会经过二层网络到达目的宿主机。</p><p><strong>所以说，Flannel host-gw模式必须要求集群宿主机之间是二层连通的。</strong></p><p>需要注意的是，宿主机之间二层不连通的情况也是广泛存在的。比如，宿主机分布在了不同的子网（VLAN）里。但是，在一个Kubernetes集群里，宿主机之间必须可以通过IP地址进行通信，也就是说至少是三层可达的。否则的话，你的集群将不满足宿主机之间IP互通的假设（Kubernetes网络模型）。当然，“三层可达”也可以通过为几个子网设置三层转发来实现。</p><p>而在容器生态中，要说到像Flannel host-gw这样的三层网络方案，我们就不得不提到这个领域里的“龙头老大”Calico项目了。</p><p>实际上，Calico项目提供的网络解决方案，与Flannel的host-gw模式，几乎是完全一样的。也就是说，Calico也会在每台宿主机上，添加一个格式如下所示的路由规则：</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs armasm">&lt;目的容器<span class="hljs-built_in">IP</span>地址段&gt; via &lt;网关的<span class="hljs-built_in">IP</span>地址&gt; dev eth0<br><br></code></pre></td></tr></table></figure><p>其中，网关的IP地址，正是目的容器所在宿主机的IP地址。</p><p>而正如前所述，这个三层网络方案得以正常工作的核心，是为每个容器的IP地址，找到它所对应的、“下一跳”的 <strong>网关</strong>。</p><p>不过， <strong>不同于Flannel通过Etcd和宿主机上的flanneld来维护路由信息的做法，Calico项目使用了一个“重型武器”来自动地在整个集群中分发路由信息。</strong></p><p>这个“重型武器”，就是BGP。</p><p><strong>BGP的全称是Border Gateway Protocol，即：边界网关协议</strong>。它是一个Linux内核原生就支持的、专门用在大规模数据中心里维护不同的“自治系统”之间路由信息的、无中心的路由协议。</p><p>这个概念可能听起来有点儿“吓人”，但实际上，可以用一个非常简单的例子来为你讲清楚。</p><p><img src="https://static001.geekbang.org/resource/image/2e/9b/2e4b3bee1d924f4ae25e2c1fd115379b.jpg?wh=1738*893" alt="img"></p><p>在这个图中，有两个自治系统（Autonomous System，简称为AS）：AS 1和AS 2。而所谓的一个自治系统，指的是一个组织管辖下的所有IP网络和路由器的全体。可以把它想象成一个小公司里的所有主机和路由器。在正常情况下，自治系统之间不会有任何“来往”。</p><p>但是，如果这样两个自治系统里的主机，要通过IP地址直接进行通信，我们就必须使用路由器把这两个自治系统连接起来。</p><p>比如，AS 1里面的主机10.10.0.2，要访问AS 2里面的主机172.17.0.3的话。它发出的IP包，就会先到达自治系统AS 1上的路由器 Router 1。</p><p>而在此时，Router 1的路由表里，有这样一条规则，即：目的地址是172.17.0.2包，应该经过Router 1的C接口，发往网关Router 2（即：自治系统AS 2上的路由器）。</p><p>所以IP包就会到达Router 2上，然后经过Router 2的路由表，从B接口出来到达目的主机172.17.0.3。</p><p>但是反过来，如果主机172.17.0.3要访问10.10.0.2，那么这个IP包，在到达Router 2之后，就不知道该去哪儿了。因为在Router 2的路由表里，并没有关于AS 1自治系统的任何路由规则。</p><p>所以这时候，网络管理员就应该给Router 2也添加一条路由规则，比如：目标地址是10.10.0.2的IP包，应该经过Router 2的C接口，发往网关Router 1。</p><p>像上面这样负责把自治系统连接在一起的路由器，我们就把它形象地称为： <strong>边界网关</strong>。它跟普通路由器的不同之处在于，它的路由表里拥有其他自治系统里的主机路由信息。</p><p>上面的这部分原理，相信你理解起来应该很容易。毕竟，路由器这个设备本身的主要作用，就是连通不同的网络。</p><p>但是，假如网络拓扑结构非常复杂，每个自治系统都有成千上万个主机、无数个路由器，甚至是由多个公司、多个网络提供商、多个自治系统组成的复合自治系统呢？</p><p>这时候，如果还要依靠人工来对边界网关的路由表进行配置和维护，那是绝对不现实的。</p><p>而这种情况下，BGP大显身手的时刻就到了。</p><p>在使用了BGP之后，你可以认为，在每个边界网关上都会运行着一个小程序，它们会将各自的路由表信息，通过TCP传输给其他的边界网关。而其他边界网关上的这个小程序，则会对收到的这些数据进行分析，然后将需要的信息添加到自己的路由表里。</p><p>这样，图2中Router 2的路由表里，就会自动出现10.10.0.2和10.10.0.3对应的路由规则了。</p><p>所以说， <strong>所谓BGP，就是在大规模网络中实现节点路由信息共享的一种协议。</strong></p><p>而BGP的这个能力，正好可以取代Flannel维护主机上路由表的功能。而且，BGP这种原生就是为大规模网络环境而实现的协议，其可靠性和可扩展性，远非Flannel自己的方案可比。</p><blockquote><p>需要注意的是，BGP协议实际上是最复杂的一种路由协议。这里的讲述和所举的例子，仅是为了能够帮助你建立对BGP的感性认识，并不代表BGP真正的实现方式。</p></blockquote><p>在了解了BGP之后，Calico项目的架构就非常容易理解了。它由三个部分组成：</p><ol><li><p>Calico的CNI插件。这是Calico与Kubernetes对接的部分。我已经在上一篇文章中，和你详细分享了CNI插件的工作原理，这里就不再赘述了。</p></li><li><p>Felix。它是一个DaemonSet，负责在宿主机上插入路由规则（即：写入Linux内核的FIB转发信息库），以及维护Calico所需的网络设备等工作。</p></li><li><p>BIRD。它就是BGP的客户端，专门负责在集群里分发路由规则信息。</p></li></ol><p><strong>除了对路由信息的维护方式之外，Calico项目与Flannel的host-gw模式的另一个不同之处，就是它不会在宿主机上创建任何网桥设备</strong>。这时候，Calico的工作方式，可以用一幅示意图来描述，如下所示（在接下来的讲述中，统一用“BGP示意图”来指代它）：</p><p><img src="https://static001.geekbang.org/resource/image/8d/1b/8db6dee96c4242738ae2878e58cecd1b.jpg?wh=1560*836" alt="img"></p><p>其中的绿色实线标出的路径，就是一个IP包从Node 1上的Container 1，到达Node 2上的Container 4的完整路径。</p><p>可以看到，Calico的CNI插件会为每个容器设置一个Veth Pair设备，然后把其中的一端放置在宿主机上（它的名字以cali前缀开头）。</p><p>此外，由于Calico没有使用CNI的网桥模式，Calico的CNI插件还需要在宿主机上为每个容器的Veth Pair设备配置一条路由规则，用于接收传入的IP包。比如，宿主机Node 2上的Container 4对应的路由规则，如下所示：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"><span class="hljs-number">10.233.2.3</span> dev cali5863f3 scope link<br><br></code></pre></td></tr></table></figure><p>即：发往10.233.2.3的IP包，应该进入cali5863f3设备。</p><blockquote><p>基于上述原因，Calico项目在宿主机上设置的路由规则，肯定要比Flannel项目多得多。不过，Flannel host-gw模式使用CNI网桥的主要原因，其实是为了跟VXLAN模式保持一致。否则的话，Flannel就需要维护两套CNI插件了。</p></blockquote><p>有了这样的Veth Pair设备之后，容器发出的IP包就会经过Veth Pair设备出现在宿主机上。然后，宿主机网络栈就会根据路由规则的下一跳IP地址，把它们转发给正确的网关。接下来的流程就跟Flannel host-gw模式完全一致了。</p><p>其中，这里最核心的“下一跳”路由规则，就是由Calico的Felix进程负责维护的。这些路由规则信息，则是通过BGP Client也就是BIRD组件，使用BGP协议传输而来的。</p><p>而这些通过BGP协议传输的消息，可以简单地理解为如下格式：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"><span class="hljs-string">[BGP消息]</span><br>我是宿主机<span class="hljs-number">192</span>.<span class="hljs-number">168</span>.<span class="hljs-number">1</span>.<span class="hljs-number">3</span><br><span class="hljs-number">10.233.2.0</span>/<span class="hljs-number">24</span>网段的容器都在我这里<br>这些容器的下一跳地址是我<br><br></code></pre></td></tr></table></figure><p>不难发现，Calico项目实际上将集群里的所有节点，都当作是边界路由器来处理，它们一起组成了一个全连通的网络，互相之间通过BGP协议交换路由规则。这些节点，我们称为BGP Peer。</p><p>需要注意的是， <strong>Calico维护的网络在默认配置下，是一个被称为“Node-to-Node Mesh”的模式</strong>。这时候，每台宿主机上的BGP Client都需要跟其他所有节点的BGP Client进行通信以便交换路由信息。但是，随着节点数量N的增加，这些连接的数量就会以N²的规模快速增长，从而给集群本身的网络带来巨大的压力。</p><p>所以，Node-to-Node Mesh模式一般推荐用在少于100个节点的集群里。而在更大规模的集群中，需要用到的是一个叫作Route Reflector的模式。</p><p>在这种模式下，Calico会指定一个或者几个专门的节点，来负责跟所有节点建立BGP连接从而学习到全局的路由规则。而其他节点，只需要跟这几个专门的节点交换路由信息，就可以获得整个集群的路由规则信息了。</p><p>这些专门的节点，就是所谓的Route Reflector节点，它们实际上扮演了“中间代理”的角色，从而把BGP连接的规模控制在N的数量级上。</p><p>此外，我在前面提到过，Flannel host-gw模式最主要的限制，就是要求集群宿主机之间是二层连通的。而这个限制对于Calico来说，也同样存在。</p><p>举个例子，假如我们有两台处于不同子网的宿主机Node 1和Node 2，对应的IP地址分别是192.168.1.2和192.168.2.2。需要注意的是，这两台机器通过路由器实现了三层转发，所以这两个IP地址之间是可以相互通信的。</p><p>而我们现在的需求，还是Container 1要访问Container 4。</p><p>按照我们前面的讲述，Calico会尝试在Node 1上添加如下所示的一条路由规则：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"><span class="hljs-number">10.233.2.0</span>/<span class="hljs-number">16</span> via <span class="hljs-number">192</span>.<span class="hljs-number">168</span>.<span class="hljs-number">2</span>.<span class="hljs-number">2</span> eth0<br><br></code></pre></td></tr></table></figure><p>但是，这时候问题就来了。</p><p>上面这条规则里的下一跳地址是192.168.2.2，可是它对应的Node 2跟Node 1却根本不在一个子网里，没办法通过二层网络把IP包发送到下一跳地址。</p><p><strong>在这种情况下，你就需要为Calico打开IPIP模式。</strong></p><p>IPIP示意图：</p><p><img src="https://static001.geekbang.org/resource/image/4d/c9/4dd9ad6415caf68da81562d9542049c9.jpg?wh=1696*1056" alt="img"></p><p>在Calico的IPIP模式下，Felix进程在Node 1上添加的路由规则，会稍微不同，如下所示：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"><span class="hljs-number">10.233.2.0</span>/<span class="hljs-number">24</span> via <span class="hljs-number">192</span>.<span class="hljs-number">168</span>.<span class="hljs-number">2</span>.<span class="hljs-number">2</span> tunl0<br><br></code></pre></td></tr></table></figure><p>可以看到，尽管这条规则的下一跳地址仍然是Node 2的IP地址，但这一次，要负责将IP包发出去的设备，变成了tunl0。注意，是T-U-N-L-0，而不是Flannel UDP模式使用的T-U-N-0（tun0），这两种设备的功能是完全不一样的。</p><p>Calico使用的这个tunl0设备，是一个IP隧道（IP tunnel）设备。</p><p>在上面的例子中，IP包进入IP隧道设备之后，就会被Linux内核的IPIP驱动接管。IPIP驱动会将这个IP包直接封装在一个宿主机网络的IP包中，如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/fc/90/fc2b4173782b7a993f4a43a2cb966f90.jpg?wh=2248*1034" alt="img"></p><p>图5 IPIP封包方式</p><p>其中，经过封装后的新的IP包的目的地址（图5中的Outer IP Header部分），正是原IP包的下一跳地址，即Node 2的IP地址：192.168.2.2。</p><p>而原IP包本身，则会被直接封装成新IP包的Payload。</p><p>这样，原先从容器到Node 2的IP包，就被伪装成了一个从Node 1到Node 2的IP包。</p><p>由于宿主机之间已经使用路由器配置了三层转发，也就是设置了宿主机之间的“下一跳”。所以这个IP包在离开Node 1之后，就可以经过路由器，最终“跳”到Node 2上。</p><p>这时，Node 2的网络内核栈会使用IPIP驱动进行解包，从而拿到原始的IP包。然后，原始IP包就会经过路由规则和Veth Pair设备到达目的容器内部。</p><p>以上，就是Calico项目主要的工作原理了。</p><p>不难看到，当Calico使用IPIP模式的时候，集群的网络性能会因为额外的封包和解包工作而下降。在实际测试中，Calico IPIP模式与Flannel VXLAN模式的性能大致相当。所以，在实际使用时，<strong>如非硬性需求，建议将所有宿主机节点放在一个子网里，避免使用IPIP。</strong></p><p>不过，通过上面对Calico工作原理的讲述，你应该能发现这样一个事实：</p><p>如果Calico项目能够让宿主机之间的路由设备（也就是网关），也通过BGP协议“学习”到Calico网络里的路由规则，那么从容器发出的IP包，不就可以通过这些设备路由到目的宿主机了么？</p><p>比如，只要在上面“IPIP示意图”中的Node 1上，添加如下所示的一条路由规则：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"><span class="hljs-number">10.233.2.0</span>/<span class="hljs-number">24</span> via <span class="hljs-number">192</span>.<span class="hljs-number">168</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span> eth0<br><br></code></pre></td></tr></table></figure><p>然后，在Router 1上（192.168.1.1），添加如下所示的一条路由规则：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"><span class="hljs-number">10.233.2.0</span>/<span class="hljs-number">24</span> via <span class="hljs-number">192</span>.<span class="hljs-number">168</span>.<span class="hljs-number">2</span>.<span class="hljs-number">1</span> eth0<br><br></code></pre></td></tr></table></figure><p>那么Container 1发出的IP包，就可以通过两次“下一跳”，到达Router 2（192.168.2.1）了。以此类推，我们可以继续在Router 2上添加“下一条”路由，最终把IP包转发到Node 2上。</p><p>遗憾的是，上述流程虽然简单明了，但是在Kubernetes被广泛使用的公有云场景里，却完全不可行。</p><p>这里的原因在于：公有云环境下，宿主机之间的网关，肯定不会允许用户进行干预和设置。</p><blockquote><p>当然，在大多数公有云环境下，宿主机（公有云提供的虚拟机）本身往往就是二层连通的，所以这个需求也不强烈。</p></blockquote><p>不过，在私有部署的环境下，宿主机属于不同子网（VLAN）反而是更加常见的部署状态。这时候，想办法将宿主机网关也加入到BGP Mesh里从而避免使用IPIP，就成了一个非常迫切的需求。</p><p>而在Calico项目中，它已经为你提供了两种将宿主机网关设置成BGP Peer的解决方案。</p><p><strong>第一种方案</strong>，就是所有宿主机都跟宿主机网关建立BGP Peer关系。</p><p>这种方案下，Node 1和Node 2就需要主动跟宿主机网关Router 1和Router 2建立BGP连接。从而将类似于10.233.2.0&#x2F;24这样的路由信息同步到网关上去。</p><p>需要注意的是，这种方式下，Calico要求宿主机网关必须支持一种叫作Dynamic Neighbors的BGP配置方式。这是因为，在常规的路由器BGP配置里，运维人员必须明确给出所有BGP Peer的IP地址。考虑到Kubernetes集群可能会有成百上千个宿主机，而且还会动态地添加和删除节点，这时候再手动管理路由器的BGP配置就非常麻烦了。而Dynamic Neighbors则允许你给路由器配置一个网段，然后路由器就会自动跟该网段里的主机建立起BGP Peer关系。</p><p>不过，相比之下，更推荐 <strong>第二种方案</strong>。</p><p>这种方案，是使用一个或多个独立组件负责搜集整个集群里的所有路由信息，然后通过BGP协议同步给网关。而我们前面提到，在大规模集群中，Calico本身就推荐使用Route Reflector节点的方式进行组网。所以，这里负责跟宿主机网关进行沟通的独立组件，直接由Route Reflector兼任即可。</p><p>更重要的是，这种情况下网关的BGP Peer个数是有限并且固定的。所以我们就可以直接把这些独立组件配置成路由器的BGP Peer，而无需Dynamic Neighbors的支持。</p><p>当然，这些独立组件的工作原理也很简单：它们只需要WATCH Etcd里的宿主机和对应网段的变化信息，然后把这些信息通过BGP协议分发给网关即可。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在本篇文章中，详细讲述了Fannel host-gw模式和Calico这两种纯三层网络方案的工作原理。</p><p>需要注意的是，在大规模集群里，三层网络方案在宿主机上的路由规则可能会非常多，这会导致错误排查变得困难。此外，在系统故障的时候，路由规则出现重叠冲突的概率也会变大。</p><p>基于上述原因，如果是在公有云上，由于宿主机网络本身比较“直白”，一般会推荐更加简单的Flannel host-gw模式。</p><p>但不难看到，在私有部署环境里，Calico项目才能够覆盖更多的场景，并提供更加可靠的组网方案和架构思路。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><p>总结一下三层网络方案和“隧道模式”的异同，以及各自的优缺点？</p><p><strong>三层和隧道的异同：</strong> </p><p>相同之处是都实现了跨主机容器的三层互通，而且都是通过对目的 MAC 地址的操作来实现的；</p><p>不同之处是三层通过配置下一条主机的路由规则来实现互通，隧道则是通过通过在 IP 包外再封装一层 MAC 包头来实现。 </p><p>三层的优点：少了封包和解包的过程，性能肯定是更高的。 </p><p>三层的缺点：需要自己想办法维护路由规则。 </p><p>隧道的优点：简单，原因是大部分工作都是由 Linux 内核的模块实现了，应用层面工作量较少。 </p><p>隧道的缺点：主要的问题就是性能低。</p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github中release里面文件下载太慢或者无法下载的解决方法</title>
    <link href="/2022/10/28/Github%E4%B8%ADrelease%E9%87%8C%E9%9D%A2%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD%E5%A4%AA%E6%85%A2%E6%88%96%E8%80%85%E6%97%A0%E6%B3%95%E4%B8%8B%E8%BD%BD%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/"/>
    <url>/2022/10/28/Github%E4%B8%ADrelease%E9%87%8C%E9%9D%A2%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD%E5%A4%AA%E6%85%A2%E6%88%96%E8%80%85%E6%97%A0%E6%B3%95%E4%B8%8B%E8%BD%BD%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<blockquote><p>最近在GitHub上面下载一个release里面的exe文件下载到10%不到就下载失败，在网上找了很多方式和插件都没有成功。但最后找到一个网站解决掉了这个问题，给大家分享一下。</p></blockquote><p><strong><a href="https://d.serctl.com/">https://d.serctl.com</a></strong><br><img src="https://img-blog.csdnimg.cn/20200628093805317.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyMDA5MjYy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="使用步骤"><a href="#使用步骤" class="headerlink" title="使用步骤"></a>使用步骤</h2><p><strong>1.在GitHub获取下载链接地址</strong><br><img src="https://img-blog.csdnimg.cn/20200628094009353.png" alt="在这里插入图片描述"><br><strong>2.复制到<a href="https://d.serctl.com/">https://d.serctl.com</a>中的下载地址并提交</strong><br><img src="https://img-blog.csdnimg.cn/20200628094106143.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyMDA5MjYy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p><strong>3.在下载地址中打开文件即可</strong><br><img src="https://img-blog.csdnimg.cn/20200628094217561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyMDA5MjYy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>]]></content>
    
    
    <categories>
      
      <category>github</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>github</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>容器跨主机网络--flannel的UDP与VXLAN模式</title>
    <link href="/2022/10/24/%E5%AE%B9%E5%99%A8%E8%B7%A8%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C-flannel%E7%9A%84UDP%E4%B8%8EVXLAN%E6%A8%A1%E5%BC%8F/"/>
    <url>/2022/10/24/%E5%AE%B9%E5%99%A8%E8%B7%A8%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C-flannel%E7%9A%84UDP%E4%B8%8EVXLAN%E6%A8%A1%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="容器跨主机网络–flannel的UDP与VXLAN模式"><a href="#容器跨主机网络–flannel的UDP与VXLAN模式" class="headerlink" title="容器跨主机网络–flannel的UDP与VXLAN模式"></a>容器跨主机网络–flannel的UDP与VXLAN模式</h1><blockquote><p>本文笔记来自：「深入剖析 Kubernetes课程」，原文：<a href="https://time.geekbang.org/column/article/64948">https://time.geekbang.org/column/article/64948</a></p></blockquote><p>在单机环境下，Linux容器网络的实现原理（网桥模式）提到了在Docker的默认配置下，不同宿主机上的容器通过IP地址进行互相访问是根本做不到的。</p><p>而正是为了解决这个容器“跨主通信”的问题，社区里才出现了那么多的容器网络方案。这些网络方案的工作原理到底是什么？</p><p>要理解容器“跨主通信”的原理，就一定要先从Flannel这个项目说起。</p><p>Flannel项目是CoreOS公司主推的容器网络方案。事实上，Flannel项目本身只是一个框架，真正为我们提供容器网络功能的，是Flannel的后端实现。目前，Flannel支持三种后端实现，分别是：</p><ol><li><p>VXLAN；</p></li><li><p>host-gw；</p></li><li><p>UDP。</p></li></ol><p>这三种不同的后端实现，正代表了三种容器跨主网络的主流实现方法。</p><p>UDP模式，是Flannel项目最早支持的一种方式，却也是性能最差的一种方式。所以，这个模式目前已经被弃用。不过，Flannel之所以最先选择UDP模式，就是因为这种模式是最直接、也是最容易理解的容器跨主网络实现。</p><p>在这个例子中，有两台宿主机。</p><ul><li>宿主机Node 1上有一个容器container-1，它的IP地址是100.96.1.2，对应的docker0网桥的地址是：100.96.1.1&#x2F;24。</li><li>宿主机Node 2上有一个容器container-2，它的IP地址是100.96.2.3，对应的docker0网桥的地址是：100.96.2.1&#x2F;24。</li></ul><p>我们现在的任务，就是让container-1访问container-2。</p><p>这种情况下，container-1容器里的进程发起的IP包，其源地址就是100.96.1.2，目的地址就是100.96.2.3。由于目的地址100.96.2.3并不在Node 1的docker0网桥的网段里，所以这个IP包会被交给默认路由规则，通过容器的网关进入docker0网桥（如果是同一台宿主机上的容器间通信，走的是直连规则），从而出现在宿主机上。</p><p>这时候，这个IP包的下一个目的地，就取决于宿主机上的路由规则了。此时，Flannel已经在宿主机上创建出了一系列的路由规则，以Node 1为例，如下所示：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"># 在Node <span class="hljs-number">1</span>上<br>$ ip route<br>default via <span class="hljs-number">10</span>.<span class="hljs-number">168</span>.<span class="hljs-number">0</span>.<span class="hljs-number">1</span> dev eth0<br><span class="hljs-number">100.96.0.0</span>/<span class="hljs-number">16</span> dev flannel0  proto kernel  scope link  src <span class="hljs-number">100</span>.<span class="hljs-number">96</span>.<span class="hljs-number">1</span>.<span class="hljs-number">0</span><br><span class="hljs-number">100.96.1.0</span>/<span class="hljs-number">24</span> dev docker0  proto kernel  scope link  src <span class="hljs-number">100</span>.<span class="hljs-number">96</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span><br><span class="hljs-number">10.168.0.0</span>/<span class="hljs-number">24</span> dev eth0  proto kernel  scope link  src <span class="hljs-number">10</span>.<span class="hljs-number">168</span>.<span class="hljs-number">0</span>.<span class="hljs-number">2</span><br><br></code></pre></td></tr></table></figure><p>可以看到，由于我们的IP包的目的地址是100.96.2.3，它匹配不到本机docker0网桥对应的100.96.1.0&#x2F;24网段，只能匹配到第二条、也就是100.96.0.0&#x2F;16对应的这条路由规则，从而进入到一个叫作flannel0的设备中。</p><p>而这个flannel0设备的类型就比较有意思了：它是一个TUN设备（Tunnel设备）。</p><p>在Linux中，TUN设备是一种工作在三层（Network Layer）的虚拟网络设备。TUN设备的功能非常简单，即： <strong>在操作系统内核和用户应用程序之间传递IP包。</strong></p><p>以flannel0设备为例：像上面提到的情况，当操作系统将一个IP包发送给flannel0设备之后，flannel0就会把这个IP包，交给创建这个设备的应用程序，也就是Flannel进程。这是一个从内核态（Linux操作系统）向用户态（Flannel进程）的流动方向。</p><p>反之，如果Flannel进程向flannel0设备发送了一个IP包，那么这个IP包就会出现在宿主机网络栈中，然后根据宿主机的路由表进行下一步处理。这是一个从用户态向内核态的流动方向。</p><p>所以，当IP包从容器经过docker0出现在宿主机，然后又根据路由表进入flannel0设备后，宿主机上的flanneld进程（Flannel项目在每个宿主机上的主进程），就会收到这个IP包。然后，flanneld看到了这个IP包的目的地址，是100.96.2.3，就把它发送给了Node 2宿主机。</p><p> <strong>flanneld是如何知道这个IP地址对应的容器，是运行在Node 2上的呢？</strong></p><p>这里用到了Flannel项目里一个非常重要的概念：子网（Subnet）。</p><p>事实上，在由Flannel管理的容器网络里，一台宿主机上的所有容器，都属于该宿主机被分配的一个“子网”。在我们的例子中，Node 1的子网是100.96.1.0&#x2F;24，container-1的IP地址是100.96.1.2。Node 2的子网是100.96.2.0&#x2F;24，container-2的IP地址是100.96.2.3。</p><p>而这些子网与宿主机的对应关系，正是保存在Etcd当中，如下所示：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs awk">$ etcdctl ls <span class="hljs-regexp">/coreos.com/</span>network/subnets<br><span class="hljs-regexp">/coreos.com/</span>network<span class="hljs-regexp">/subnets/</span><span class="hljs-number">100.96</span>.<span class="hljs-number">1.0</span>-<span class="hljs-number">24</span><br><span class="hljs-regexp">/coreos.com/</span>network<span class="hljs-regexp">/subnets/</span><span class="hljs-number">100.96</span>.<span class="hljs-number">2.0</span>-<span class="hljs-number">24</span><br><span class="hljs-regexp">/coreos.com/</span>network<span class="hljs-regexp">/subnets/</span><span class="hljs-number">100.96</span>.<span class="hljs-number">3.0</span>-<span class="hljs-number">24</span><br><br></code></pre></td></tr></table></figure><p>所以，flanneld进程在处理由flannel0传入的IP包时，就可以根据目的IP的地址（比如100.96.2.3），匹配到对应的子网（比如100.96.2.0&#x2F;24），从Etcd中找到这个子网对应的宿主机的IP地址是10.168.0.3，如下所示：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">$ etcdctl get <span class="hljs-regexp">/coreos.com/</span>network<span class="hljs-regexp">/subnets/</span><span class="hljs-number">100.96</span>.<span class="hljs-number">2.0</span>-<span class="hljs-number">24</span><br>&#123;<span class="hljs-string">&quot;PublicIP&quot;</span>:<span class="hljs-string">&quot;10.168.0.3&quot;</span>&#125;<br><br></code></pre></td></tr></table></figure><p>而对于flanneld来说，只要Node 1和Node 2是互通的，那么flanneld作为Node 1上的一个普通进程，就一定可以通过上述IP地址（10.168.0.3）访问到Node 2，这没有任何问题。</p><p>所以说，flanneld在收到container-1发给container-2的IP包之后，就会把这个IP包直接封装在一个UDP包里，然后发送给Node 2。不难理解，这个UDP包的源地址，就是flanneld所在的Node 1的地址，而目的地址，则是container-2所在的宿主机Node 2的地址。</p><p>这个请求得以完成的原因是，每台宿主机上的flanneld，都监听着一个8285端口，所以flanneld只要把UDP包发往Node 2的8285端口即可。</p><p>通过这样一个普通的、宿主机之间的UDP通信，一个UDP包就从Node 1到达了Node 2。而Node 2上监听8285端口的进程也是flanneld，所以这时候，flanneld就可以从这个UDP包里解析出封装在里面的、container-1发来的原IP包。</p><p>而接下来flanneld的工作就非常简单了：flanneld会直接把这个IP包发送给它所管理的TUN设备，即flannel0设备。</p><p>根据TUN设备的原理，这正是一个从用户态向内核态的流动方向（Flannel进程向TUN设备发送数据包），所以Linux内核网络栈就会负责处理这个IP包，具体的处理方法，就是通过本机的路由表来寻找这个IP包的下一步流向。</p><p>而Node 2上的路由表，跟Node 1非常类似，如下所示：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"># 在Node <span class="hljs-number">2</span>上<br>$ ip route<br>default via <span class="hljs-number">10</span>.<span class="hljs-number">168</span>.<span class="hljs-number">0</span>.<span class="hljs-number">1</span> dev eth0<br><span class="hljs-number">100.96.0.0</span>/<span class="hljs-number">16</span> dev flannel0  proto kernel  scope link  src <span class="hljs-number">100</span>.<span class="hljs-number">96</span>.<span class="hljs-number">2</span>.<span class="hljs-number">0</span><br><span class="hljs-number">100.96.2.0</span>/<span class="hljs-number">24</span> dev docker0  proto kernel  scope link  src <span class="hljs-number">100</span>.<span class="hljs-number">96</span>.<span class="hljs-number">2</span>.<span class="hljs-number">1</span><br><span class="hljs-number">10.168.0.0</span>/<span class="hljs-number">24</span> dev eth0  proto kernel  scope link  src <span class="hljs-number">10</span>.<span class="hljs-number">168</span>.<span class="hljs-number">0</span>.<span class="hljs-number">3</span><br><br></code></pre></td></tr></table></figure><p>由于这个IP包的目的地址是100.96.2.3，它跟第三条、也就是100.96.2.0&#x2F;24网段对应的路由规则匹配更加精确。所以，Linux内核就会按照这条路由规则，把这个IP包转发给docker0网桥。</p><p>接下来，docker0网桥会扮演二层交换机的角色，将数据包发送给正确的端口，进而通过Veth Pair设备进入到container-2的Network Namespace里。</p><p>而container-2返回给container-1的数据包，则会经过与上述过程完全相反的路径回到container-1中。</p><p>需要注意的是，上述流程要正确工作还有一个重要的前提，那就是docker0网桥的地址范围必须是Flannel为宿主机分配的子网。这个很容易实现，以Node 1为例，你只需要给它上面的Docker Daemon启动时配置如下所示的bip参数即可：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros">$ <span class="hljs-attribute">FLANNEL_SUBNET</span>=100.96.1.1/24<br>$ dockerd <span class="hljs-attribute">--bip</span>=<span class="hljs-variable">$FLANNEL_SUBNET</span> <span class="hljs-built_in">..</span>.<br><br></code></pre></td></tr></table></figure><p>以上，就是基于Flannel UDP模式的跨主通信的基本原理了。原理图如下所示。</p><p><img src="https://static001.geekbang.org/resource/image/83/6c/8332564c0547bf46d1fbba2a1e0e166c.jpg?wh=1857*878" alt="img"></p><p>可以看到，Flannel UDP模式提供的其实是一个三层的Overlay网络，即：它首先对发出端的IP包进行UDP封装，然后在接收端进行解封装拿到原始的IP包，进而把这个IP包转发给目标容器。这就好比，Flannel在不同宿主机上的两个容器之间打通了一条“隧道”，使得这两个容器可以直接使用IP地址进行通信，而无需关心容器和宿主机的分布情况。</p><p>上述UDP模式有严重的性能问题，所以已经被废弃了。那么性能问题出现在了哪里呢？</p><p>实际上，相比于两台宿主机之间的直接通信，基于Flannel UDP模式的容器通信多了一个额外的步骤，即flanneld的处理过程。而这个过程，由于使用到了flannel0这个TUN设备，仅在发出IP包的过程中，就需要经过三次用户态与内核态之间的数据拷贝，如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/84/8d/84caa6dc3f9dcdf8b88b56bd2e22138d.png?wh=890*593" alt="img"></p><p>可以看到：</p><p>第一次，用户态的容器进程发出的IP包经过docker0网桥进入内核态；</p><p>第二次，IP包根据路由表进入TUN（flannel0）设备，从而回到用户态的flanneld进程；</p><p>第三次，flanneld进行UDP封包之后重新进入内核态，将UDP包通过宿主机的eth0发出去。</p><p>此外，我们还可以看到，Flannel进行<strong>UDP封装（Encapsulation）和解封装（Decapsulation）</strong>的过程，也都是在用户态完成的。在Linux操作系统中，上述这些上下文切换和用户态操作的代价其实是比较高的，这也正是造成Flannel UDP模式性能不好的主要原因。</p><p>所以说， <strong>我们在进行系统级编程的时候，有一个非常重要的优化原则，就是要减少用户态到内核态的切换次数，并且把核心的处理逻辑都放在内核态进行</strong>。这也是为什么，Flannel后来支持的VXLAN模式，逐渐成为了主流的容器网络方案的原因。</p><p><strong>VXLAN</strong>，即V<strong>irtual Extensible LAN（虚拟可扩展局域网</strong>），是Linux内核本身就支持的一种网络虚似化技术。所以说，VXLAN可以完全在内核态实现上述封装和解封装的工作，从而通过与前面相似的“隧道”机制，构建出覆盖网络（Overlay Network）。</p><p>VXLAN的覆盖网络的设计思想是：在现有的三层网络之上，“覆盖”一层虚拟的、由内核VXLAN模块负责维护的二层网络，使得连接在这个VXLAN二层网络上的“主机”（虚拟机或者容器都可以）之间，可以像在同一个局域网（LAN）里那样自由通信。当然，实际上，这些“主机”可能分布在不同的宿主机上，甚至是分布在不同的物理机房里。</p><p>而为了能够在二层网络上打通“隧道”，VXLAN会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。这个设备就叫作VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）。</p><p>而VTEP设备的作用，其实跟前面的flanneld进程非常相似。只不过，它进行封装和解封装的对象，是二层数据帧（Ethernet frame）；而且这个工作的执行流程，全部是在内核里完成的（因为VXLAN本身就是Linux内核中的一个模块）。</p><p>上述基于VTEP设备进行“隧道”通信的流程，如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/03/f5/03185fab251a833fef7ed6665d5049f5.jpg?wh=1767*933" alt="img"></p><p>可以看到，图中每台宿主机上名叫flannel.1的设备，就是VXLAN所需的VTEP设备，它既有IP地址，也有MAC地址。</p><p>现在，我们的container-1的IP地址是10.1.15.2，要访问的container-2的IP地址是10.1.16.3。</p><p>那么，与前面UDP模式的流程类似，当container-1发出请求之后，这个目的地址是10.1.16.3的IP包，会先出现在docker0网桥，然后被路由到本机flannel.1设备进行处理。也就是说，来到了“隧道”的入口。为了方便叙述，我接下来会把这个IP包称为“原始IP包”。</p><p>为了能够将“原始IP包”封装并且发送到正确的宿主机，VXLAN就需要找到这条“隧道”的出口，即：目的宿主机的VTEP设备。</p><p>而这个设备的信息，正是每台宿主机上的flanneld进程负责维护的。</p><p>比如，当Node 2启动并加入Flannel网络之后，在Node 1（以及所有其他节点）上，flanneld就会添加一条如下所示的路由规则：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs accesslog">$ route -n<br>Kernel IP routing table<br>Destination     Gateway         Genmask         Flags Metric Ref    Use Iface<br>...<br><span class="hljs-number">10.1.16.0</span>       <span class="hljs-number">10</span>.<span class="hljs-number">1</span>.<span class="hljs-number">16</span>.<span class="hljs-number">0</span>       <span class="hljs-number">255</span>.<span class="hljs-number">255</span>.<span class="hljs-number">255</span>.<span class="hljs-number">0</span>   UG    <span class="hljs-number">0</span>      <span class="hljs-number">0</span>        <span class="hljs-number">0</span> flannel.<span class="hljs-number">1</span><br><br></code></pre></td></tr></table></figure><p>这条规则的意思是：凡是发往10.1.16.0&#x2F;24网段的IP包，都需要经过flannel.1设备发出，并且，它最后被发往的网关地址是：10.1.16.0。</p><p>从图3的Flannel VXLAN模式的流程图中我们可以看到，10.1.16.0正是Node 2上的VTEP设备（也就是flannel.1设备）的IP地址。</p><p>为了方便叙述，接下来把Node 1和Node 2上的flannel.1设备分别称为“源VTEP设备”和“目的VTEP设备”。</p><p>而这些VTEP设备之间，就需要想办法组成一个虚拟的二层网络，即：通过二层数据帧进行通信。</p><p>所以在我们的例子中，“源VTEP设备”收到“原始IP包”后，就要想办法把“原始IP包”加上一个目的MAC地址，封装成一个二层数据帧，然后发送给“目的VTEP设备”（这么做是因为这个IP包的目的地址不是本机）。</p><p>这里需要解决的问题就是： <strong>“目的VTEP设备”的MAC地址是什么？</strong></p><p>此时，根据前面的路由记录，我们已经知道了“目的VTEP设备”的IP地址。而要根据三层IP地址查询对应的二层MAC地址，这正是ARP（Address Resolution Protocol ）表的功能。</p><p>而这里要用到的ARP记录，也是flanneld进程在Node 2节点启动时，自动添加在Node 1上的。我们可以通过ip命令看到它，如下所示：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"># 在Node <span class="hljs-number">1</span>上<br>$ ip neigh show dev flannel.<span class="hljs-number">1</span><br><span class="hljs-number">10.1.16.0</span> lladdr 5e:f8:4f:<span class="hljs-number">00</span>:e3:<span class="hljs-number">37</span> PERMANENT<br><br></code></pre></td></tr></table></figure><p>这条记录的意思非常明确，即：IP地址10.1.16.0，对应的MAC地址是5e:f8:4f:00:e3:37。</p><blockquote><p>可以看到，最新版本的Flannel并不依赖L3 MISS事件和ARP学习，而会在每台节点启动时把它的VTEP设备对应的ARP记录，直接下放到其他每台宿主机上。</p></blockquote><p>有了这个“目的VTEP设备”的MAC地址， <strong>Linux内核就可以开始二层封包工作了</strong>。这个二层帧的格式，如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/f2/55/f208fba66d2b58405864882342b23255.jpg?wh=799*179" alt="img"></p><p>可以看到，Linux内核会把“目的VTEP设备”的MAC地址，填写在图中的Inner Ethernet Header字段，得到一个二层数据帧。</p><p>需要注意的是，上述封包过程只是加一个二层头，不会改变“原始IP包”的内容。所以图中的Inner IP Header字段，依然是container-2的IP地址，即10.1.16.3。</p><p>但是，上面提到的这些VTEP设备的MAC地址，对于宿主机网络来说并没有什么实际意义。所以上面封装出来的这个数据帧，并不能在我们的宿主机二层网络里传输。为了方便叙述，我们把它称为“内部数据帧”（Inner Ethernet Frame）。</p><p>所以接下来，Linux内核还需要再把“内部数据帧”进一步封装成为宿主机网络里的一个普通的数据帧，好让它“载着”“内部数据帧”，通过宿主机的eth0网卡进行传输。</p><p>这次要封装出来的、宿主机对应的数据帧称为“<strong>外部数据帧</strong>”（Outer Ethernet Frame）。</p><p>为了实现这个“搭便车”的机制，Linux内核会在“内部数据帧”前面，加上一个特殊的VXLAN头，用来表示这个“乘客”实际上是一个VXLAN要使用的数据帧。</p><p>而这个VXLAN头里有一个重要的标志叫作 <strong>VNI</strong>，它是VTEP设备识别某个数据帧是不是应该归自己处理的重要标识。而在Flannel中，VNI的默认值是1，这也是为何，宿主机上的VTEP设备都叫作flannel.1的原因，这里的“1”，其实就是VNI的值。</p><p><strong>然后，Linux内核会把这个数据帧封装进一个UDP包里发出去。</strong></p><p>所以，跟UDP模式类似，在宿主机看来，它会以为自己的flannel.1设备只是在向另外一台宿主机的flannel.1设备，发起了一次普通的UDP链接。它哪里会知道，这个UDP包里面，其实是一个完整的二层数据帧。这是不是跟特洛伊木马的故事非常像呢？</p><p>不过，不要忘了，一个flannel.1设备只知道另一端的flannel.1设备的MAC地址，却不知道对应的宿主机地址是什么。</p><p>也就是说，这个UDP包该发给哪台宿主机呢？</p><p>在这种场景下，flannel.1设备实际上要扮演一个“网桥”的角色，在二层网络进行UDP包的转发。而在Linux内核里面，“网桥”设备进行转发的依据，来自于一个叫作FDB（Forwarding Database）的转发数据库。</p><p>不难想到，这个flannel.1“网桥”对应的FDB信息，也是flanneld进程负责维护的。它的内容可以通过bridge fdb命令查看到，如下所示：</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs elixir"><span class="hljs-comment"># 在Node 1上，使用“目的VTEP设备”的MAC地址进行查询</span><br><span class="hljs-variable">$ </span>bridge fdb show flannel.<span class="hljs-number">1</span> | grep <span class="hljs-number">5</span><span class="hljs-symbol">e:</span><span class="hljs-symbol">f8:</span><span class="hljs-number">4</span><span class="hljs-symbol">f:</span><span class="hljs-number">00</span><span class="hljs-symbol">:e3</span><span class="hljs-symbol">:</span><span class="hljs-number">37</span><br><span class="hljs-number">5</span><span class="hljs-symbol">e:</span><span class="hljs-symbol">f8:</span><span class="hljs-number">4</span><span class="hljs-symbol">f:</span><span class="hljs-number">00</span><span class="hljs-symbol">:e3</span><span class="hljs-symbol">:</span><span class="hljs-number">37</span> dev flannel.<span class="hljs-number">1</span> dst <span class="hljs-number">10.168</span>.<span class="hljs-number">0.3</span> self permanent<br><br></code></pre></td></tr></table></figure><p>可以看到，在上面这条FDB记录里，指定了这样一条规则，即：</p><p>发往我们前面提到的“目的VTEP设备”（MAC地址是5e:f8:4f:00:e3:37）的二层数据帧，应该通过flannel.1设备，发往IP地址为10.168.0.3的主机。显然，这台主机正是Node 2，UDP包要发往的目的地就找到了。</p><p>所以 <strong>接下来的流程，就是一个正常的、宿主机网络上的封包工作。</strong></p><p>我们知道，UDP包是一个四层数据包，所以Linux内核会在它前面加上一个IP头，即原理图中的Outer IP Header，组成一个IP包。并且，在这个IP头里，会填上前面通过FDB查询出来的目的主机的IP地址，即Node 2的IP地址10.168.0.3。</p><p>然后，Linux内核再在这个IP包前面加上二层数据帧头，即原理图中的Outer Ethernet Header，并把Node 2的MAC地址填进去。这个MAC地址本身，是Node 1的ARP表要学习的内容，无需Flannel维护。这时候，我们封装出来的“外部数据帧”的格式，如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/8c/85/8cede8f74a57617494027ba137383f85.jpg?wh=1864*192" alt="img"></p><p>这样，封包工作就宣告完成了。</p><p>接下来，Node 1上的flannel.1设备就可以把这个数据帧从Node 1的eth0网卡发出去。显然，这个帧会经过宿主机网络来到Node 2的eth0网卡。</p><p>这时候，Node 2的内核网络栈会发现这个数据帧里有VXLAN Header，并且VNI&#x3D;1。所以Linux内核会对它进行拆包，拿到里面的内部数据帧，然后根据VNI的值，把它交给Node 2上的flannel.1设备。</p><p>而flannel.1设备则会进一步拆包，取出“原始IP包”。接下来就回到了之前分享的单机容器网络的处理流程。最终，IP包就进入到了container-2容器的Network Namespace里。</p><p>以上，就是Flannel VXLAN模式的具体工作原理了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在本篇文章中，详细讲解了Flannel UDP和VXLAN模式的工作原理。这两种模式其实都可以称作“隧道”机制，也是很多其他容器网络插件的基础。比如Weave的两种模式，以及Docker的Overlay模式。</p><p>此外，从上面的讲解中我们可以看到，VXLAN模式组建的覆盖网络，其实就是一个由不同宿主机上的VTEP设备，也就是flannel.1设备组成的虚拟二层网络。对于VTEP设备来说，它发出的“内部数据帧”就仿佛是一直在这个虚拟的二层网络上流动。这，也正是覆盖网络的含义。</p><blockquote><p>备注：如果你想要在我们前面部署的集群中实践Flannel的话，可以在Master节点上执行如下命令来替换网络插件。</p><p>第一步，执行 <code>$ rm -rf /etc/cni/net.d/*</code>；</p><p>第二步，执行 <code>$ kubectl delete -f &quot;https://cloud.weave.works/k8s/net?k8s-version=1.11&quot;</code>；</p><p>第三步，在 <code>/etc/kubernetes/manifests/kube-controller-manager.yaml</code> 里，为容器启动命令添加如下两个参数：</p><p><code>--allocate-node-cidrs=true</code></p><p><code>--cluster-cidr=10.244.0.0/16</code></p><p>第四步， 重启所有kubelet；</p><p>第五步， 执行 <code>$ kubectl create -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml</code>。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>浅谈容器网络--单节点容器通信</title>
    <link href="/2022/10/24/%E6%B5%85%E8%B0%88%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C-%E5%8D%95%E8%8A%82%E7%82%B9%E5%AE%B9%E5%99%A8%E9%80%9A%E4%BF%A1/"/>
    <url>/2022/10/24/%E6%B5%85%E8%B0%88%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C-%E5%8D%95%E8%8A%82%E7%82%B9%E5%AE%B9%E5%99%A8%E9%80%9A%E4%BF%A1/</url>
    
    <content type="html"><![CDATA[<h1 id="浅谈容器网络–单节点容器通信"><a href="#浅谈容器网络–单节点容器通信" class="headerlink" title="浅谈容器网络–单节点容器通信"></a>浅谈容器网络–单节点容器通信</h1><blockquote><p>本文笔记来自：「深入剖析 Kubernetes课程」，原文链接：<a href="https://time.geekbang.org/column/article/64948">https://time.geekbang.org/column/article/64948</a></p></blockquote><p>所谓“网络栈”，包括了：<strong>网卡（Network Interface）、回环设备（Loopback Device）、路由表（Routing Table）和iptables规则</strong>。对于一个进程来说，这些要素，构成了它发起和响应网络请求的基本环境。</p><p>需要指出的是，作为一个容器，它可以声明直接使用宿主机的网络栈（–net&#x3D;host），即：不开启Network Namespace，比如：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">$ docker <span class="hljs-built_in">run</span> –d –<span class="hljs-attribute">net</span>=host --name nginx-host nginx<br></code></pre></td></tr></table></figure><p>在这种情况下，这个容器启动后，直接监听的就是宿主机的80端口。</p><p>像这样直接使用宿主机网络栈的方式，虽然可以为容器提供良好的网络性能，但也会不可避免地引入共享网络资源的问题，比如端口冲突。所以， <strong>在大多数情况下，我们都希望容器进程能使用自己Network Namespace里的网络栈，即：拥有属于自己的IP地址和端口。</strong></p><p>这时候，一个显而易见的问题就是：这个被隔离的容器进程，该如何跟其他Network Namespace里的容器进程进行交互呢？</p><p>为了理解这个问题，可以把每一个容器看做一台主机，它们都有一套独立的“网络栈”。</p><p>如果想要实现两台主机之间的通信，最直接的办法，就是把它们用一根网线连接起来；而如果你想要实现多台主机之间的通信，那就需要用网线，把它们连接在一台交换机上。</p><p>在Linux中，能够起到虚拟交换机作用的网络设备，是网桥（Bridge）。它是一个工作在数据链路层（Data Link）的设备，主要功能是根据MAC地址学习来将数据包转发到网桥的不同端口（Port）上。</p><p>·至于为什么这些主机之间需要MAC地址才能进行通信，这就是网络分层模型的基础知识了。不熟悉这块可以通过 <a href="https://www.lifewire.com/layers-of-the-osi-model-illustrated-818017">这篇文章</a> 来学习一下。</p><p>而为了实现上述目的，Docker项目会默认在宿主机上创建一个名叫docker0的网桥，凡是连接在docker0网桥上的容器，就可以通过它来进行通信。</p><p>可是，又该如何把这些容器“连接”到docker0网桥上呢？</p><p>这时候，我们就需要使用一种名叫 <strong>Veth Pair</strong> 的虚拟设备了。</p><p>Veth Pair设备的特点是：它被创建出来后，总是以两张虚拟网卡（Veth Peer）的形式成对出现的。并且，从其中一个“网卡”发出的数据包，可以直接出现在与它对应的另一张“网卡”上，哪怕这两个“网卡”在不同的Network Namespace里。</p><p>这就使得Veth Pair常常被用作连接不同Network Namespace 的“网线”。</p><p>比如，现在我们启动了一个叫作nginx-1的容器：</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs applescript">$ docker <span class="hljs-built_in">run</span> –d <span class="hljs-comment">--name nginx-1 nginx</span><br><br></code></pre></td></tr></table></figure><p>然后进入到这个容器中查看一下它的网络设备：</p><figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs tap"><span class="hljs-comment"># 在宿主机上</span><br>$ docker exec -it nginx-1 /bin/bash<br><span class="hljs-comment"># 在容器里</span><br>root@2b3c181aecf1:/<span class="hljs-comment"># ifconfig</span><br>eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500<br>        inet 172.17.0.2  netmask 255.255.0.0  broadcast 0.0.0.0<br>        inet6 fe80::42:acff:fe11:2  prefixlen<span class="hljs-number"> 64 </span> scopeid 0x20&lt;link&gt;<br>        ether 02:42:ac:11:00:02  txqueuelen<span class="hljs-number"> 0 </span> (Ethernet)<br>        RX packets<span class="hljs-number"> 364 </span> bytes<span class="hljs-number"> 8137175 </span>(7.7 MiB)<br>        RX errors<span class="hljs-number"> 0 </span> dropped<span class="hljs-number"> 0 </span> overruns<span class="hljs-number"> 0 </span> frame 0<br>        TX packets<span class="hljs-number"> 281 </span> bytes<span class="hljs-number"> 21161 </span>(20.6 KiB)<br>        TX errors<span class="hljs-number"> 0 </span> dropped<span class="hljs-number"> 0 </span>overruns<span class="hljs-number"> 0 </span> carrier<span class="hljs-number"> 0 </span> collisions 0<br><br>lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536<br>        inet 127.0.0.1  netmask 255.0.0.0<br>        inet6 ::1  prefixlen<span class="hljs-number"> 128 </span> scopeid 0x10&lt;host&gt;<br>        loop  txqueuelen<span class="hljs-number"> 1000 </span> (Local Loopback)<br>        RX packets<span class="hljs-number"> 0 </span> bytes<span class="hljs-number"> 0 </span>(0.0 B)<br>        RX errors<span class="hljs-number"> 0 </span> dropped<span class="hljs-number"> 0 </span> overruns<span class="hljs-number"> 0 </span> frame 0<br>        TX packets<span class="hljs-number"> 0 </span> bytes<span class="hljs-number"> 0 </span>(0.0 B)<br>        TX errors<span class="hljs-number"> 0 </span> dropped<span class="hljs-number"> 0 </span>overruns<span class="hljs-number"> 0 </span> carrier<span class="hljs-number"> 0 </span> collisions 0<br><br>$ route<br>Kernel IP routing table<br>Destination     Gateway         Genmask         Flags Metric Ref    Use Iface<br>default         172.17.0.1      0.0.0.0         UG   <span class="hljs-number"> 0 </span>    <span class="hljs-number"> 0 </span>      <span class="hljs-number"> 0 </span>eth0<br>172.17.0.0      0.0.0.0         255.255.0.0     U    <span class="hljs-number"> 0 </span>    <span class="hljs-number"> 0 </span>      <span class="hljs-number"> 0 </span>eth0<br><br></code></pre></td></tr></table></figure><p>可以看到，这个容器里有一张叫作eth0的网卡，它正是一个Veth Pair设备在容器里的这一端。</p><p>通过route命令查看nginx-1容器的路由表，我们可以看到，这个eth0网卡是这个容器里的默认路由设备；所有对172.17.0.0&#x2F;16网段的请求，也会被交给eth0来处理（第二条172.17.0.0路由规则）。</p><p>而这个Veth Pair设备的另一端，则在宿主机上。你可以通过查看宿主机的网络设备看到它，如下所示：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs vim"># 在宿主机上<br>$ ifconfig<br>...<br>docker0   Link encap:Ethernet  HWaddr <span class="hljs-number">02</span>:<span class="hljs-number">42</span>:d8:e4:df:c1<br>          inet addr:<span class="hljs-number">172.17</span>.<span class="hljs-number">0.1</span>  Bcas<span class="hljs-variable">t:0</span>.<span class="hljs-number">0.0</span>.<span class="hljs-number">0</span>  Mask:<span class="hljs-number">255.255</span>.<span class="hljs-number">0.0</span><br>          inet6 addr: fe80::<span class="hljs-number">42</span>:d8ff:fee4:dfc1/<span class="hljs-number">64</span> Scope:Link<br>          UP BROADCAST RUNNING MULTICAST  MTU:<span class="hljs-number">1500</span>  Metric:<span class="hljs-number">1</span><br>          RX packet<span class="hljs-variable">s:309</span> error<span class="hljs-variable">s:0</span> dropped:<span class="hljs-number">0</span> overrun<span class="hljs-variable">s:0</span> frame:<span class="hljs-number">0</span><br>          TX packet<span class="hljs-variable">s:372</span> error<span class="hljs-variable">s:0</span> dropped:<span class="hljs-number">0</span> overrun<span class="hljs-variable">s:0</span> carrier:<span class="hljs-number">0</span><br> collision<span class="hljs-variable">s:0</span> txqueuelen:<span class="hljs-number">0</span><br>          RX byte<span class="hljs-variable">s:18944</span> (<span class="hljs-number">18.9</span> KB)  TX byte<span class="hljs-variable">s:8137789</span> (<span class="hljs-number">8.1</span> MB)<br>veth9c02e56 Link encap:Ethernet  HWaddr <span class="hljs-number">52</span>:<span class="hljs-number">81</span>:<span class="hljs-number">0</span><span class="hljs-variable">b:24</span>:<span class="hljs-number">3</span>d:da<br>          inet6 addr: fe80::<span class="hljs-number">5081</span>:bff:fe24:<span class="hljs-number">3</span>dda/<span class="hljs-number">64</span> Scope:Link<br>          UP BROADCAST RUNNING MULTICAST  MTU:<span class="hljs-number">1500</span>  Metric:<span class="hljs-number">1</span><br>          RX packet<span class="hljs-variable">s:288</span> error<span class="hljs-variable">s:0</span> dropped:<span class="hljs-number">0</span> overrun<span class="hljs-variable">s:0</span> frame:<span class="hljs-number">0</span><br>          TX packet<span class="hljs-variable">s:371</span> error<span class="hljs-variable">s:0</span> dropped:<span class="hljs-number">0</span> overrun<span class="hljs-variable">s:0</span> carrier:<span class="hljs-number">0</span><br> collision<span class="hljs-variable">s:0</span> txqueuelen:<span class="hljs-number">0</span><br>          RX byte<span class="hljs-variable">s:21608</span> (<span class="hljs-number">21.6</span> KB)  TX byte<span class="hljs-variable">s:8137719</span> (<span class="hljs-number">8.1</span> MB)<br><br>$ brctl show<br>bridge name bridge id  STP enabled interfaces<br>docker0  <span class="hljs-number">8000.0242</span>d8e4dfc1 <span class="hljs-keyword">no</span>  veth9c02e56<br><br></code></pre></td></tr></table></figure><p>通过ifconfig命令的输出，可以看到，nginx-1容器对应的Veth Pair设备，在宿主机上是一张虚拟网卡。它的名字叫作veth9c02e56。并且，通过brctl show的输出，可以看到这张网卡被“插”在了docker0上。</p><p>这时候，如果再在这台宿主机上启动另一个Docker容器，比如nginx-2：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs routeros">$ docker <span class="hljs-built_in">run</span> –d --name nginx-2 nginx<br>$ brctl show<span class="hljs-built_in"></span><br><span class="hljs-built_in">bridge </span>name<span class="hljs-built_in"> bridge </span>id  STP enabled interfaces<br>docker0  8000.0242d8e4dfc1 <span class="hljs-literal">no</span>  veth9c02e56<br>       vethb4963f3<br><br></code></pre></td></tr></table></figure><p>就会发现一个新的、名叫vethb4963f3的虚拟网卡，也被“插”在了docker0网桥上。</p><p>这时候，如果在nginx-1容器里ping一下nginx-2容器的IP地址（172.17.0.3），就会发现同一宿主机上的两个容器默认就是相互连通的。</p><p>这其中的原理其实非常简单。</p><p>当你在nginx-1容器里访问nginx-2容器的IP地址（比如ping 172.17.0.3）的时候，这个目的IP地址会匹配到nginx-1容器里的第二条路由规则。可以看到，这条路由规则的网关（Gateway）是0.0.0.0，这就意味着这是一条直连规则，即：凡是匹配到这条规则的IP包，应该经过本机的eth0网卡，通过二层网络直接发往目的主机。</p><p>而要通过二层网络到达nginx-2容器，就需要有172.17.0.3这个IP地址对应的MAC地址。所以nginx-1容器的网络协议栈，就需要通过eth0网卡发送一个ARP广播，来通过IP地址查找对应的MAC地址。</p><blockquote><p>备注：ARP（Address Resolution Protocol），是通过三层的IP地址找到对应的二层MAC地址的协议。</p></blockquote><p>这个eth0网卡，是一个Veth Pair，它的一端在这个nginx-1容器的Network Namespace里，而另一端则位于宿主机上（Host Namespace），并且被“插”在了宿主机的docker0网桥上。</p><p>一旦一张虚拟网卡被“插”在网桥上，它就会变成该网桥的“从设备”。从设备会被“剥夺”调用网络协议栈处理数据包的资格，从而“降级”成为网桥上的一个端口。而这个端口唯一的作用，就是接收流入的数据包，然后把这些数据包的“生杀大权”（比如转发或者丢弃），全部交给对应的网桥。</p><p>所以，在收到这些ARP请求之后，docker0网桥就会扮演二层交换机的角色，把ARP广播转发到其他被“插”在docker0上的虚拟网卡上。这样，同样连接在docker0上的nginx-2容器的网络协议栈就会收到这个ARP请求，从而将172.17.0.3所对应的MAC地址回复给nginx-1容器。</p><p>有了这个目的MAC地址，nginx-1容器的eth0网卡就可以将数据包发出去。</p><p>而根据Veth Pair设备的原理，这个数据包会立刻出现在宿主机上的veth9c02e56虚拟网卡上。不过，此时这个veth9c02e56网卡的网络协议栈的资格已经被“剥夺”，所以这个数据包就直接流入到了docker0网桥里。</p><p>docker0处理转发的过程，则继续扮演二层交换机的角色。此时，docker0网桥根据数据包的目的MAC地址（也就是nginx-2容器的MAC地址），在它的CAM表（即交换机通过MAC地址学习维护的端口和MAC地址的对应表）里查到对应的端口（Port）为：vethb4963f3，然后把数据包发往这个端口。</p><p>而这个端口，正是nginx-2容器“插”在docker0网桥上的另一块虚拟网卡，当然，它也是一个Veth Pair设备。这样，数据包就进入到了nginx-2容器的Network Namespace里。</p><p>所以，nginx-2容器看到的情况是，它自己的eth0网卡上出现了流入的数据包。这样，nginx-2的网络协议栈就会对请求进行处理，最后将响应（Pong）返回到nginx-1。</p><p>以上，就是同一个宿主机上的不同容器通过docker0网桥进行通信的流程了。我把这个流程总结成了一幅示意图，如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/e0/66/e0d28e0371f93af619e91a86eda99a66.png?wh=1715*995" alt="img"></p><p>需要注意的是，在实际的数据传递时，上述数据的传递过程在网络协议栈的不同层次，都有Linux内核Netfilter参与其中。所以，可以通过打开iptables的TRACE功能查看到数据包的传输过程，具体方法如下所示：</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs gauss"><span class="hljs-meta"># 在宿主机上执行</span><br>$ iptables -t raw -A <span class="hljs-keyword">OUTPUT</span> -p icmp -j <span class="hljs-keyword">TRACE</span><br>$ iptables -t raw -A PREROUTING -p icmp -j <span class="hljs-keyword">TRACE</span><br><br></code></pre></td></tr></table></figure><p>通过上述设置，就可以在&#x2F;var&#x2F;log&#x2F;syslog里看到数据包传输的日志了。这一部分内容，可以结合 <a href="https://en.wikipedia.org/wiki/Iptables">iptables的相关知识</a> 进行实践，从而验证数据包传递流程。</p><p>熟悉了docker0网桥的工作方式，就可以理解，在默认情况下， <strong>被限制在Network Namespace里的容器进程，实际上是通过Veth Pair设备+宿主机网桥的方式，实现了跟同其他容器的数据交换。</strong></p><p>与之类似地，当在一台宿主机上，访问该宿主机上的容器的IP地址时，这个请求的数据包，也是先根据路由规则到达docker0网桥，然后被转发到对应的Veth Pair设备，最后出现在容器里。这个过程的示意图，如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/9f/01/9fb381d1e49318bb6a67bda3f9db6901.png?wh=1715*995" alt="img"></p><p>同样地，当一个容器试图连接到另外一个宿主机时，比如：ping 10.168.0.3，它发出的请求数据包，首先经过docker0网桥出现在宿主机上。然后根据宿主机的路由表里的直连路由规则（10.168.0.0&#x2F;24 via eth0)），对10.168.0.3的访问请求就会交给宿主机的eth0处理。</p><p>所以接下来，这个数据包就会经宿主机的eth0网卡转发到宿主机网络上，最终到达10.168.0.3对应的宿主机上。当然，这个过程的实现要求这两台宿主机本身是连通的。这个过程的示意图，如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/90/95/90bd630c0723ea8a1fb7ccd738ad1f95.png?wh=1834*994" alt="img"></p><p>所以说， <strong>当遇到容器连不通“外网”的时候，都应该先试试docker0网桥能不能ping通，然后查看一下跟docker0和Veth Pair设备相关的iptables规则是不是有异常，往往就能够找到问题的答案了。</strong></p><p>不过，在最后一个“Docker容器连接其他宿主机”的例子里，如果在另外一台宿主机（比如：10.168.0.3）上，也有一个Docker容器。那么，nginx-1容器又该如何访问它呢？</p><p>这个问题，其实就是容器的“<strong>跨主通信</strong>”问题。</p><p>在Docker的默认配置下，一台宿主机上的docker0网桥，和其他宿主机上的docker0网桥，没有任何关联，它们互相之间也没办法连通。所以，连接在这些网桥上的容器，自然也没办法进行通信了。</p><p>不过，万变不离其宗。</p><p>如果我们通过软件的方式，创建一个整个集群“公用”的网桥，然后把集群里的所有容器都连接到这个网桥上，不就可以相互通信了吗？</p><p>说得没错。</p><p>这样一来，我们整个集群里的容器网络就会类似于下图所示的样子：</p><p><img src="https://static001.geekbang.org/resource/image/b4/3d/b4387a992352109398a66d1dbe6e413d.png?wh=1828*721" alt="img"></p><p>可以看到，构建这种容器网络的核心在于：我们需要在已有的宿主机网络上，再通过软件构建一个覆盖在已有宿主机网络之上的、可以把所有容器连通在一起的虚拟网络。所以，这种技术就被称为：Overlay Network（覆盖网络）。</p><p>而这个Overlay Network本身，可以由每台宿主机上的一个“特殊网桥”共同组成。比如，当Node 1上的Container 1要访问Node 2上的Container 3的时候，Node 1上的“特殊网桥”在收到数据包之后，能够通过某种方式，把数据包发送到正确的宿主机，比如Node 2上。而Node 2上的“特殊网桥”在收到数据包后，也能够通过某种方式，把数据包转发给正确的容器，比如Container 3。</p><p>甚至，每台宿主机上，都不需要有一个这种特殊的网桥，而仅仅通过某种方式配置宿主机的路由表，就能够把数据包转发到正确的宿主机上。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在今天这篇文章中，介绍了在本地环境下，单机容器网络的实现原理和docker0网桥的作用。</p><p>这里的关键在于，容器要想跟外界进行通信，它发出的IP包就必须从它的Network Namespace里出来，来到宿主机上。</p><p>而解决这个问题的方法就是：为容器创建一个一端在容器里充当默认网卡、另一端在宿主机上的Veth Pair设备。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><p>尽管容器的Host Network模式有一些缺点，但是它性能好、配置简单，并且易于调试，所以很多团队会直接使用Host Network。那么，如果要在生产环境中使用容器的Host Network模式，需要做哪些额外的准备工作？</p><p>使用host网络，要提前规划好每个服务应该使用的端口吧</p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分布式存储系统核心知识点</title>
    <link href="/2022/10/16/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F%E6%A0%B8%E5%BF%83%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
    <url>/2022/10/16/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F%E6%A0%B8%E5%BF%83%E7%9F%A5%E8%AF%86%E7%82%B9/</url>
    
    <content type="html"><![CDATA[<h1 id="分布式存储系统核心知识点"><a href="#分布式存储系统核心知识点" class="headerlink" title="分布式存储系统核心知识点"></a>分布式存储系统核心知识点</h1><p><img src="https://static001.geekbang.org/resource/image/7b/28/7b54b6ca9134c130bf3940c7db497928.png?wh=1920*1177" alt="img"></p>]]></content>
    
    
    <categories>
      
      <category>分布式存储</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>分布式存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>24.运维：如何构建高可靠的etcd集群运维体系？</title>
    <link href="/2022/10/16/24-%E8%BF%90%E7%BB%B4%EF%BC%9A%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E9%AB%98%E5%8F%AF%E9%9D%A0%E7%9A%84etcd%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4%E4%BD%93%E7%B3%BB%EF%BC%9F/"/>
    <url>/2022/10/16/24-%E8%BF%90%E7%BB%B4%EF%BC%9A%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E9%AB%98%E5%8F%AF%E9%9D%A0%E7%9A%84etcd%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4%E4%BD%93%E7%B3%BB%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="24-运维：如何构建高可靠的etcd集群运维体系？"><a href="#24-运维：如何构建高可靠的etcd集群运维体系？" class="headerlink" title="24.运维：如何构建高可靠的etcd集群运维体系？"></a>24.运维：如何构建高可靠的etcd集群运维体系？</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><p>在使用etcd过程中，我们经常会面临着一系列问题与选择，比如：</p><ul><li>etcd是使用虚拟机还是容器部署，各有什么优缺点？</li><li>如何及时发现etcd集群隐患项（比如数据不一致）？</li><li>如何及时监控及告警etcd的潜在隐患（比如db大小即将达到配额）？</li><li>如何优雅的定时、甚至跨城备份etcd数据？</li><li>如何模拟磁盘IO等异常来复现Bug、故障？</li></ul><p>今天聊聊如何解决以上问题。接下来将通过从etcd集群部署、集群组建、监控体系、巡检、备份及还原、高可用、混沌工程等维度，带你了解如何构建一个高可靠的etcd集群运维体系。</p><h2 id="整体解决方案"><a href="#整体解决方案" class="headerlink" title="整体解决方案"></a>整体解决方案</h2><p>那要如何构建高可靠的etcd集群运维体系呢?</p><p>通过下面这个思维脑图给你总结了etcd运维体系建设核心要点，它由etcd集群部署、成员管理、监控及告警体系、备份及还原、巡检、高可用及自愈、混沌工程等维度组成。</p><p><img src="https://static001.geekbang.org/resource/image/80/c2/803b20362b21d13396ee099f413968c2.png?wh=1920*1409" alt="img"></p><h2 id="集群部署"><a href="#集群部署" class="headerlink" title="集群部署"></a>集群部署</h2><p>要想使用etcd集群，我们面对的第一个问题是如何选择合适的方案去部署etcd集群。</p><p>首先是计算资源的选择，它本质上就是计算资源的交付演进史，分别如下：</p><ul><li>物理机；</li><li>虚拟机；</li><li>裸容器（如Docker实例）；</li><li>Kubernetes容器编排。</li></ul><p><strong>物理机资源交付慢、成本高、扩缩容流程费时</strong>，<strong>一般情况下大部分业务团队不再考虑物理机</strong>，除非是超大规模的上万个节点的Kubernetes集群，对CPU、内存、网络资源有着极高诉求。</p><p>虚拟机是目前各个云厂商售卖的主流实例，无论是基于KVM还是Xen实现，都具有良好的稳定性、隔离性，支持故障热迁移，可弹性伸缩，被etcd、数据库等存储业务大量使用。</p><p>在基于物理机和虚拟机的部署方案中，可以使用ansible、puppet等自动运维工具，构建标准、自动化的etcd集群搭建、扩缩容流程。基于ansible部署etcd集群可以拆分成以下若干个任务:</p><ul><li>下载及安装etcd二进制到指定目录；</li><li>将etcd加入systemd等服务管理；</li><li>为etcd增加配置文件，合理设置相关参数；</li><li>为etcd集群各个节点生成相关证书，构建一个安全的集群；</li><li>组建集群版（静态配置、动态配置，发现集群其他节点）；</li><li>开启etcd服务，启动etcd集群。</li></ul><p>详细可以参考digitalocean <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-and-secure-an-etcd-cluster-with-ansible-on-ubuntu-18-04">这篇博客文章</a>，它介绍了如何使用ansible去部署一个安全的etcd集群，并给出了对应的yaml任务文件。</p><p><strong>容器化部署则具有极速的交付效率、更灵活的资源控制、更低的虚拟化开销等一系列优点</strong>。自从Docker诞生后，容器化部署就风靡全球。有的业务直接使用裸Docker容器来跑etcd集群。然而裸Docker容器不具备调度、故障自愈、弹性扩容等特性，存在较大局限性。</p><p>随后为了解决以上问题，诞生了以Kubernetes、Swarm为首的容器编排平台，Kubernetes成为了容器编排之战中的王者，大量业务使用Kubernetes来部署etcd、ZooKeeper等有状态服务。在开源社区中，也诞生了若干个etcd的Kubernetes容器化解决方案，分别如下：</p><ul><li>etcd-operator；</li><li>bitnami etcd&#x2F;statefulset；</li><li>etcd-cluster-operator；</li><li>openshit&#x2F;cluster-etcd-operator；</li><li>kubeadm。</li></ul><p><a href="https://github.com/coreos/etcd-operator">etcd-operator</a> 目前已处于Archived状态，无人维护，基本废弃。同时它是基于裸Pod实现的，要做好各种备份。在部分异常情况下存在集群宕机、数据丢失风险，我仅建议你使用它的数据备份etcd-backup-operator。</p><p><strong><a href="https://bitnami.com/stack/etcd/helm">bitnami etcd</a> 提供了一个helm包一键部署etcd集群，支持各个云厂商，支持使用PV、PVC持久化存储数据，底层基于StatefulSet实现，较稳定。目前不少开源项目使用的是它。</strong></p><p>可以通过如下helm命令，快速在Kubernete集群中部署一个etcd集群。</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs arduino">helm repo add bitnami https:<span class="hljs-comment">//charts.bitnami.com/bitnami</span><br>helm install my-release bitnami/etcd<br><br></code></pre></td></tr></table></figure><p><a href="https://github.com/improbable-eng/etcd-cluster-operator">etcd-cluster-operator</a> 和openshit&#x2F; <a href="https://github.com/openshift/cluster-etcd-operator">cluster-etcd-operator</a> 比较小众，目前star不多，但是有相应的开发者维护，你可参考下它们的实现思路，与etcd-operator基于Pod、bitnami etcd基于Statefulset实现不一样的是，它们是基于ReplicaSet和Static Pod实现的。</p><p><strong>最后介绍的是 <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/">kubeadm</a>，它是Kubernetes集群中的etcd高可用部署方案的提供者，kubeadm是基于Static Pod部署etcd集群的。Static Pod相比普通Pod有其特殊性，它是直接由节点上的kubelet进程来管理，无需通过kube-apiserver。</strong></p><p>创建Static Pod方式有两种，分别是配置文件和HTTP。kubeadm使用的是配置文件，也就是在kubelet监听的静态Pod目录下（一般是&#x2F;etc&#x2F;kubernetes&#x2F;manifests）放置相应的etcd Pod YAML文件即可，如下图所示。</p><p><img src="https://static001.geekbang.org/resource/image/d7/05/d7c28814d3f83ff4ef474df72b10b305.png?wh=1920*1007" alt="img"></p><p>注意在这种部署方式中，部署etcd的节点需要部署docker、kubelet、kubeadm组件，依赖较重。</p><h2 id="集群组建"><a href="#集群组建" class="headerlink" title="集群组建"></a>集群组建</h2><p>聊完etcd集群部署的几种模式和基本原理后，我们接下来看看在实际部署过程中最棘手的部分，那就是集群组建。因为集群组建涉及到etcd成员管理的原理和节点发现机制。</p><p>要特别注意，当变更集群成员节点时，节点的initial-cluster-state参数的取值可以是new或existing。</p><ul><li>new，一般用于初始化启动一个新集群的场景。当设置成new时，它会根据initial-cluster-token、initial-cluster等参数信息计算集群ID、成员ID信息。</li><li>existing，表示etcd节点加入一个已存在的集群，它会根据peerURLs信息从Peer节点获取已存在的集群ID信息，更新自己本地配置、并将本身节点信息发布到集群中。</li></ul><p>那么当要组建一个三节点的etcd集群的时候，有哪些方法呢?</p><p>在etcd中，无论是Leader选举还是日志同步，都涉及到与其他节点通信。因此组建集群的第一步得知道集群总成员数、各个成员节点的IP地址等信息。</p><p>这个过程就是发现（Discovery）。目前etcd主要通过两种方式来获取以上信息，分别是 <strong>static configuration</strong> 和 <strong>dynamic service discovery</strong>。</p><p><strong>static configuration</strong> 是指集群总成员节点数、成员节点的IP地址都是已知、固定的，根据我们上面介绍的initial-cluster-state原理，有如下两个方法可基于静态配置组建一个集群。</p><ul><li>方法1，三个节点的initial-cluster-state都配置为new，静态启动，initial-cluster参数包含三个节点信息即可，详情你可参考 <a href="https://etcd.io/docs/v3.4.0/op-guide/clustering/">社区文档</a>。</li><li>方法2，第一个节点initial-cluster-state设置为new，独立成集群，随后第二和第三个节点都为existing，通过扩容的方式，不断加入到第一个节点所组成的集群中。</li></ul><p>如果成员节点信息是未知的，你可以通过 <strong>dynamic service discovery</strong> 机制解决。</p><p>etcd社区还提供了通过公共服务来发现成员节点信息，组建集群的方案。它的核心是集群内的各个成员节点向公共服务注册成员地址等信息，各个节点通过公共服务来发现彼此，可以参考 <a href="https://etcd.io/docs/v3.4.0/dev-internal/discovery_protocol/">官方详细文档。</a></p><h2 id="监控及告警体系"><a href="#监控及告警体系" class="headerlink" title="监控及告警体系"></a>监控及告警体系</h2><p>当我们把集群部署起来后，在业务开始使用之前，部署监控是必不可少的一个环节，它是保障业务稳定性，提前发现风险、隐患点的重要核心手段。那么要如何快速监控你的etcd集群呢？</p><p>正如之前在14和15里和你介绍延时、内存时所提及的，etcd提供了丰富的metrics来展示整个集群的核心指标、健康度。metrics按模块可划分为磁盘、网络、MVCC事务、gRPC RPC、etcdserver。</p><p>磁盘相关的metrics及含义如下图所示。</p><p><img src="https://static001.geekbang.org/resource/image/7b/a5/7b3df60d26f5363e36100525a44472a5.png?wh=1920*616" alt="img"></p><p>网络相关的metrics及含义如下图所示。</p><p><img src="https://static001.geekbang.org/resource/image/da/32/da489a9796a016dc2yy99e101d9ab832.png?wh=1920*529" alt="img"></p><p>mvcc相关的较多，我在下图中列举了部分其含义，如下所示。</p><p><img src="https://static001.geekbang.org/resource/image/d1/51/d17446f657b110afd874yyea87176051.png?wh=1920*564" alt="img"></p><p>etcdserver相关的如下，集群是否有leader、堆积的proposal数等都在此模块。</p><p><img src="https://static001.geekbang.org/resource/image/cb/6e/cbb95c525a6748bfaee48e95ca622f6e.png?wh=1920*759" alt="img"></p><p>更多metrics，可以通过如下方法查看。</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">curl</span> <span class="hljs-number">127.0.0.1:2379</span>/metrics<br><br></code></pre></td></tr></table></figure><p>了解常见的metrics后，我们只需要配置Prometheus服务，采集etcd集群的2379端口的metrics路径。</p><p>采集的方案一般有两种， <a href="https://etcd.io/docs/v3.4.0/op-guide/monitoring/">静态配置</a> 和动态配置。</p><p>静态配置是指添加待监控的etcd target到Prometheus配置文件，如下所示。</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs 1c">global:<br>  scrape_interval: <span class="hljs-number">10</span>s<br>scrape_configs:<br>  - job_name: test-etcd<br>    static_configs:<br>    - targets:<br> [&#x27;10.240.0.32:<span class="hljs-number">2379</span>&#x27;,&#x27;10.240.0.33:<span class="hljs-number">2379</span>&#x27;,&#x27;10.240.0.34:<span class="hljs-number">2379</span>&#x27;]<br><br></code></pre></td></tr></table></figure><p>静态配置的缺点是每次新增集群、成员变更都需要人工修改配置，而动态配置就可解决这个痛点。</p><p>动态配置是通过Prometheus-Operator的提供ServiceMonitor机制实现的，当想采集一个etcd实例时，若etcd服务部署在同一个Kubernetes集群，只需要通过Kubernetes的API创建一个如下的ServiceMonitor资源即可。若etcd集群与Promehteus-Operator不在同一个集群，你需要去创建、更新对应的集群Endpoint。</p><p>那Prometheus是如何知道该采集哪些服务的metrics信息呢?</p><p>答案ServiceMonitor资源通过Namespace、Labels描述了待采集实例对应的Service Endpoint。</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">apiVersion:</span> monitoring.coreos.com/v1<br><span class="hljs-symbol">kind:</span> ServiceMonitor<br><span class="hljs-symbol">metadata:</span><br><span class="hljs-symbol">  name:</span> prometheus-prometheus-oper-kube-etcd<br><span class="hljs-symbol">  namespace:</span> monitoring<br><span class="hljs-symbol">spec:</span><br><span class="hljs-symbol">  endpoints:</span><br>  - bearerTokenFile: <span class="hljs-keyword">/var/</span>run<span class="hljs-keyword">/secrets/</span>kubernetes.io<span class="hljs-keyword">/serviceaccount/</span>token<br><span class="hljs-symbol">    port:</span> http-metrics<br><span class="hljs-symbol">    scheme:</span> https<br><span class="hljs-symbol">    tlsConfig:</span><br><span class="hljs-symbol">      caFile:</span> <span class="hljs-keyword">/etc/</span>prometheus<span class="hljs-keyword">/secrets/</span>etcd-certs/ca.crt<br><span class="hljs-symbol">      certFile:</span> <span class="hljs-keyword">/etc/</span>prometheus<span class="hljs-keyword">/secrets/</span>etcd-certs/client.crt<br><span class="hljs-symbol">      insecureSkipVerify:</span> true<br><span class="hljs-symbol">      keyFile:</span> <span class="hljs-keyword">/etc/</span>prometheus<span class="hljs-keyword">/secrets/</span>etcd-certs/client.key<br><span class="hljs-symbol">  jobLabel:</span> jobLabel<br><span class="hljs-symbol">  namespaceSelector:</span><br><span class="hljs-symbol">    matchNames:</span><br>    - kube-system<br><span class="hljs-symbol">  selector:</span><br><span class="hljs-symbol">    matchLabels:</span><br><span class="hljs-symbol">      app:</span> prometheus-operator-kube-etcd<br><span class="hljs-symbol">      release:</span> prometheus<br><br></code></pre></td></tr></table></figure><p>采集了metrics监控数据后，下一步就是要基于metrics监控数据告警了。可以通过Prometheus和 <a href="https://github.com/prometheus/alertmanager">Alertmanager</a> 组件实现，那你应该为哪些核心指标告警呢？</p><p>当然是影响集群可用性的最核心的metric。比如是否有Leader、Leader切换次数、WAL和事务操作延时。etcd社区提供了一个 <a href="https://github.com/etcd-io/etcd/blob/v3.4.9/Documentation/op-guide/etcd3_alert.rules">丰富的告警规则</a>，你可以参考下。</p><p>最后，为了方便查看etcd集群运行状况和提升定位问题的效率，可以基于采集的metrics配置个 <a href="https://github.com/etcd-io/etcd/blob/v3.4.9/Documentation/op-guide/grafana.json">grafana可视化面板</a>。下面列出了集群是否有Leader、总的key数、总的watcher数、出流量、WAL持久化延时的可视化面板。</p><p><img src="https://static001.geekbang.org/resource/image/a3/9f/a3b42d1e81dd706897edf32ecbc65f9f.png?wh=848*482" alt="img"></p><p><img src="https://static001.geekbang.org/resource/image/d3/7d/d3bc1f984ea8b2e301471ef2923d1b7d.png?wh=1306*366" alt="img"><img src="https://static001.geekbang.org/resource/image/yy/9f/yy73b00dd4d48d473c1d900c96dd0a9f.png?wh=1308*358" alt="img"></p><p><img src="https://static001.geekbang.org/resource/image/2d/25/2d28317yyc38957ae2125e460b83f825.png?wh=1282*362" alt="img"></p><p><img src="https://static001.geekbang.org/resource/image/9c/b9/9c471d05b1452c4f0aa8yy24c79915b9.png?wh=1268*418" alt="img"></p><h2 id="备份及还原"><a href="#备份及还原" class="headerlink" title="备份及还原"></a>备份及还原</h2><p>监控及告警就绪后，就可以提供给业务在生产环境使用了吗？</p><p>当然不行，数据是业务的安全红线，所以还需要做好最核心的数据备份工作。</p><p>如何做呢？</p><p>主要有以下方法，首先是通过etcdctl snapshot命令行人工备份。在发起重要变更的时候，你可以通过如下命令进行备份，并查看快照状态。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-attribute">ETCDCTL_API</span>=3 etcdctl --endpoints <span class="hljs-variable">$ENDPOINT</span><br>snapshot save snapshotdb<br><span class="hljs-attribute">ETCDCTL_API</span>=3 etcdctl <span class="hljs-attribute">--write-out</span>=table snapshot status snapshotdb<br><br></code></pre></td></tr></table></figure><p>其次是通过定时任务进行定时备份，建议至少每隔1个小时备份一次。</p><p>然后是通过 <a href="https://github.com/coreos/etcd-operator/blob/master/doc/user/walkthrough/backup-operator.md#:~:text=etcd%20backup%20operator%20backs%20up,storage%20such%20as%20AWS%20S3.">etcd-backup-operator</a> 进行自动化的备份，类似ServiceMonitor，你可以通过创建一个备份任务CRD实现。CRD如下：</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">apiVersion:</span> <span class="hljs-string">&quot;etcd.database.coreos.com/v1beta2&quot;</span><br><span class="hljs-symbol">kind:</span> <span class="hljs-string">&quot;EtcdBackup&quot;</span><br><span class="hljs-symbol">metadata:</span><br><span class="hljs-symbol">  name:</span> example-etcd-cluster-periodic-backup<br><span class="hljs-symbol">spec:</span><br><span class="hljs-symbol">  etcdEndpoints:</span> [<span class="hljs-params">&lt;etcd-cluster-endpoints&gt;</span>]<br><span class="hljs-symbol">  storageType:</span> S3<br><span class="hljs-symbol">  backupPolicy:</span><br>    <span class="hljs-meta"># 0 &gt; enable periodic backup</span><br><span class="hljs-symbol">    backupIntervalInSecond:</span> <span class="hljs-number">125</span><br><span class="hljs-symbol">    maxBackups:</span> <span class="hljs-number">4</span><br><span class="hljs-symbol">  s3:</span><br>    <span class="hljs-meta"># The format of <span class="hljs-string">&quot;path&quot;</span> must be: <span class="hljs-string">&quot;&lt;s3-bucket-name&gt;/&lt;path-to-backup-file&gt;&quot;</span></span><br>    <span class="hljs-meta"># e.g: <span class="hljs-string">&quot;mybucket/etcd.backup&quot;</span></span><br><span class="hljs-symbol">    path:</span> <span class="hljs-params">&lt;full-s3-path&gt;</span><br><span class="hljs-symbol">    awsSecret:</span> <span class="hljs-params">&lt;aws-secret&gt;</span><br><br></code></pre></td></tr></table></figure><p>最后可以通过给etcd集群增加Learner节点，实现跨地域热备。因Learner节点属于非投票成员的节点，因此它并不会影响你集群的性能。它的基本工作原理是当Leader收到写请求时，它会通过Raft模块将日志同步给Learner节点。你需要注意的是，在etcd 3.4中目前只支持1个Learner节点，并且只允许串行读。</p><h2 id="巡检"><a href="#巡检" class="headerlink" title="巡检"></a>巡检</h2><p>完成集群部署、了解成员管理、构建好监控及告警体系并添加好定时备份策略后，这时终于可以放心给业务使用了。然而在后续业务使用过程中，你可能会遇到各类问题，而这些问题很可能是metrics监控无法发现的，比如如下：</p><ul><li>etcd集群因重启进程、节点等出现数据不一致；</li><li>业务写入大 key-value 导致 etcd 性能骤降；</li><li>业务异常写入大量key数，稳定性存在隐患；</li><li>业务少数 key 出现写入 QPS 异常，导致 etcd 集群出现限速等错误；</li><li>重启、升级 etcd 后，需要人工从多维度检查集群健康度；</li><li>变更 etcd 集群过程中，操作失误可能会导致 etcd 集群出现分裂；</li></ul><p>……</p><p>因此为了实现高效治理etcd集群，我们可将这些潜在隐患总结成一个个自动化检查项，比如：</p><ul><li>如何高效监控 etcd 数据不一致性？</li><li>如何及时发现大 key-value?</li><li>如何及时通过监控发现 key 数异常增长？</li><li>如何及时监控异常写入 QPS?</li><li>如何从多维度的对集群进行自动化的健康检测，更安心变更？</li><li>……</li></ul><p>如何将这些 etcd 的最佳实践策略反哺到现网大规模 etcd 集群的治理中去呢？</p><p>答案就是<strong>巡检</strong>。</p><p>参考ServiceMonitor和EtcdBackup机制，同样可以通过CRD的方式描述此巡检任务，然后通过相应的Operator实现此巡检任务。比如下面就是一个数据一致性巡检的YAML文件，其对应的Operator组件会定时、并发检查其关联的etcd集群各个节点的key差异数。</p><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs vbnet"><span class="hljs-symbol">apiVersion:</span> etcd.cloud.tencent.com/v1beta1<br><span class="hljs-symbol">kind:</span> EtcdMonitor<br><span class="hljs-symbol">metadata:</span><br><span class="hljs-symbol">creationTimestamp:</span> <span class="hljs-string">&quot;2020-06-15T12:19:30Z&quot;</span><br><span class="hljs-symbol">generation:</span> <span class="hljs-number">1</span><br><span class="hljs-symbol">labels:</span><br><span class="hljs-symbol">clusterName:</span> gz-qcloud-etcd-<span class="hljs-number">03</span><br><span class="hljs-symbol">region:</span> gz<br><span class="hljs-symbol">source:</span> etcd-life-cycle-<span class="hljs-keyword">operator</span><br><span class="hljs-symbol">name:</span> gz-qcloud-etcd-<span class="hljs-number">03</span>-etcd-node-<span class="hljs-keyword">key</span>-diff<br><span class="hljs-symbol">namespace:</span> gz<br><span class="hljs-symbol">spec:</span><br><span class="hljs-symbol">clusterId:</span> gz-qcloud-etcd-<span class="hljs-number">03</span><br><span class="hljs-symbol">metricName:</span> etcd-node-<span class="hljs-keyword">key</span>-diff<br><span class="hljs-symbol">metricProviderName:</span> cruiser<br><span class="hljs-symbol">name:</span> gz-qcloud-etcd-<span class="hljs-number">03</span><br><span class="hljs-symbol">productName:</span> tke<br><span class="hljs-symbol">region:</span> gz<br><span class="hljs-symbol">status:</span><br><span class="hljs-symbol">records:</span><br>- endTime: <span class="hljs-string">&quot;2021-02-25T11:22:26Z&quot;</span><br><span class="hljs-symbol">message:</span> collectEtcdNodeKeyDiff,etcd cluster gz-qcloud-etcd-<span class="hljs-number">03</span>,total <span class="hljs-keyword">key</span> num <span class="hljs-built_in">is</span><br><span class="hljs-number">122143</span>,nodeKeyDiff <span class="hljs-built_in">is</span> <span class="hljs-number">0</span><br><span class="hljs-symbol">startTime:</span> <span class="hljs-string">&quot;2021-02-25T12:39:28Z&quot;</span><br><span class="hljs-symbol">updatedAt:</span> <span class="hljs-string">&quot;2021-02-25T12:39:28Z&quot;</span><br><br></code></pre></td></tr></table></figure><h2 id="高可用及自愈"><a href="#高可用及自愈" class="headerlink" title="高可用及自愈"></a>高可用及自愈</h2><p>通过以上机制，我们已经基本建设好一个高可用的etcd集群运维体系了。最后提供几个集群高可用及自愈的小建议：</p><ul><li>若etcd集群性能已满足业务诉求，可容忍一定的延时上升，建议将etcd集群做高可用部署，比如对3个节点来说，把每个节点部署在独立的可用区，可容忍任意一个可用区故障。</li><li>逐步尝试使用Kubernetes容器化部署etcd集群。当节点出现故障时，能通过Kubernetes的自愈机制，实现故障自愈。</li><li>设置合理的db quota值，配置合理的压缩策略，避免集群db quota满从而导致集群不可用的情况发生。</li></ul><h2 id="混沌工程"><a href="#混沌工程" class="headerlink" title="混沌工程"></a>混沌工程</h2><p>在使用etcd的过程中，你可能会遇到磁盘、网络、进程异常重启等异常导致的故障。如何快速复现相关故障进行问题定位呢？</p><p>答案就是混沌工程。一般常见的异常我们可以分为如下几类：</p><ul><li>磁盘IO相关的。比如模拟磁盘IO延时上升、IO操作报错。之前遇到的一个底层磁盘硬件异常导致IO延时飙升，最终触发了etcd死锁的Bug，我们就是通过模拟磁盘IO延时上升后来验证的。</li><li>网络相关的。比如模拟网络分区、网络丢包、网络延时、包重复等。</li><li>进程相关的。比如模拟进程异常被杀、重启等。之前遇到的一个非常难定位和复现的数据不一致Bug，我们就是通过注入进程异常重启等故障，最后成功复现。</li><li>压力测试相关的。比如模拟CPU高负载、内存使用率等。</li></ul><p>开源社区在混沌工程领域诞生了若干个优秀的混沌工程项目，如chaos-mesh、chaos-blade、litmus。这里重点介绍下 <a href="https://github.com/chaos-mesh/chaos-mesh">chaos-mesh</a>，它是基于Kubernetes实现的云原生混沌工程平台，下图是其架构图（引用自社区）。</p><p><img src="https://static001.geekbang.org/resource/image/b8/a7/b87d187ea2ab60d824223662fd6033a7.png?wh=1920*1287" alt="img"></p><p>为了实现以上异常场景的故障注入，chaos-mesh定义了若干种资源类型，分别如下：</p><ul><li>IOChaos，<strong>用于模拟文件系统相关的IO延时和读写错误等。</strong></li><li>NetworkChaos，<strong>用于模拟网络延时、丢包等。</strong></li><li>PodChaos，<strong>用于模拟业务Pod异常，比如Pod被杀、Pod内的容器重启等。</strong></li><li>StressChaos，<strong>用于模拟CPU和内存压力测试。</strong></li></ul><p>当你希望给etcd Pod注入一个磁盘IO延时的故障时，只需要创建此YAML文件就好。</p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">apiVersion</span><span class="hljs-punctuation">:</span> <span class="hljs-string">chaos-mesh.org/v1alpha1</span><br><span class="hljs-attribute">kind</span><span class="hljs-punctuation">:</span> <span class="hljs-string">IoChaos</span><br><span class="hljs-attribute">metadata</span><span class="hljs-punctuation">:</span><br>  <span class="hljs-attribute">name</span><span class="hljs-punctuation">:</span> <span class="hljs-string">io-delay-example</span><br><span class="hljs-attribute">spec</span><span class="hljs-punctuation">:</span><br>  <span class="hljs-attribute">action</span><span class="hljs-punctuation">:</span> <span class="hljs-string">latency</span><br>  <span class="hljs-attribute">mode</span><span class="hljs-punctuation">:</span> <span class="hljs-string">one</span><br>  <span class="hljs-attribute">selector</span><span class="hljs-punctuation">:</span><br>    <span class="hljs-attribute">labelSelectors</span><span class="hljs-punctuation">:</span><br>      <span class="hljs-attribute">app</span><span class="hljs-punctuation">:</span> <span class="hljs-string">etcd</span><br>  <span class="hljs-attribute">volumePath</span><span class="hljs-punctuation">:</span> <span class="hljs-string">/var/run/etcd</span><br>  <span class="hljs-attribute">path</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&#x27;/var/run/etcd/**/*&#x27;</span><br>  <span class="hljs-attribute">delay</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&#x27;100ms&#x27;</span><br>  <span class="hljs-attribute">percent</span><span class="hljs-punctuation">:</span> <span class="hljs-string">50</span><br>  <span class="hljs-attribute">duration</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&#x27;400s&#x27;</span><br>  <span class="hljs-attribute">scheduler</span><span class="hljs-punctuation">:</span><br>    <span class="hljs-attribute">cron</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&#x27;@every 10m&#x27;</span><br><br></code></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>首先通过从集群部署、集群组建、监控及告警体系、备份、巡检、高可用、混沌工程几个维度，深入介绍了如何构建一个高可靠的etcd集群运维体系。</p><p>在集群部署上，当你的业务集群规模非常大、对稳定性有着极高的要求时，推荐使用大规格、高性能的物理机、虚拟机独占部署，并且使用ansible等自动化运维工具，进行标准化的操作etcd，避免人工一个个修改操作。</p><p>对容器化部署来说，Kubernetes场景推荐你使用kubeadm，其他场景可考虑分批、逐步使用bitnami提供的etcd helm包，它是基于statefulset、PV、PVC实现的，各大云厂商都广泛支持，建议在生产环境前，多验证各个极端情况下的行为是否符合你的预期。</p><p>在集群组建上，各个节点需要一定机制去发现集群中的其他成员节点，主要可分为 <strong>static configuration</strong> 和 <strong>dynamic service discovery</strong>。</p><p>static configuration是指集群中各个成员节点信息是已知的，dynamic service discovery是指可以通过服务发现组件去注册自身节点信息、发现集群中其他成员节点信息。另外我和你介绍了重要参数initial-cluster-state的含义，它也是影响集群组建的一个核心因素。</p><p>在监控及告警体系上，介绍了etcd网络、磁盘、etcdserver、gRPC核心的metrics。通过修改Prometheues配置文件，添加etcd target，就可以方便的采集etcd的监控数据。然后介绍了ServiceMonitor机制，可通过它实现动态新增、删除、修改待监控的etcd实例，灵活的、高效的采集etcd Metrcis。</p><p>备份及还原上，重点介绍了etcd snapshot命令，etcd-backup-operator的备份任务CRD机制，推荐使用后者。</p><p>最后是巡检、混沌工程，它能帮助我们高效治理etcd集群，及时发现潜在隐患，低成本、快速的复现Bug和故障等。</p><p>简单介绍下：etcd一站式治理平台kstone. </p><p>Kstone 是一个针对 etcd 的全方位运维解决方案，提供集群管理（关联已有集群、创建新集群等)、监控、备份、巡检、数据迁移、数据可视化、智能诊断等一系列特性。 Kstone 将帮助你高效管理etcd集群，显著降低运维成本、及时发现潜在隐患、提升k8s etcd存储的稳定性和用户体验。 <a href="https://github.com/tkestack/kstone,%E3%80%82">https://github.com/tkestack/kstone,。</a></p><p><img src="https://static001.geekbang.org/resource/image/7b/28/7b54b6ca9134c130bf3940c7db497928.png?wh=1920*1177" alt="img"></p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>23.选型：etcd/ZooKeeper/Consul等我们该如何选择？</title>
    <link href="/2022/10/15/23-%E9%80%89%E5%9E%8B%EF%BC%9Aetcd-ZooKeeper-Consul%E7%AD%89%E6%88%91%E4%BB%AC%E8%AF%A5%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%EF%BC%9F/"/>
    <url>/2022/10/15/23-%E9%80%89%E5%9E%8B%EF%BC%9Aetcd-ZooKeeper-Consul%E7%AD%89%E6%88%91%E4%BB%AC%E8%AF%A5%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="23-选型：etcd-x2F-ZooKeeper-x2F-Consul等我们该如何选择？"><a href="#23-选型：etcd-x2F-ZooKeeper-x2F-Consul等我们该如何选择？" class="headerlink" title="23.选型：etcd&#x2F;ZooKeeper&#x2F;Consul等我们该如何选择？"></a>23.选型：etcd&#x2F;ZooKeeper&#x2F;Consul等我们该如何选择？</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><p>在软件开发过程中，当我们需要解决配置、服务发现、分布式锁等业务痛点，在面对 <a href="https://github.com/etcd-io/etcd">etcd</a>、 <a href="https://github.com/apache/zookeeper">ZooKeeper</a>、 <a href="https://github.com/hashicorp/consul">Consul</a>、 <a href="https://github.com/alibaba/nacos">Nacos</a> 等一系列候选开源项目时，我们应该如何结合自己的业务场景，选择合适的分布式协调服务呢？</p><p>今天聊聊主要分布式协调服务的对比。将从基本架构、共识算法、数据模型、重点特性、容灾能力等维度出发，了解主要分布式协调服务的基本原理和彼此之间的差异性。</p><h2 id="基本架构及原理"><a href="#基本架构及原理" class="headerlink" title="基本架构及原理"></a>基本架构及原理</h2><p>在详细和你介绍对比etcd、ZooKeeper、Consul特性之前，我们先从整体架构上来了解一下各开源项目的核心架构及原理。</p><h3 id="etcd架构及原理"><a href="#etcd架构及原理" class="headerlink" title="etcd架构及原理"></a>etcd架构及原理</h3><p>首先是etcd，etcd我们知道它是基于复制状态机实现的分布式协调服务。如下图所示，由Raft共识模块、日志模块、基于boltdb持久化存储的状态机组成。</p><p><img src="https://static001.geekbang.org/resource/image/5c/4f/5c7a3079032f90120a6b309ee401fc4f.png?wh=605*319" alt="img"></p><p>以下是etcd基于复制状态机模型的写请求流程：</p><ul><li>client发起一个写请求（put x &#x3D; 3）；</li><li>etcdserver模块向Raft共识模块提交请求，共识模块生成一个写提案日志条目。若server是Leader，则把日志条目广播给其他节点，并持久化日志条目到WAL中；</li><li>当一半以上节点持久化日志条目后，Leader的共识模块将此日志条目标记为已提交（committed），并通知其他节点提交；</li><li>etcdserver模块从Raft共识模块获取已经提交的日志条目，异步应用到boltdb状态机存储中，然后返回给client。</li></ul><h3 id="ZooKeeper架构及原理"><a href="#ZooKeeper架构及原理" class="headerlink" title="ZooKeeper架构及原理"></a>ZooKeeper架构及原理</h3><p>接下来简要介绍下 <a href="https://zookeeper.apache.org/doc/current/zookeeperOver.html">ZooKeeper</a> 原理，下图是它的架构图。</p><p>如下面架构图所示，你可以看到ZooKeeper中的节点与etcd类似，也划分为Leader节点、Follower节点、Observer节点（对应的Raft协议的Learner节点）。同时，写请求统一由Leader处理，读请求各个节点都能处理。</p><p>不一样的是它们的读行为和共识算法。</p><ul><li>在读行为上，ZooKeeper默认读可能会返回stale data，而etcd使用的线性读，能确保读取到反应集群共识的最新数据。</li><li>共识算法上，etcd使用的是Raft，ZooKeeper使用的是Zab。</li></ul><p><img src="https://static001.geekbang.org/resource/image/7a/d3/7a84bcaef9e53ba19d7d88e6ed6504d3.png?wh=1920*1013" alt="img"></p><p>那什么是Zab协议呢？</p><p>Zab协议可以分为以下阶段：</p><ul><li>Phase 0，Leader选举（Leader Election)。一个节点只要求获得半数以上投票，就可以当选为准Leader；</li><li>Phase 1，发现（Discovery）。准Leader收集其他节点的数据信息，并将最新的数据复制到自身；</li><li>Phase 2，同步（Synchronization）。准Leader将自身最新数据复制给其他落后的节点，并告知其他节点自己正式当选为Leader；</li><li>Phase 3，广播（Broadcast）。Leader正式对外服务，处理客户端写请求，对消息进行广播。当收到一个写请求后，它会生成Proposal广播给各个Follower节点，一半以上Follower节点应答之后，Leader再发送Commit命令给各个Follower，告知它们提交相关提案；</li></ul><p>ZooKeeper是如何实现的Zab协议的呢？</p><p>ZooKeeper在实现中并未严格按 <a href="https://marcoserafini.github.io/papers/zab.pdf">论文</a> 定义的分阶段实现，而是对部分阶段进行了整合，分别如下：</p><ul><li>Fast Leader Election。首先ZooKeeper使用了一个名为Fast Leader Election的选举算法，通过Leader选举安全规则限制，确保选举出来的Leader就含有最新数据， 避免了Zab协议的Phase 1阶段准Leader收集各个节点数据信息并复制到自身，也就是将Phase 0和Phase 1进行了合并。</li><li>Recovery Phase。各个Follower发送自己的最新数据信息给Leader，Leader根据差异情况，选择发送SNAP、DIFF差异数据、Truncate指令删除冲突数据等，确保Follower追赶上Leader数据进度并保持一致。</li><li>Broadcast Phase。与Zab论文Broadcast Phase一致。</li></ul><p>总体而言，从分布式系统CAP维度来看，ZooKeeper与etcd类似的是，它也是一个CP系统，在出现网络分区等错误时，它优先保障的数据一致性，牺牲的是A可用性。</p><h3 id="Consul架构及原理"><a href="#Consul架构及原理" class="headerlink" title="Consul架构及原理"></a>Consul架构及原理</h3><p>了解完ZooKeeper架构及原理后，我们再看看Consul，它的架构和原理是怎样的呢？</p><p>下图是 <a href="https://www.consul.io/docs/architecture">Consul架构图</a>（引用自HashiCorp官方文档）。</p><p><img src="https://static001.geekbang.org/resource/image/c4/90/c4feaebbdbe19d3f4e09899f8cd52190.png?wh=1920*1990" alt="img"></p><p>从图中你可以看到，它由Client、Server、Gossip协议、Raft共识算法、两个数据中心组成。每个数据中心内的Server基于Raft共识算法复制日志，Server节点分为Leader、Follower等角色。Client通过Gossip协议发现Server地址、分布式探测节点健康状态等。</p><p>那什么是Gossip协议呢？</p><p>Gossip中文名称叫流言协议，它是一种消息传播协议。它的核心思想其实源自我们生活中的八卦、闲聊。我们在日常生活中所看到的劲爆消息其实源于两类，一类是权威机构如国家新闻媒体发布的消息，另一类则是大家通过微信等社交聊天软件相互八卦，一传十，十传百的结果。</p><p>Gossip协议的基本工作原理与我们八卦类似，在Gossip协议中，如下图所示，各个节点会周期性地选择一定数量节点，然后将消息同步给这些节点。收到消息后的节点同样做出类似的动作，随机的选择节点，继续扩散给其他节点。</p><p>最终经过一定次数的扩散、传播，整个集群的各个节点都能感知到此消息，各个节点的数据趋于一致。<strong>Gossip协议被广泛应用在多个知名项目中，比如Redis Cluster集群版，Apache Cassandra，AWS Dynamo。</strong></p><p><img src="https://static001.geekbang.org/resource/image/84/4d/847ae4bcb531065c2797f1c91d4f464d.png?wh=1920*989" alt="img"></p><p>了解完Gossip协议，我们再看看架构图中的多数据中心，Consul支持数据跨数据中心自动同步吗？</p><p>需要注意的是，虽然Consul天然支持多数据中心，但是<strong>多数据中心内的服务数据并不会跨数据中心同步</strong>，各个数据中心的Server集群是独立的。不过，Consul提供了 <a href="https://www.consul.io/api-docs/query">Prepared Query</a> 功能，它支持根据一定的策略返回多数据中心下的最佳的服务实例地址，使你的服务具备跨数据中心容灾。</p><p>比如当你的API网关收到用户请求查询A服务，API网关服务优先从缓存中查找A服务对应的最佳实例。若无缓存则向Consul发起一个Prepared Query请求查询A服务实例，Consul收到请求后，优先返回本数据中心下的服务实例。如果本数据中心没有或异常则根据数据中心间 RTT 由近到远查询其它数据中心数据，最终网关可将用户请求转发给最佳的数据中心下的实例地址。</p><p>了解完Consul的Gossip协议、多数据中心支持，我们再看看Consul是如何处理读请求的呢?</p><p>Consul支持以下三种模式的读请求：</p><ul><li>默认（default）。默认是此模式，绝大部分场景下它能保证数据的强一致性。但在老的Leader出现网络分区被隔离、新的Leader被选举出来的一个极小时间窗口内，可能会导致stale read。这是因为Consul为了提高读性能，使用的是基于Lease机制来维持Leader身份，避免了与其他节点进行交互确认的开销。</li><li>强一致性（consistent）。强一致性读与etcd默认线性读模式一样，每次请求需要集群多数节点确认Leader身份，因此相比default模式读，性能会有所下降。</li><li>弱一致性（stale)。任何节点都可以读，无论它是否Leader。可能读取到陈旧的数据，类似etcd的串行读。这种读模式不要求集群有Leader，因此当集群不可用时，只要有节点存活，它依然可以响应读请求。</li></ul><h2 id="重点特性比较"><a href="#重点特性比较" class="headerlink" title="重点特性比较"></a>重点特性比较</h2><p>初步了解完etcd、ZooKeeper、Consul架构及原理后，你可以看到，他们都是基于共识算法实现的强一致的分布式存储系统，并都提供了多种模式的读机制。</p><p>除了以上共性，那么它们之间有哪些差异呢？ 下表是etcd开源社区总结的一个 <a href="https://etcd.io/docs/current/learning/why/">详细对比项</a>，我们就从并发原语、健康检查及服务发现、数据模型、Watch特性等功能上详细比较下它们功能和区别。</p><p><img src="https://static001.geekbang.org/resource/image/4d/50/4d0d9a05790f8ee9b66daf66ea741a50.jpg?wh=622*1147" alt="img"></p><h3 id="并发原语"><a href="#并发原语" class="headerlink" title="并发原语"></a>并发原语</h3><p>etcd和ZooKeeper、Consul的典型应用场景都是分布式锁、Leader选举，以上场景就涉及到并发原语控制。然而etcd和ZooKeeper并未提供原生的分布式锁、Leader选举支持，只提供了核心的基本数据读写、并发控制API，由应用上层去封装。</p><p>为了帮助开发者更加轻松的使用etcd去解决分布式锁、Leader选举等问题，etcd社区提供了 <a href="https://github.com/etcd-io/etcd/tree/v3.4.9/clientv3/concurrency">concurrency包</a> 来实现以上功能。同时，在etcdserver中内置了Lock和Election服务，不过其也是基于concurrency包做了一层封装而已，clientv3并未提供Lock和Election服务API给Client使用。 ZooKeeper所属的Apache社区提供了 <a href="http://curator.apache.org/curator-recipes/index.html">Apache Curator Recipes</a> 库来帮助大家快速使用分布式锁、Leader选举功能。</p><p>相比etcd、ZooKeeper依赖应用层基于API上层封装，Consul对分布式锁就提供了 <a href="https://www.consul.io/commands/lock">原生的支持</a>，可直接通过命令行使用。</p><p>总体而言，etcd、ZooKeeper、Consul都能解决分布式锁、Leader选举的痛点，在选型时，你可能会重点考虑其提供的API语言是否与业务服务所使用的语言一致。</p><h3 id="健康检查、服务发现"><a href="#健康检查、服务发现" class="headerlink" title="健康检查、服务发现"></a>健康检查、服务发现</h3><p>分布式协调服务的另外一个核心应用场景是服务发现、健康检查。</p><p>与并发原语类似，etcd和ZooKeeper并未提供原生的服务发现支持。相反，Consul在服务发现方面做了很多解放用户双手的工作，提供了服务发现的框架，帮助你的业务快速接入，并提供了HTTP和DNS两种获取服务方式。</p><p>比如下面就是通过DNS的方式获取服务地址：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">$ dig @<span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span> -<span class="hljs-selector-tag">p</span> <span class="hljs-number">8600</span> redis<span class="hljs-selector-class">.service</span><span class="hljs-selector-class">.dc1</span><span class="hljs-selector-class">.consul</span>. ANY<br><br></code></pre></td></tr></table></figure><p>最重要的是它还集成了分布式的健康检查机制。与etcd和ZooKeeper健康检查不一样的是，它是一种基于client、Gossip协议、分布式的健康检查机制，具备低延时、可扩展的特点。业务可通过Consul的健康检查机制，实现HTTP接口返回码、内存乃至磁盘空间的检测。</p><p>Consul提供了 <a href="https://learn.hashicorp.com/tutorials/consul/service-registration-health-checks">多种机制实现注册健康检查</a>，如脚本、HTTP、TCP等。</p><p>脚本是怎么工作的呢？介绍Consul架构时，Agent角色的任务之一就是执行分布式的健康检查。</p><p>比如将如下脚本放在Agent相应目录下，当Linux机器内存使用率超过70%的时候，它会返回告警状态。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash">&#123;<br>  ​<span class="hljs-string">&quot;check&quot;</span>:<br>    ​<span class="hljs-string">&quot;id&quot;</span>: <span class="hljs-string">&quot;mem-util&quot;</span><br>    ​<span class="hljs-string">&quot;name&quot;</span>: <span class="hljs-string">&quot;Memory utilization&quot;</span><br>    ​<span class="hljs-string">&quot;args&quot;</span>:<br>      ​<span class="hljs-string">&quot;/bin/sh&quot;</span><br>      ​<span class="hljs-string">&quot;-c&quot;</span><br>      ​<span class="hljs-string">&quot;/usr/bin/free | awk &#x27;/Mem/&#123;printf(<span class="hljs-variable">$3</span>/<span class="hljs-variable">$2</span>*100)&#125;&#x27; | awk &#x27;&#123; print(<span class="hljs-variable">$0</span>); if(<span class="hljs-variable">$1</span> &gt; 70) exit 1;&#125;&#x27;</span><br><span class="hljs-string">    ​]</span><br><span class="hljs-string">    ​&quot;</span>interval<span class="hljs-string">&quot;: &quot;</span>10s<span class="hljs-string">&quot;</span><br><span class="hljs-string">    ​&quot;</span><span class="hljs-built_in">timeout</span><span class="hljs-string">&quot;: &quot;</span>1s<br>  &#125;​<br>&#125;<br><br></code></pre></td></tr></table></figure><p>相比Consul，etcd、ZooKeeper它们提供的健康检查机制和能力就非常有限了。</p><p><strong>etcd提供了Lease机制来实现活性检测</strong>。它是一种中心化的健康检查，依赖用户不断地发送心跳续租、更新TTL。</p><p>ZooKeeper使用的是一种名为临时节点的状态来实现健康检查。当client与ZooKeeper节点连接断掉时，ZooKeeper就会删除此临时节点的key-value数据。它比基于心跳机制更复杂，也给client带去了更多的复杂性，所有client必须维持与ZooKeeper server的活跃连接并保持存活。</p><h3 id="数据模型比较"><a href="#数据模型比较" class="headerlink" title="数据模型比较"></a>数据模型比较</h3><p>从并发原语、健康检查、服务发现等维度了解完etcd、ZooKeeper、Consul的实现区别之后，我们再从数据模型上对比下三者。</p><p>首先etcd正如之前boltdb所介绍的，它是个扁平的key-value模型，内存索引通过B-tree实现，数据持久化存储基于B+ tree的boltdb，支持范围查询、适合读多写少，可容纳数G的数据。</p><p><a href="https://www.usenix.org/legacy/event/atc10/tech/full_papers/Hunt.pdf">ZooKeeper的数据模型</a> 如下。</p><p><img src="https://static001.geekbang.org/resource/image/93/fb/93edd0575e5a5a1080dac40415b779fb.png?wh=1012*588" alt="img"></p><p>如上图所示，它是一种层次模型，你可能已经发现，etcd v2的内存数据模型与它是一样的。ZooKeeper作为分布式协调服务的祖师爷，早期etcd v2的确就是参考它而设计的。</p><p>ZooKeeper的层次模型中的每个节点叫Znode，它分为持久性和临时型两种。</p><ul><li>持久性顾名思义，除非你通过API删除它，否则它将永远存在。</li><li>临时型是指它与客户端会话绑定，若客户端会话结束或出现异常中断等，它都将被ZooKeeper server自动删除，被广泛应用于活性检测。</li></ul><p>同时你创建节点的时候，还可以指定一个顺序标识，这样节点名创建出来后就具有顺序性，一般应用于分布式选举等场景中。</p><p>那ZooKeeper是如何实现以上层次模型的呢？</p><p>ZooKeeper使用的是内存ConcurrentHashMap来实现此数据结构，因此具有良好的读性能。但是受限于内存的瓶颈，一般ZooKeeper的数据库文件大小是几百M左右。</p><p><strong>Consul的数据模型及存储是怎样的呢？</strong></p><p>它也提供了常用key-value操作，它的存储引擎是基于 <a href="https://en.wikipedia.org/wiki/Radix_tree#">Radix Tree</a> 实现的 <a href="https://github.com/hashicorp/go-memdb">go-memdb</a>，要求value大小不能超过512个字节，数据库文件大小一般也是几百M左右。与boltdb类似，它也<strong>支持事务、MVCC</strong>。</p><h3 id="Watch特性比较"><a href="#Watch特性比较" class="headerlink" title="Watch特性比较"></a>Watch特性比较</h3><p>接下来再看看Watch特性的比较。</p><p>正在之前Watch特性中所介绍的，<strong>etcd v3的Watch是基于MVCC机制实现</strong>的，而<strong>Consul是采用滑动窗口实现</strong>的。<strong>Consul存储引擎是基于 <a href="https://en.wikipedia.org/wiki/Radix_tree#">Radix Tree</a> 实现的，因此它不支持范围查询和监听，只支持前缀查询和监听，而etcd都支持。</strong></p><p>相比etcd与Consul，ZooKeeper的Watch特性有更多的局限性，它是个一次性触发器。</p><p>在ZooKeeper中，client对Znode设置了Watch时，如果Znode内容发生改变，那么client就会获得Watch事件。然而此Znode再次发生变化，那client是无法收到Watch事件的，除非client设置了新的Watch。</p><h3 id="其他比较"><a href="#其他比较" class="headerlink" title="其他比较"></a>其他比较</h3><p>最后我们再从其他方面做些比较。</p><ul><li><p>线性读。etcd和Consul都支持线性读，而ZooKeeper并不具备。</p></li><li><p>权限机制比较。etcd实现了RBAC的权限校验，而ZooKeeper和Consul实现的ACL。</p></li><li><p>事务比较。etcd和Consul都提供了简易的事务能力，支持对字段进行比较，而ZooKeeper只提供了版本号检查能力，功能较弱。</p></li><li><p>多数据中心。在多数据中心支持上，只有Consul是天然支持的，虽然它本身不支持数据自动跨数据中心同步，但是它提供的服务发现机制、 <a href="https://www.consul.io/api-docs/query">Prepared Query</a> 功能，赋予了业务在一个可用区后端实例故障时，可将请求转发到最近的数据中心实例。而etcd和ZooKeeper并不支持。</p></li></ul><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>最后小结下今天的内容。首先从顶层视角介绍了etcd、ZooKeeper、Consul基本架构及核心原理。</p><p>从共识算法角度上看，etcd、Consul是基于Raft算法实现的数据复制，ZooKeeper则是基于Zab算法实现的。Raft算法由Leader选举、日志同步、安全性组成，而Zab协议则由Leader选举、发现、同步、广播组成。无论Leader选举还是日志复制，它们都需要集群多数节点存活、确认才能继续工作。</p><p>从CAP角度上看，<strong>在发生网络分区时，etcd、Consul、ZooKeeper都是一个CP系统，无法写入新数据</strong>。同时，etcd、Consul、ZooKeeper提供了各种模式的读机制，总体上可分为强一致性读、非强一致性读。</p><p>其中etcd和Consul则提供了线性读，ZooKeeper默认是非强一致性读，不过业务可以通过sync()接口，等待Follower数据追赶上Leader进度，以读取最新值。</p><p>然后从并发原语、健康检查、服务发现、数据模型、Watch特性、多数据中心比较等方面和你重点介绍了三者的实现与区别。</p><p>其中<strong>Consul提供了原生的分布式锁、健康检查、服务发现机制支持，让业务可以更省心</strong>，不过etcd和ZooKeeper也都有相应的库，帮助你降低工作量。<strong>Consul最大的亮点则是对多数据中心的支持</strong>。</p><p><strong>最后如果业务使用Go语言编写的，国内一般使用etcd较多，文档、书籍、最佳实践案例丰富。Consul在国外应用比较多，中文文档及实践案例相比etcd较少。</strong>ZooKeeper一般是Java业务使用较多，广泛应用在大数据领域。另外Nacos也是个非常优秀的开源项目，支持服务发现、配置管理等，是Java业务的热门选择。</p><p><strong>给我感觉consul功能上除了上面说的不支持范围查询和监听外，对比etcd优势挺多，就是文档太少！！！欢迎大家补充</strong></p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
      <tag>consul</tag>
      
      <tag>Zookeeper</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>22.配置及服务发现：解析etcd在API Gateway开源项目中应用</title>
    <link href="/2022/10/15/22-%E9%85%8D%E7%BD%AE%E5%8F%8A%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%EF%BC%9A%E8%A7%A3%E6%9E%90etcd%E5%9C%A8APIGateway%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E4%B8%AD%E5%BA%94%E7%94%A8/"/>
    <url>/2022/10/15/22-%E9%85%8D%E7%BD%AE%E5%8F%8A%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%EF%BC%9A%E8%A7%A3%E6%9E%90etcd%E5%9C%A8APIGateway%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E4%B8%AD%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="22-配置及服务发现：解析etcd在API-Gateway开源项目中应用"><a href="#22-配置及服务发现：解析etcd在API-Gateway开源项目中应用" class="headerlink" title="22.配置及服务发现：解析etcd在API Gateway开源项目中应用"></a>22.配置及服务发现：解析etcd在API Gateway开源项目中应用</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><p>在软件开发的过程中，为了提升代码的灵活性和开发效率，我们大量使用配置去控制程序的运行行为。</p><p>从简单的数据库账号密码配置，到 <a href="https://github.com/kelseyhightower/confd">confd</a> 支持以etcd为后端存储的本地配置及模板管理，再到 <a href="https://github.com/apache/apisix">Apache APISIX</a> 等API Gateway项目使用etcd存储服务配置、路由信息等，最后到Kubernetes更实现了Secret和ConfigMap资源对象来解决配置管理的问题。</p><p>那么它们是如何实现实时、动态调整服务配置而不需要重启相关服务的呢？</p><p>接下来将以开源项目Apache APISIX为例，分析服务发现的原理，带你了解etcd的key-value模型，Watch机制，鉴权机制，Lease特性，事务特性在其中的应用。</p><h2 id="服务发现"><a href="#服务发现" class="headerlink" title="服务发现"></a>服务发现</h2><p>首先聊聊服务发现，服务发现是指什么？为什么需要它呢?</p><p>为了搞懂这个问题，首先分享下程序部署架构的演进。</p><h3 id="单体架构"><a href="#单体架构" class="headerlink" title="单体架构"></a>单体架构</h3><p>在早期软件开发时使用的是单体架构，也就是所有功能耦合在同一个项目中，统一构建、测试、发布。单体架构在项目刚启动的时候，架构简单、开发效率高，比较容易部署、测试。但是随着项目不断增大，它具有若干缺点，比如：</p><ul><li>所有功能耦合在同一个项目中，修复一个小Bug就需要发布整个大工程项目，增大引入问题风险。同时随着开发人员增多、单体项目的代码增长、各模块堆砌在一起、代码质量参差不齐，内部复杂度会越来越高，可维护性差。</li><li>无法按需针对仅出现瓶颈的功能模块进行弹性扩容，只能作为一个整体继续扩展，因此扩展性较差。</li><li>一旦单体应用宕机，将导致所有服务不可用，因此可用性较差。</li></ul><h3 id="分布式及微服务架构"><a href="#分布式及微服务架构" class="headerlink" title="分布式及微服务架构"></a>分布式及微服务架构</h3><p>如何解决以上痛点呢？</p><p>当然是将单体应用进行拆分，大而化小。如何拆分呢？ 这里我就以一个我曾经参与重构建设的电商系统为案例给你分析一下。在一个单体架构中，完整的电商系统应包括如下模块：</p><ul><li>商城系统，负责用户登录、查看及搜索商品、购物车商品管理、优惠券管理、订单管理、支付等功能。</li><li>物流及仓储系统，根据用户订单，进行发货、退货、换货等一系列仓储、物流管理。</li><li>其他客服系统、客户管理系统等。</li></ul><p>因此在分布式架构中，你可以按整体功能，将单体应用垂直拆分成以上三大功能模块，各个功能模块可以选择不同的技术栈实现，按需弹性扩缩容，如下图所示。</p><p><img src="https://static001.geekbang.org/resource/image/ca/20/ca6090e229dde9a0361d6yy2c3df8d20.png?wh=1920*1046" alt="img"></p><p>那什么又是微服务架构呢？</p><p>它是对各个功能模块进行更细立度的拆分，比如商城系统模块可以拆分成：</p><ul><li>用户鉴权模块；</li><li>商品模块；</li><li>购物车模块；</li><li>优惠券模块；</li><li>支付模块；</li><li>……</li></ul><p>在微服务架构中，每个模块职责更单一、独立部署、开发迭代快，如下图所示。</p><p><img src="https://static001.geekbang.org/resource/image/cf/4a/cf62b7704446c05d8747b4672b5fb74a.png?wh=1920*1048" alt="img"></p><p>那么在分布式及微服务架构中，各个模块之间如何及时知道对方网络地址与端口、协议，进行接口调用呢？</p><h3 id="为什么需要服务发现中间件"><a href="#为什么需要服务发现中间件" class="headerlink" title="为什么需要服务发现中间件?"></a>为什么需要服务发现中间件?</h3><p>其实这个知道的过程，就是服务发现。在早期的时候我们往往通过硬编码、配置文件声明各个依赖模块的网络地址、端口，然而这种方式在分布式及微服务架构中，其运维效率、服务可用性是远远不够的。</p><p>那么我们能否实现通过一个特殊服务就查询到各个服务的后端部署地址呢？ 各服务启动的时候，就自动将IP和Port、协议等信息注册到特殊服务上，当某服务出现异常的时候，特殊服务就自动删除异常实例信息？</p><p>是的，当然可以，这个特殊服务就是注册中心服务，你可以基于etcd、ZooKeeper、consul等实现。</p><h3 id="etcd服务发现原理"><a href="#etcd服务发现原理" class="headerlink" title="etcd服务发现原理"></a>etcd服务发现原理</h3><p>那么如何基于etcd实现服务发现呢?</p><p>下面给出一个通用的服务发现原理架构图，通过此图，为你介绍下服务发现的基本原理。详细如下：</p><ul><li>整体上分为四层，client层、proxy层(可选)、业务server、etcd存储层组成。引入proxy层的原因是使client更轻、逻辑更简单，无需直接访问存储层，同时可通过proxy层支持各种协议。</li><li>client层通过负载均衡访问proxy组件。proxy组件启动的时候，通过etcd的Range RPC方法从etcd读取初始化服务配置数据，随后通过Watch接口持续监听后端业务server扩缩容变化，实时修改路由。</li><li>proxy组件收到client的请求后，它根据从etcd读取到的对应服务的路由配置、负载均衡算法（比如Round-robin）转发到对应的业务server。</li><li>业务server启动的时候，通过etcd的写接口Txn&#x2F;Put等，注册自身地址信息、协议到高可用的etcd集群上。业务server缩容、故障时，对应的key应能自动从etcd集群删除，因此相关key需要关联lease信息，设置一个合理的TTL，并定时发送keepalive请求给Leader续租，以防止租约及key被淘汰。</li></ul><p><img src="https://static001.geekbang.org/resource/image/26/e4/26d0d18c0725de278eeb7505f20642e4.png?wh=1920*1137" alt="img"></p><p>当然，在分布式及微服务架构中，我们面对的问题不仅仅是服务发现，还包括如下痛点：</p><ul><li>限速；</li><li>鉴权；</li><li>安全；</li><li>日志；</li><li>监控；</li><li>丰富的发布策略；</li><li>链路追踪；</li><li>……</li></ul><p>为了解决以上痛点，各大公司及社区开发者推出了大量的开源项目。这里以国内开发者广泛使用的Apache APISIX项目为例，分析etcd在其中的应用，了解下它是怎么玩转服务发现的。</p><h3 id="Apache-APISIX原理"><a href="#Apache-APISIX原理" class="headerlink" title="Apache APISIX原理"></a>Apache APISIX原理</h3><p>Apache APISIX它具备哪些功能呢？</p><p>它的本质是一个无状态、高性能、实时、动态、可水平扩展的API网关。核心原理就是基于你配置的服务信息、路由规则等信息，将收到的请求通过一系列规则后，正确转发给后端的服务。</p><p>Apache APISIX其实就是上面服务发现原理架构图中的proxy组件，如下图红色虚线框所示。</p><p><img src="https://static001.geekbang.org/resource/image/20/fd/20a539bdd37db2d4632c7b0c5f4119fd.png?wh=1920*1246" alt="img"></p><p>Apache APISIX详细架构图如下（ <a href="https://github.com/apache/apisix">引用自社区项目文档</a>）。从图中可以看到，它由控制面和数据面组成。</p><p>控制面顾名思义，就是你通过Admin API下发服务、路由、安全配置的操作。控制面默认的服务发现存储是etcd，当然也支持consul、nacos等。</p><p>如果没有使用过Apache APISIX的话，可以参考下这个 <a href="https://github.com/apache/apisix-docker/tree/master/example">example</a>，快速、直观的了解下Apache APISIX是如何通过Admin API下发服务和路由配置的。</p><p>数据面是在实现基于服务路由信息数据转发的基础上，提供了限速、鉴权、安全、日志等一系列功能，也就是解决了我们上面提的分布式及微服务架构中的典型痛点。</p><p><img src="https://static001.geekbang.org/resource/image/83/f4/834502c6ed7e59fe0b4643c11b2d31f4.png?wh=1746*838" alt="img"></p><p>那么当我们通过控制面API新增一个服务时，Apache APISIX是是如何实现实时、动态调整服务配置，而不需要重启网关服务的呢？</p><p>下面，我就和你聊聊etcd在Apache APISIX项目中的应用。</p><h3 id="etcd在Apache-APISIX中的应用"><a href="#etcd在Apache-APISIX中的应用" class="headerlink" title="etcd在Apache APISIX中的应用"></a>etcd在Apache APISIX中的应用</h3><p>在搞懂这个问题之前，我们先看看Apache APISIX在etcd中，都存储了哪些数据呢？它的数据存储格式是怎样的？</p><h4 id="数据存储格式"><a href="#数据存储格式" class="headerlink" title="数据存储格式"></a>数据存储格式</h4><p>下面我参考Apache APISIX的 <a href="https://github.com/apache/apisix-docker/tree/master/example">example</a> 案例（apisix:2.3），通过Admin API新增了两个服务、路由规则后，执行如下查看etcd所有key的命令：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs vim">etcdctl <span class="hljs-built_in">get</span> <span class="hljs-string">&quot;&quot;</span> --prefix --<span class="hljs-built_in">keys</span>-<span class="hljs-keyword">only</span><br><br></code></pre></td></tr></table></figure><p>etcd输出结果如下：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/apisix/</span>consumers/<br><span class="hljs-regexp">/apisix/</span>data_plane<span class="hljs-regexp">/server_info/</span>f7285805-<span class="hljs-number">73</span>e9-<span class="hljs-number">4</span>ce4-acc6-a38d619afdc3<br><span class="hljs-regexp">/apisix/g</span>lobal_rules/<br><span class="hljs-regexp">/apisix/</span>node_status/<br><span class="hljs-regexp">/apisix/</span>plugin_metadata/<br><span class="hljs-regexp">/apisix/</span>plugins<br><span class="hljs-regexp">/apisix/</span>plugins/<br><span class="hljs-regexp">/apisix/</span>proto/<br><span class="hljs-regexp">/apisix/</span>routes/<br><span class="hljs-regexp">/apisix/</span>routes/<span class="hljs-number">12</span><br><span class="hljs-regexp">/apisix/</span>routes/<span class="hljs-number">22</span><br><span class="hljs-regexp">/apisix/</span>services/<br><span class="hljs-regexp">/apisix/</span>services/<span class="hljs-number">1</span><br><span class="hljs-regexp">/apisix/</span>services/<span class="hljs-number">2</span><br><span class="hljs-regexp">/apisix/</span>ssl/<br><span class="hljs-regexp">/apisix/</span>ssl/<span class="hljs-number">1</span><br><span class="hljs-regexp">/apisix/</span>ssl/<span class="hljs-number">2</span><br><span class="hljs-regexp">/apisix/</span>stream_routes/<br><span class="hljs-regexp">/apisix/u</span>pstreams/<br><br></code></pre></td></tr></table></figure><p>然后我们继续通过etcdctl get命令查看下services都存储了哪些信息呢？</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs awk">root@e9d3b477ca1f:<span class="hljs-regexp">/opt/</span>bitnami<span class="hljs-regexp">/etcd# etcdctl get /</span>apisix/services --prefix<br><span class="hljs-regexp">/apisix/</span>services/<br>init_dir<br><span class="hljs-regexp">/apisix/</span>services/<span class="hljs-number">1</span><br>&#123;<span class="hljs-string">&quot;update_time&quot;</span>:<span class="hljs-number">1614293352</span>,<span class="hljs-string">&quot;create_time&quot;</span>:<span class="hljs-number">1614293352</span>,<span class="hljs-string">&quot;upstream&quot;</span>:&#123;<span class="hljs-string">&quot;type&quot;</span>:<span class="hljs-string">&quot;roundrobin&quot;</span>,<span class="hljs-string">&quot;nodes&quot;</span>:&#123;<span class="hljs-string">&quot;172.18.5.12:80&quot;</span>:<span class="hljs-number">1</span>&#125;,<span class="hljs-string">&quot;hash_on&quot;</span>:<span class="hljs-string">&quot;vars&quot;</span>,<span class="hljs-string">&quot;scheme&quot;</span>:<span class="hljs-string">&quot;http&quot;</span>,<span class="hljs-string">&quot;pass_host&quot;</span>:<span class="hljs-string">&quot;pass&quot;</span>&#125;,<span class="hljs-string">&quot;id&quot;</span>:<span class="hljs-string">&quot;1&quot;</span>&#125;<br><span class="hljs-regexp">/apisix/</span>services/<span class="hljs-number">2</span><br>&#123;<span class="hljs-string">&quot;update_time&quot;</span>:<span class="hljs-number">1614293361</span>,<span class="hljs-string">&quot;create_time&quot;</span>:<span class="hljs-number">1614293361</span>,<span class="hljs-string">&quot;upstream&quot;</span>:<br>&#123;<span class="hljs-string">&quot;type&quot;</span>:<span class="hljs-string">&quot;roundrobin&quot;</span>,<span class="hljs-string">&quot;nodes&quot;</span>:&#123;<span class="hljs-string">&quot;172.18.5.13:80&quot;</span>:<span class="hljs-number">1</span>&#125;,<span class="hljs-string">&quot;hash_on&quot;</span>:<span class="hljs-string">&quot;vars&quot;</span>,<span class="hljs-string">&quot;scheme&quot;</span>:<span class="hljs-string">&quot;http&quot;</span>,<span class="hljs-string">&quot;pass_host&quot;</span>:<span class="hljs-string">&quot;pass&quot;</span>&#125;,<span class="hljs-string">&quot;id&quot;</span>:<span class="hljs-string">&quot;2&quot;</span>&#125;<br><br></code></pre></td></tr></table></figure><p>从中我们可以总结出如下信息：</p><ul><li>Apache APSIX 2.x系列版本使用的是etcd3。</li><li>服务、路由、ssl、插件等配置存储格式前缀是&#x2F;apisix + “&#x2F;“ + 功能特性类型（routes&#x2F;services&#x2F;ssl等），我们通过Admin API添加的路由、服务等配置就保存在相应的前缀下。</li><li>路由和服务配置的value是个Json对象，其中服务对象包含了id、负载均衡算法、后端节点、协议等信息。</li></ul><p>了解完Apache APISIX在etcd中的数据存储格式后，那么它是如何动态、近乎实时地感知到服务配置变化的呢？</p><h4 id="Watch机制的应用"><a href="#Watch机制的应用" class="headerlink" title="Watch机制的应用"></a>Watch机制的应用</h4><p>与Kubernetes一样，它们都是通过etcd的 <strong>Watch机制</strong> 来实现的。</p><p>Apache APISIX在启动的时候，首先会通过Range操作获取网关的配置、路由等信息，随后就通过Watch机制，获取增量变化事件。</p><p>使用Watch机制最容易犯错的地方是什么呢？</p><p>答案是不处理Watch返回的相关错误信息，比如已压缩ErrCompacted错误。Apache APISIX项目在从etcd v2中切换到etcd v3早期的时候，同样也犯了这个错误。</p><h4 id="鉴权机制的应用"><a href="#鉴权机制的应用" class="headerlink" title="鉴权机制的应用"></a>鉴权机制的应用</h4><p>除了Watch机制，Apache APISIX项目还使用了鉴权，毕竟配置网关是个高危操作，那它是如何使用etcd鉴权机制的呢？ <strong>etcd鉴权机制</strong> 中最容易踩的坑是什么呢？</p><p>答案是不复用client和鉴权token，频繁发起Authenticate操作，导致etcd高负载。正如之前介绍的，一个8核32G的高配节点在100个连接时，Authenticate QPS仅为8。可想而知，你如果不复用token，那么出问题就很自然不过了。</p><p>Apache APISIX是否也踩了这个坑呢？</p><p>Apache APISIX是基于Lua构建的，使用的是 <a href="https://github.com/api7/lua-resty-etcd/blob/master/lib/resty/etcd/v3.lua">lua-resty-etcd</a> 这个项目访问etcd，从相关 <a href="https://github.com/apache/apisix/issues/2899">issue</a> 反馈看，的确也踩了这个坑。社区用户反馈后，随后通过复用client、更完善的token复用机制解决了Authenticate的性能瓶颈，详细信息可参考 <a href="https://github.com/apache/apisix/pull/2932">PR 2932</a>、 <a href="https://github.com/api7/lua-resty-etcd/pull/100">PR 100</a>。</p><p>除了以上介绍的Watch机制、鉴权机制，Apache APISIX还使用了etcd的Lease特性和事务接口。</p><h4 id="Lease特性的应用"><a href="#Lease特性的应用" class="headerlink" title="Lease特性的应用"></a>Lease特性的应用</h4><p>为什么Apache APISIX项目需要Lease特性呢？</p><p>服务发现的核心工作原理是服务启动的时候将地址信息登录到注册中心，服务异常时自动从注册中心删除。</p><p>这是不是跟前面介绍的&lt;Lease特性: 如何检测客户端的存活性&gt;应用场景很匹配呢？</p><p>没错，Apache APISIX通过etcd v2的TTL特性、etcd v3的Lease特性来实现类似的效果，它提供的增加服务路由API，支持设置TTL属性，如下面所示：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-comment"># Create a route expires after 60 seconds, then it&#x27;s deleted automatically</span><br>$ curl http:<span class="hljs-string">//127.0.0.1</span><span class="hljs-function">:9080</span>/apisix/admin/routes/2?ttl=60 -H &#x27;X-API-KEY: edd1c9f034335f136f87ad84b625c8f1&#x27; -X PUT -i -d &#x27;<br>&#123;<br>    <span class="hljs-string">&quot;uri&quot;</span>: <span class="hljs-string">&quot;/aa/index.html&quot;</span>,<br>    <span class="hljs-string">&quot;upstream&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;roundrobin&quot;</span>,<br>        <span class="hljs-string">&quot;nodes&quot;</span>: &#123;<br>            <span class="hljs-string">&quot;39.97.63.215:80&quot;</span>: 1<br>        &#125;<br>    &#125;<br>&#125;&#x27;<br><br></code></pre></td></tr></table></figure><p>当一个路由设置非0 TTL后，Apache APISIX就会为它创建Lease，关联key，相关代码如下：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-comment">-- lease substitute ttl in v3</span><br><span class="hljs-keyword">local</span> res, err<br><span class="hljs-keyword">if</span> ttl <span class="hljs-keyword">then</span><br>    <span class="hljs-keyword">local</span> data, grant_err = etcd_cli:<span class="hljs-keyword">grant</span>(tonumber(ttl))<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> data <span class="hljs-keyword">then</span><br>        <span class="hljs-keyword">return</span> nil, grant_err<br>    <span class="hljs-keyword">end</span><br>    res, err = etcd_cli:<span class="hljs-keyword">set</span>(prefix .. key, <span class="hljs-keyword">value</span>, &#123;prev_kv = <span class="hljs-keyword">true</span>, lease = data.body.ID&#125;)<br><span class="hljs-keyword">else</span><br>    res, err = etcd_cli:<span class="hljs-keyword">set</span>(prefix .. key, <span class="hljs-keyword">value</span>, &#123;prev_kv = <span class="hljs-keyword">true</span>&#125;)<br><span class="hljs-keyword">end</span><br><br></code></pre></td></tr></table></figure><h4 id="事务特性的应用"><a href="#事务特性的应用" class="headerlink" title="事务特性的应用"></a>事务特性的应用</h4><p>介绍完Lease特性在Apache APISIX项目中的应用后，我们再来思考两个问题。为什么它还依赖etcd的事务特性呢？简单的执行put接口有什么问题？</p><p>答案是它跟Kubernetes是一样的使用目的。使用事务是为了防止并发场景下的数据写冲突，比如你可能同时发起两个Patch Admin API去修改配置等。如果简单地使用put接口，就会导致第一个写请求的结果被覆盖。</p><p>Apache APISIX是如何使用事务接口提供的乐观锁机制去解决并发冲突的问题呢？</p><p>核心依然是我们前面课程中一直强调的<strong>mod_revision</strong>，它会比较事务提交时的mod_revision与预期是否一致，一致才能执行put操作，Apache APISIX相关使用代码如下：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs maxima"><span class="hljs-built_in">local</span> <span class="hljs-built_in">compare</span> = &#123;<br>    &#123;<br>        <span class="hljs-built_in">key</span> = <span class="hljs-built_in">key</span>,<br>        target = <span class="hljs-string">&quot;MOD&quot;</span>,<br>        result = <span class="hljs-string">&quot;EQUAL&quot;</span>,<br>        mod_revision = mod_revision,<br>    &#125;<br>&#125;<br><span class="hljs-built_in">local</span> success = &#123;<br>    &#123;<br>        requestPut = &#123;<br>            <span class="hljs-built_in">key</span> = <span class="hljs-built_in">key</span>,<br>            value = value,<br>            lease = lease_id,<br>        &#125;<br>    &#125;<br>&#125;<br><span class="hljs-built_in">local</span> res, err = etcd_cli:txn(<span class="hljs-built_in">compare</span>, success)<br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> res <span class="hljs-keyword">then</span><br>    <span class="hljs-built_in">return</span> nil, err<br>end<br><br></code></pre></td></tr></table></figure><p>关于Apache APISIX事务特性的引入、背景以及更详细的实现，也可以参考 <a href="https://github.com/apache/apisix/pull/2216">PR 2216</a>。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>闪现介绍了服务部署架构的演进，从单体架构的缺陷开始、到分布式及微服务架构的诞生，分享了分布式及微服务架构中面临的一系列痛点（如服务发现，鉴权，安全，限速等等）。</p><p>而开源项目Apache APISIX正是一个基于etcd的项目，它为后端存储提供了一系列的解决方案，通过它的架构图为你介绍了其控制面和数据面的工作原理。</p><p>随后从数据存储格式、Watch机制、鉴权机制、Lease特性以及事务特性维度，分析了它们在Apache APISIX项目中的应用。</p><p>数据存储格式上，APISIX采用典型的prefix + 功能特性组织格式。key是相关配置id，value是个json对象，包含一系列业务所需要的核心数据。你需要注意的是Apache APISIX 1.x版本使用的etcd v2 API，2.x版本使用的是etcd v3 API，要求至少是etcd v3.4版本以上。</p><p>Watch机制上，APISIX依赖它进行配置的动态、实时更新，避免了传统的修改配置，需要服务重启等缺陷。</p><p>鉴权机制上，APISIX使用密码认证，进行多租户认证、授权，防止用户出现越权访问，保护网关服务的安全。</p><p>Lease及事务特性上，APISIX通过Lease来设置自动过期的路由规则，解决服务发现中的节点异常自动剔除等问题，通过事务特性的乐观锁机制来实现并发场景下覆盖更新等问题。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><p>假设老板让你去设计一个大型配置系统，满足公司各个业务场景的诉求，期望的设计目标如下：</p><ul><li>高可靠。配置系统的作为核心基础设施，期望可用性能达到99.99%。</li><li>高性能。公司业务多，规模大，配置系统应具备高性能、并能水平扩容。</li><li>支持多业务、多版本管理、多种发布策略。</li></ul><p>你认为etcd适合此业务场景吗？</p><p>答案1：</p><p>1.高可靠。etcd基于raft的多副本可以满足。</p><p> 2.高性能。公司业务多，规模大，可以依据不同业务不同etcd的方法，分担etcd的写压力，以及数据存储量有限的问题。各自业务的etcd可以水平扩展。 </p><p>3.支持多业务、多版本管理、多种发布策略。</p><p>etcd可以做到多版本管理，多发布策略的话，可以级联多个etcd的方法。 另外，可能更加理想的存储架构方式是采用计算与存储分离的方法，计算部分处理读写以及扩展，存储部分处理多版本，多业务，多发布策略。</p><p>答案2：</p><p>认为etcd并不合适，适合使用可平行扩容的分布式数据库如tidb，运维复杂度不更低点吗，容量也更大，还能支持各种key value大小配置</p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>21.分布式锁：为什么基于etcd实现分布式锁比Redis锁更安全？</title>
    <link href="/2022/10/15/21-%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E5%9F%BA%E4%BA%8Eetcd%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E6%AF%94Redis%E9%94%81%E6%9B%B4%E5%AE%89%E5%85%A8%EF%BC%9F/"/>
    <url>/2022/10/15/21-%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E5%9F%BA%E4%BA%8Eetcd%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E6%AF%94Redis%E9%94%81%E6%9B%B4%E5%AE%89%E5%85%A8%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="21-分布式锁：为什么基于etcd实现分布式锁比Redis锁更安全？"><a href="#21-分布式锁：为什么基于etcd实现分布式锁比Redis锁更安全？" class="headerlink" title="21.分布式锁：为什么基于etcd实现分布式锁比Redis锁更安全？"></a>21.分布式锁：为什么基于etcd实现分布式锁比Redis锁更安全？</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><p>在软件开发过程中，我们经常会遇到各种场景要求对共享资源进行互斥操作，否则整个系统的数据一致性就会出现问题。典型场景如商品库存操作、Kubernertes调度器为Pod分配运行的Node。</p><p>那要如何实现对共享资源进行互斥操作呢？</p><p>锁就是其中一个非常通用的解决方案。在单节点多线程环境，你使用本地的互斥锁就可以完成资源的互斥操作。然而单节点存在单点故障，为了保证服务高可用，你需要多节点部署。在多节点部署的分布式架构中，你就需要使用分布式锁来解决资源互斥操作了。</p><p>但是为什么有的业务使用了分布式锁还会出现各种严重超卖事故呢？分布式锁的实现和使用过程需要注意什么？</p><h2 id="从茅台超卖案例看分布式锁要素"><a href="#从茅台超卖案例看分布式锁要素" class="headerlink" title="从茅台超卖案例看分布式锁要素"></a>从茅台超卖案例看分布式锁要素</h2><p>首先我们从去年一个因Redis分布式锁实现问题导致 <a href="https://juejin.cn/post/6854573212831842311">茅台超卖案例</a> 说起，在这个真实案例中，因茅台的稀缺性，事件最终定级为P0级生产事故，后果影响严重。</p><p>那么它是如何导致超卖的呢？</p><p>首先简单介绍下此案例中的Redis简易分布式锁实现方案，它使用了Redis SET命令来实现。</p><figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs coq">SET key value [EX seconds|<span class="hljs-type">PX</span> milliseconds|<span class="hljs-type">EXAT</span> timestamp|<span class="hljs-type">PXAT</span> milliseconds-timestamp|<span class="hljs-type">KEEPTTL</span>] [NX|<span class="hljs-type">XX</span>]<br>[GET]<br><br></code></pre></td></tr></table></figure><p>简单给你介绍下SET命令重点参数含义：</p><ul><li><code>EX</code> 设置过期时间，单位秒；</li><li><code>NX</code> 当key不存在的时候，才设置key；</li><li><code>XX</code> 当key存在的时候，才设置key。</li></ul><p>此业务就是基于Set key value EX 10 NX命令来实现的分布式锁，并通过JAVA的try-finally语句，执行Del key语句来释放锁，简易流程如下：</p><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs vbnet"># 对资源<span class="hljs-keyword">key</span>加锁，<span class="hljs-keyword">key</span>不存在时创建，并且设置，<span class="hljs-number">10</span>秒自动过期<br><span class="hljs-keyword">SET</span> <span class="hljs-keyword">key</span> value EX <span class="hljs-number">10</span> NX<br>业务逻辑流程<span class="hljs-number">1</span>，校验用户身份<br>业务逻辑流程<span class="hljs-number">2</span>，查询并校验库存(<span class="hljs-keyword">get</span> <span class="hljs-built_in">and</span> <span class="hljs-keyword">compare</span>)<br>业务逻辑流程<span class="hljs-number">3</span>，库存&gt;<span class="hljs-number">0</span>，扣减库存(Decr stock)，生成秒杀茅台订单<br><br># 释放锁<br>Del <span class="hljs-keyword">key</span><br><br></code></pre></td></tr></table></figure><p>以上流程中其实存在以下思考点:</p><ul><li>NX参数有什么作用?</li><li>为什么需要原子的设置key及过期时间？</li><li>为什么基于Set key value EX 10 NX命令还出现了超卖呢?</li><li>为什么大家都比较喜欢使用Redis作为分布式锁实现？</li></ul><p>首先来看第一个问题，NX参数的作用。NX参数是为了保证当分布式锁不存在时，只有一个client能写入此key成功，获取到此锁。我们使用分布式锁的目的就是希望在高并发系统中，有一种互斥机制来防止彼此相互干扰，保证数据的一致性。</p><p><strong>因此分布式锁的第一核心要素就是互斥性、安全性。在同一时间内，不允许多个client同时获得锁。</strong></p><p>再看第二个问题，假设我们未设置key自动过期时间，在Set key value NX后，如果程序crash或者发生网络分区后无法与Redis节点通信，毫无疑问其他client将永远无法获得锁。这将导致死锁，服务出现中断。</p><p>有的同学意识到这个问题后，使用如下SETNX和EXPIRE命令去设置key和过期时间，这也是不正确的，因为你无法保证SETNX和EXPIRE命令的原子性。</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs gauss"><span class="hljs-meta"># 对资源key加锁，key不存在时创建</span><br>SETNX <span class="hljs-built_in">key</span> value<br><span class="hljs-meta"># 设置KEY过期时间</span><br>EXPIRE <span class="hljs-built_in">key</span> <span class="hljs-number">10</span><br>业务逻辑流程<br><br><span class="hljs-meta"># 释放锁</span><br>Del <span class="hljs-built_in">key</span><br><br></code></pre></td></tr></table></figure><p><strong>这就是分布式锁第二个核心要素，活性。在实现分布式锁的过程中要考虑到client可能会出现crash或者网络分区，你需要原子申请分布式锁及设置锁的自动过期时间，通过过期、超时等机制自动释放锁，避免出现死锁，导致业务中断。</strong></p><p>再看第三个问题，为什么使用了Set key value EX 10 NX命令，还出现了超卖呢？</p><p>原来是抢购活动开始后，加锁逻辑中的业务流程1访问的用户身份服务出现了高负载，导致阻塞在校验用户身份流程中(超时30秒)，然而锁10秒后就自动过期了，因此其他client能获取到锁。关键是阻塞的请求执行完后，它又把其他client的锁释放掉了，导致进入一个恶性循环。</p><p>因此申请锁时，写入的value应确保唯一性（随机值等）。client在释放锁时，应通过Lua脚本原子校验此锁的value与自己写入的value一致，若一致才能执行释放工作。</p><p>更关键的是库存校验是通过get and compare方式，它压根就无法防止超卖。正确的解决方案应该是通过LUA脚本实现Redis比较库存、扣减库存操作的原子性（或者在每次只能抢购一个的情况下，通过判断 <a href="https://redis.io/commands/DECR">Redis Decr命令</a> 的返回值即可。此命令会返回扣减后的最新库存，若小于0则表示超卖）。</p><p><strong>从这个问题中我们可以看到，分布式锁实现具备一定的复杂度，它不仅依赖存储服务提供的核心机制，同时依赖业务领域的实现。无论是遭遇高负载、还是宕机、网络分区等故障，都需确保锁的互斥性、安全性，否则就会出现严重的超卖生产事故。</strong></p><p>再看最后一个问题，为什么大家都比较喜欢使用Redis做分布式锁的实现呢?</p><p>考虑到在秒杀等业务场景上存在大量的瞬间、高并发请求，加锁与释放锁的过程应是高性能、高可用的。而Redis核心优点就是<strong>快、简单</strong>，是随处可见的基础设施，部署、使用也及其方便，因此广受开发者欢迎。</p><p><strong>这就是分布式锁第三个核心要素，高性能、高可用。加锁、释放锁的过程性能开销要尽量低，同时要保证高可用，确保业务不会出现中断。</strong></p><p>那么除了以上案例中人为实现问题导致的锁不安全因素外，基于Redis实现的以上分布式锁还有哪些安全性问题呢？</p><h2 id="Redis分布式锁问题"><a href="#Redis分布式锁问题" class="headerlink" title="Redis分布式锁问题"></a>Redis分布式锁问题</h2><p>我们从茅台超卖案例中为你总结出的分布式核心要素（<strong>互斥性、安全性、活性、高可用、高性能</strong>）说起。</p><p>首先，如果我们的分布式锁跑在单节点的Redis Master节点上，那么它就存在单点故障，无法保证分布式锁的高可用。</p><p>于是我们需要一个主备版的Redis服务，至少具备一个Slave节点。</p><p>我们又知道Redis是基于主备异步复制协议实现的Master-Slave数据同步，如下图所示，若client A执行SET key value EX 10 NX命令，redis-server返回给client A成功后，Redis Master节点突然出现crash等异常，这时候Redis Slave节点还未收到此命令的同步。</p><p><img src="/images/350285/cd3d4ab1af45c6eb76e7dccd9c666245.png"></p><p>若你部署了Redis Sentinel等主备切换服务，那么它就会以Slave节点提升为主，此时Slave节点因并未执行SET key value EX 10 NX命令，因此它收到client B发起的加锁的此命令后，它也会返回成功给client。</p><p>那么在同一时刻，集群就出现了两个client同时获得锁，分布式锁的互斥性、安全性就被破坏了。</p><p>除了主备切换可能会导致基于Redis实现的分布式锁出现安全性问题，在发生网络分区等场景下也可能会导致出现脑裂，Redis集群出现多个Master，进而也会导致多个client同时获得锁。</p><p>如下图所示，Master节点在可用区1，Slave节点在可用区2，当可用区1和可用区2发生网络分区后，部署在可用区2的Redis Sentinel服务就会将可用区2的Slave提升为Master，而此时可用区1的Master也在对外提供服务。因此集群就出现了脑裂，出现了两个Master，都可对外提供分布式锁申请与释放服务，分布式锁的互斥性被严重破坏。</p><p><img src="/images/350285/cb4cb52cf2244d2000884ef5f5ff3db1.png"></p><p><strong>主备切换、脑裂是Redis分布式锁的两个典型不安全的因素，本质原因是Redis为了满足高性能，采用了主备异步复制协议，同时也与负责主备切换的Redis Sentinel服务是否合理部署有关。</strong></p><p>有没有其他方案解决呢？</p><p>当然有，Redis作者为了解决SET key value [EX] 10 [NX]命令实现分布式锁不安全的问题，提出了 <a href="https://redis.io/topics/distlock">RedLock算法</a>。它是基于多个独立的Redis Master节点的一种实现（一般为5）。client依次向各个节点申请锁，若能从多数个节点中申请锁成功并满足一些条件限制，那么client就能获取锁成功。</p><p>它通过独立的N个Master节点，避免了使用主备异步复制协议的缺陷，只要多数Redis节点正常就能正常工作，显著提升了分布式锁的安全性、可用性。</p><p>但是，它的实现建立在一个不安全的系统模型上的，它依赖系统时间，当时钟发生跳跃时，也可能会出现安全性问题。你要有兴趣的话，可以详细阅读下分布式存储专家Martin对 <a href="https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html">RedLock的分析文章</a>，Redis作者的也专门写了 <a href="http://antirez.com/news/101">一篇文章进行了反驳</a>。</p><h2 id="分布式锁常见实现方案"><a href="#分布式锁常见实现方案" class="headerlink" title="分布式锁常见实现方案"></a>分布式锁常见实现方案</h2><p>了解完Redis分布式锁的一系列问题和实现方案后，我们再看看还有哪些典型的分布式锁实现。</p><p>除了Redis分布式锁，其他使用最广的应该是ZooKeeper分布式锁和etcd分布式锁。</p><p>ZooKeeper也是一个典型的分布式元数据存储服务，它的分布式锁实现基于ZooKeeper的临时节点和顺序特性。</p><p>首先什么是临时节点呢？</p><p>临时节点具备数据自动删除的功能。当client与ZooKeeper连接和session断掉时，相应的临时节点就会被删除。</p><p>其次ZooKeeper也提供了Watch特性可监听key的数据变化。</p><p><a href="https://www.usenix.org/legacy/event/atc10/tech/full_papers/Hunt.pdf">使用Zookeeper加锁的伪代码如下</a>：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs awk">Lock<br><span class="hljs-number">1</span> n = create(l + “/lock-”, EPHEMERAL|SEQUENTIAL)<br><span class="hljs-number">2</span> C = getChildren(l, false)<br><span class="hljs-number">3</span> <span class="hljs-keyword">if</span> n is lowest znode <span class="hljs-keyword">in</span> C, <span class="hljs-keyword">exit</span><br><span class="hljs-number">4</span> p = znode <span class="hljs-keyword">in</span> C ordered just before n<br><span class="hljs-number">5</span> <span class="hljs-keyword">if</span> exists(p, true) wait <span class="hljs-keyword">for</span> watch event<br><span class="hljs-number">6</span> goto <span class="hljs-number">2</span><br>Unlock<br><span class="hljs-number">1</span> <span class="hljs-keyword">delete</span>(n)<br><br></code></pre></td></tr></table></figure><p>接下来重点介绍一下基于etcd的分布式锁实现。</p><h2 id="etcd分布式锁实现"><a href="#etcd分布式锁实现" class="headerlink" title="etcd分布式锁实现"></a>etcd分布式锁实现</h2><p>那么基于etcd实现的分布式锁是如何确保安全性、互斥性、活性的呢？</p><h3 id="事务与锁的安全性"><a href="#事务与锁的安全性" class="headerlink" title="事务与锁的安全性"></a>事务与锁的安全性</h3><p>从Redis案例中我们可以看到，加锁的过程需要确保安全性、互斥性。比如，当key不存在时才能创建，否则查询相关key信息，而etcd提供的事务能力正好可以满足我们的诉求。</p><p>正如之前介绍的事务特性，它由IF语句、Then语句、Else语句组成。其中在IF语句中，支持比较key的是<strong>修改版本号mod_revision和创建版本号create_revision</strong>。</p><p>在分布式锁场景，你就可以通过key的创建版本号create_revision来检查key是否已存在，因为一个key不存在的话，它的create_revision版本号就是0。</p><p>若create_revision是0，就可发起put操作创建相关key，具体代码如下:</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs css">txn := client.<span class="hljs-built_in">Txn</span>(ctx).<span class="hljs-built_in">If</span>(v3.<span class="hljs-built_in">Compare</span>(v3.<span class="hljs-built_in">CreateRevision</span>(k),<br><span class="hljs-string">&quot;=&quot;</span>, <span class="hljs-number">0</span>))<br><br></code></pre></td></tr></table></figure><p>要注意的是，实现分布式锁的方案有多种，比如你可以通过client是否成功创建一个固定的key，来判断此client是否获得锁，你也可以通过多个client创建prefix相同，名称不一样的key，<strong>哪个key的revision最小，最终就是它获得锁。（解决惊群效应的方式之一）</strong>。</p><p>相比Redis基于主备异步复制导致锁的安全性问题，etcd是基于<strong>Raft共识算法</strong>实现的，一个写请求需要经过集群多数节点确认。因此一旦分布式锁申请返回给client成功后，它一定是持久化到了集群多数节点上，不会出现Redis主备异步复制可能导致丢数据的问题，具备更高的安全性。</p><h3 id="Lease与锁的活性"><a href="#Lease与锁的活性" class="headerlink" title="Lease与锁的活性"></a>Lease与锁的活性</h3><p>通过事务实现原子的检查key是否存在、创建key后，我们确保了分布式锁的安全性、互斥性。那么etcd是如何确保锁的活性呢? 也就是发生任何故障，都可避免出现死锁呢？</p><p>正如在租约特性中介绍的，Lease就是一种活性检测机制，它提供了检测各个客户端存活的能力。你的业务client需定期向etcd服务发送”特殊心跳”汇报健康状态，若你未正常发送心跳，并超过和etcd服务约定的最大存活时间后，就会被etcd服务移除此Lease和其关联的数据。</p><p>通过Lease机制就优雅地解决了client出现crash故障、client与etcd集群网络出现隔离等各类故障场景下的死锁问题。一旦超过Lease TTL，它就能自动被释放，确保了其他client在TTL过期后能正常申请锁，保障了业务的可用性。</p><p>具体代码如下:</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs css">txn := client.<span class="hljs-built_in">Txn</span>(ctx).<span class="hljs-built_in">If</span>(v3.<span class="hljs-built_in">Compare</span>(v3.<span class="hljs-built_in">CreateRevision</span>(k), <span class="hljs-string">&quot;=&quot;</span>, <span class="hljs-number">0</span>))<br>txn = txn.<span class="hljs-built_in">Then</span>(v3.<span class="hljs-built_in">OpPut</span>(k, val, v3.<span class="hljs-built_in">WithLease</span>(s.<span class="hljs-built_in">Lease</span>())))<br>txn = txn.<span class="hljs-built_in">Else</span>(v3.<span class="hljs-built_in">OpGet</span>(k))<br>resp, err := txn.<span class="hljs-built_in">Commit</span>()<br>if err != nil &#123;<br>    return err<br>&#125;<br><br></code></pre></td></tr></table></figure><h3 id="Watch与锁的可用性"><a href="#Watch与锁的可用性" class="headerlink" title="Watch与锁的可用性"></a>Watch与锁的可用性</h3><p>当一个持有锁的client crash故障后，其他client如何快速感知到此锁失效了，快速获得锁呢，最大程度降低锁的不可用时间呢？</p><p>答案是<strong>Watch</strong>特性。正如在Watch特性中和介绍的，Watch提供了高效的数据监听能力。当其他client收到Watch Delete事件后，就可快速判断自己是否有资格获得锁，极大减少了锁的不可用时间。</p><p>具体代码如下所示：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs maxima"><span class="hljs-built_in">var</span> wr v3.WatchResponse<br>wch := client.Watch(cctx, <span class="hljs-built_in">key</span>, v3.WithRev(rev))<br><span class="hljs-keyword">for</span> wr = <span class="hljs-built_in">range</span> wch &#123;<br>   <span class="hljs-keyword">for</span> <span class="hljs-symbol">_</span>, <span class="hljs-built_in">ev</span> := <span class="hljs-built_in">range</span> wr.Events &#123;<br>      <span class="hljs-keyword">if</span> <span class="hljs-built_in">ev</span>.Type == mvccpb.DELETE &#123;<br>         <span class="hljs-built_in">return</span> nil<br>      &#125;<br>   &#125;<br>&#125;<br><br></code></pre></td></tr></table></figure><h3 id="etcd自带的concurrency包"><a href="#etcd自带的concurrency包" class="headerlink" title="etcd自带的concurrency包"></a>etcd自带的concurrency包</h3><p>为了简化分布式锁、分布式选举、分布式事务的实现，etcd社区提供了一个名为concurrency包实现更简单、正确地使用分布式锁、分布式选举。</p><p>下面简单介绍下分布式锁 <a href="https://github.com/etcd-io/etcd/tree/v3.4.9/clientv3/concurrency">concurrency</a> 包的使用和实现，它的使用非常简单，如下代码所示，核心流程如下：</p><ul><li>首先通过concurrency.NewSession方法创建Session，本质是创建了一个TTL为10的Lease。</li><li>其次得到session对象后，通过concurrency.NewMutex创建了一个mutex对象，包含Lease、key prefix等信息。</li><li>然后通过mutex对象的Lock方法尝试获取锁。</li><li>最后使用结束，可通过mutex对象的Unlock方法释放锁。</li></ul><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs stata"><span class="hljs-keyword">cli</span>, <span class="hljs-keyword">err</span> := clientv3.New(clientv3.Config&#123;Endpoints: endpoints&#125;)<br><span class="hljs-keyword">if</span> <span class="hljs-keyword">err</span> != nil &#123;<br>   <span class="hljs-keyword">log</span>.Fatal(<span class="hljs-keyword">err</span>)<br>&#125;<br>defer <span class="hljs-keyword">cli</span>.<span class="hljs-keyword">Close</span>()<br><span class="hljs-comment">// create two separate sessions for lock competition</span><br>s1, <span class="hljs-keyword">err</span> := concurrency.NewSession(<span class="hljs-keyword">cli</span>, concurrency.WithTTL(10))<br><span class="hljs-keyword">if</span> <span class="hljs-keyword">err</span> != nil &#123;<br>   <span class="hljs-keyword">log</span>.Fatal(<span class="hljs-keyword">err</span>)<br>&#125;<br>defer s1.<span class="hljs-keyword">Close</span>()<br>m1 := concurrency.NewMutex(s1, <span class="hljs-string">&quot;/my-lock/&quot;</span>)<br><span class="hljs-comment">// acquire lock for s1</span><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">err</span> := m1.Lock(context.TODO()); <span class="hljs-keyword">err</span> != nil &#123;<br>   <span class="hljs-keyword">log</span>.Fatal(<span class="hljs-keyword">err</span>)<br>&#125;<br>fmt.Println(<span class="hljs-string">&quot;acquired lock for s1&quot;</span>)<br><span class="hljs-keyword">if</span> <span class="hljs-keyword">err</span> := m1.Unlock(context.TODO()); <span class="hljs-keyword">err</span> != nil &#123;<br>   <span class="hljs-keyword">log</span>.Fatal(<span class="hljs-keyword">err</span>)<br>&#125;<br>fmt.Println(<span class="hljs-string">&quot;released lock for s1&quot;</span>)<br><br></code></pre></td></tr></table></figure><p>那么mutex对象的Lock方法是如何加锁的呢？</p><p>核心还是使用了上面介绍的事务和Lease特性，当CreateRevision为0时，它会创建一个prefix为&#x2F;my-lock的key（ &#x2F;my-lock + LeaseID)，并获取到&#x2F;my-lock prefix下面最早创建的一个key（revision最小），分布式锁最终是由写入此key的client获得，其他client则进入等待模式。</p><p>详细代码如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs stylus">m<span class="hljs-selector-class">.myKey</span> = fmt<span class="hljs-selector-class">.Sprintf</span>(<span class="hljs-string">&quot;%s%x&quot;</span>, m<span class="hljs-selector-class">.pfx</span>, s<span class="hljs-selector-class">.Lease</span>())<br>cmp := v3<span class="hljs-selector-class">.Compare</span>(v3<span class="hljs-selector-class">.CreateRevision</span>(m.myKey), <span class="hljs-string">&quot;=&quot;</span>, <span class="hljs-number">0</span>)<br><span class="hljs-comment">// put self in lock waiters via myKey; oldest waiter holds lock</span><br>put := v3<span class="hljs-selector-class">.OpPut</span>(m<span class="hljs-selector-class">.myKey</span>, <span class="hljs-string">&quot;&quot;</span>, v3<span class="hljs-selector-class">.WithLease</span>(s<span class="hljs-selector-class">.Lease</span>()))<br><span class="hljs-comment">// reuse key in case this session already holds the lock</span><br>get := v3<span class="hljs-selector-class">.OpGet</span>(m.myKey)<br><span class="hljs-comment">// fetch current holder to complete uncontended path with only one RPC</span><br>getOwner := v3<span class="hljs-selector-class">.OpGet</span>(m<span class="hljs-selector-class">.pfx</span>, v3<span class="hljs-selector-class">.WithFirstCreate</span>()...)<br>resp, err := client<span class="hljs-selector-class">.Txn</span>(ctx)<span class="hljs-selector-class">.If</span>(cmp)<span class="hljs-selector-class">.Then</span>(put, getOwner)<span class="hljs-selector-class">.Else</span>(get, getOwner)<span class="hljs-selector-class">.Commit</span>()<br><span class="hljs-keyword">if</span> err != nil &#123;<br>   return err<br>&#125;<br><br></code></pre></td></tr></table></figure><p>那未获得锁的client是如何等待的呢?</p><p>答案是通过Watch机制各自监听prefix相同，revision比自己小的key，因为只有revision比自己小的key释放锁，才能有机会获得锁，如下代码所示，其中waitDelete会使用上面的介绍的Watch去监听比自己小的key，详细代码可参考 <a href="https://github.com/etcd-io/etcd/blob/v3.4.9/clientv3/concurrency/mutex.go">concurrency mutex</a> 的实现。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-comment">// wait for deletion revisions prior to myKey</span><br>hdr, werr := <span class="hljs-built_in">waitDeletes</span>(ctx, client, m<span class="hljs-selector-class">.pfx</span>, m.myRev-<span class="hljs-number">1</span>)<br><span class="hljs-comment">// release lock key if wait failed</span><br><span class="hljs-keyword">if</span> werr != nil &#123;<br>   m<span class="hljs-selector-class">.Unlock</span>(client<span class="hljs-selector-class">.Ctx</span>())<br>&#125; <span class="hljs-keyword">else</span> &#123;<br>   m<span class="hljs-selector-class">.hdr</span> = hdr<br>&#125;<br><br></code></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>首先通过一个Redis分布式锁实现问题——茅台超卖案例，介绍了分布式锁的三个主要核心要素，它们分别如下：</p><ul><li>安全性、互斥性。在同一时间内，不允许多个client同时获得锁。</li><li>活性。无论client出现crash还是遭遇网络分区，都需要确保任意故障场景下，都不会出现死锁，常用的解决方案是超时和自动过期机制。</li><li>高可用、高性能。加锁、释放锁的过程性能开销要尽量低，同时要保证高可用，避免单点故障。</li></ul><p>随后通过这个案例，继续分析了Redis SET命令创建分布式锁的安全性问题。单Redis Master节点存在单点故障，一主多备Redis实例又因为Redis主备异步复制，当Master节点发生crash时，可能会导致同时多个client持有分布式锁，违反了锁的安全性问题。</p><p>为了优化以上问题，Redis作者提出了<strong>RedLock</strong>分布式锁，它基于多个独立的Redis Master节点工作，只要一半以上节点存活就能正常工作，同时不依赖Redis主备异步复制，具有良好的安全性、高可用性。然而它的实现依赖于系统时间，当发生时钟跳变的时候，也会出现安全性问题。</p><p>最后重点介绍了etcd的分布式锁实现过程中的一些技术点。它通过etcd事务机制，校验CreateRevision为0才能写入相关key。若多个client同时申请锁，则client通过比较各个key的revision大小，判断是否获得锁，确保了锁的安全性、互斥性。通过Lease机制确保了锁的活性，无论client发生crash还是网络分区，都能保证不会出现死锁。通过Watch机制使其他client能快速感知到原client持有的锁已释放，提升了锁的可用性。最重要的是etcd是基于Raft协议实现的高可靠、强一致存储，正常情况下，不存在Redis主备异步复制协议导致的数据丢失问题。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><p>这节课到这里也就结束了，最后我给你留了两个思考题。</p><p>第一，死锁、脑裂、惊群效应是分布式锁的核心问题，你知道它们各自是怎么一回事吗？ZooKeeper和etcd是如何应对这些问题的呢？</p><p>死锁：加锁后由于没有添加锁的过期时间或者锁的时间设置过长，服务异常crash或者服务执行后漏执行释放锁操作，导致锁长时间没有释放</p><p>脑裂：分布式体系结构中的集中式结构（也称为 Master&#x2F;Slave 架构），一个Master节点多个Slave节点，所有的请求数据处理必须先经过Master中央服务器，由Master统一进行资源和任务调度，中央服务器根据这些信息，将任务下达给节点服务器，节点服务器执行任务，并将结果反馈给中央服务器。脑裂是在某些特殊条件下，如主备切换，Slave在与Master的网络出现故障的时候，Slave会认为Master已经故障，从而成为新的master，而原来的master也没有卸任，从而导致存在两个Master在对外服务，存在多master会导致共享资源的互斥性遭到破坏，出现资源争抢，数据不一致等问题 </p><p>惊群效应：指多进程（多线程）在同时阻塞等待同一个事件的时候（休眠状态），如果等待的这个事件发生，那么他就会唤醒等待的所有进程（或者线程），但是最终却只能有一个进程（线程）获得这个时间的“控制权”，对该事件进行处理，而其他进程（线程）获取“控制权”失败，只能重新进入休眠状态，这种现象和性能浪费就叫做惊群效应。（当你往一群鸽子中间扔一块食物，虽然最终只有一个鸽子抢到食物，但所有鸽子都会被惊动来争夺，没有抢到食物的鸽子只好回去继续睡觉， 等待下一块食物到来。这样，每扔一块食物，都会惊动所有的鸽子，即为惊群。） </p><p>etcd如何避免死锁：利用Lease的活性检测机制，它提供了检测各个客户端存活的能力。你的业务 client 需定期向 etcd 服务发送”特殊心跳”汇报健康状态，若你未正常发送心跳，并超过和 etcd 服务约定的最大存活时间后，就会被 etcd 服务移除此 Lease 和其关联的数据。通过 Lease 机制就优雅地解决了 client 出现 crash 故障、client 与 etcd 集群网络出现隔离等各类故障场景下的死锁问题。一旦超过 Lease TTL，它就能自动被释放，确保了其他 client 在 TTL 过期后能正常申请锁，保障了业务的可用性。 </p><p>etcd如何避免集群脑裂：在leader选举上采用了过半机制，即得到一半以上的follow才能成为leader，且新leader就任后旧leader必须卸任，所以etcd不存在脑裂问题 </p><p>etcd如何避免惊群效应：mutex，通过 Watch 机制各自监听 prefix 相同，revision 比自己小的 key，因为只有 revision 比自己小的 key 释放锁，才能有机会，获得锁</p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20.Kubernetes高级应用：如何优化业务场景使etcd能支撑上万节点集群？</title>
    <link href="/2022/10/15/20-Kubernetes%E9%AB%98%E7%BA%A7%E5%BA%94%E7%94%A8%EF%BC%9A%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96%E4%B8%9A%E5%8A%A1%E5%9C%BA%E6%99%AF%E4%BD%BFetcd%E8%83%BD%E6%94%AF%E6%92%91%E4%B8%8A%E4%B8%87%E8%8A%82%E7%82%B9%E9%9B%86%E7%BE%A4%EF%BC%9F/"/>
    <url>/2022/10/15/20-Kubernetes%E9%AB%98%E7%BA%A7%E5%BA%94%E7%94%A8%EF%BC%9A%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96%E4%B8%9A%E5%8A%A1%E5%9C%BA%E6%99%AF%E4%BD%BFetcd%E8%83%BD%E6%94%AF%E6%92%91%E4%B8%8A%E4%B8%87%E8%8A%82%E7%82%B9%E9%9B%86%E7%BE%A4%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="20-Kubernetes高级应用：如何优化业务场景使etcd能支撑上万节点集群？"><a href="#20-Kubernetes高级应用：如何优化业务场景使etcd能支撑上万节点集群？" class="headerlink" title="20.Kubernetes高级应用：如何优化业务场景使etcd能支撑上万节点集群？"></a>20.Kubernetes高级应用：如何优化业务场景使etcd能支撑上万节点集群？</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><p>Kubernetes社区官网文档目前声称支持最大集群节点数为5000，但是云厂商已经号称支持15000节点的Kubernetes集群了，那么为什么一个小小的etcd能支撑15000节点Kubernetes集群呢？</p><p>今天聊聊为了支撑15000节点，Kubernetes和etcd的做的一系列优化。将重点分析Kubernetes针对etcd的瓶颈是如何从应用层采取一系列优化措施，去解决大规模集群场景中各个痛点。</p><h2 id="大集群核心问题分析"><a href="#大集群核心问题分析" class="headerlink" title="大集群核心问题分析"></a>大集群核心问题分析</h2><p>在大规模Kubernetes集群中会遇到哪些问题呢？</p><p>大规模Kubernetes集群的外在表现是节点数成千上万，资源对象数量高达几十万。本质是更频繁地查询、写入更大的资源对象。</p><p>首先是查询相关问题。在大集群中最重要的就是如何最大程度地减少<strong>expensive reques</strong>t。因为对几十万级别的对象数量来说，按标签、namespace查询Pod，获取所有Node等场景时，很容易造成etcd和kube-apiserver OOM和丢包，乃至雪崩等问题发生。</p><p>其次是写入相关问题。Kubernetes为了维持上万节点的心跳，会产生大量写请求。而按照之前介绍的etcd MVCC、boltdb、线性读等原理，etcd适用场景是读多写少，大量写请求可能会导致db size持续增长、写性能达到瓶颈被限速、影响读性能。</p><p>最后是大资源对象相关问题。etcd适合存储较小的key-value数据，etcd本身也做了一系列硬限制，比如key的value大小默认不能超过1.5MB。</p><p>本讲我就和你重点分析下Kubernetes是如何优化以上问题，以实现支撑上万节点的。以及我会简单和你讲下etcd针对Kubernetes场景做了哪些优化。</p><h2 id="如何减少expensive-request"><a href="#如何减少expensive-request" class="headerlink" title="如何减少expensive request"></a>如何减少expensive request</h2><p>首先是第一个问题，Kubernetes如何减少expensive request？</p><p>在这个问题中，我将Kubernetes解决此问题的方案拆分成几个核心点和你分析。</p><h3 id="分页"><a href="#分页" class="headerlink" title="分页"></a>分页</h3><p>首先List资源操作是个基本功能点。各个组件在启动的时候，都不可避免会产生List操作，从etcd获取集群资源数据，构建初始状态。因此优化的第一步就是要避免一次性读取数十万的资源操作。</p><p>解决方案是Kubernetes List接口支持分页特性。分页特性依赖底层存储支持，早期的etcd v2并未支持分页被饱受诟病，非常容易出现kube-apiserver大流量、高负载等问题。在etcd v3中，实现了指定返回Limit数量的范围查询，因此也赋能kube-apiserver 对外提供了分页能力。</p><p>如下所示，在List接口的ListOption结构体中，Limit和Continue参数就是为了实现分页特性而增加的。</p><p>Limit表示一次List请求最多查询的对象数量，一般为500。如果实际对象数量大于Limit，kube-apiserver则会更新ListMeta的Continue字段，client发起的下一个List请求带上这个字段就可获取下一批对象数量。直到kube-apiserver返回空的Continue值，就获取完成了整个对象结果集。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-comment">// ListOptions is the query options to a standard REST</span><br><span class="hljs-built_in">list</span> call.<br><span class="hljs-keyword">type</span> ListOptions <span class="hljs-keyword">struct</span> &#123;<span class="hljs-operator"></span><br><span class="hljs-operator">   ...</span><br><span class="hljs-operator">   </span>Limit <span class="hljs-built_in">int64</span> `json:<span class="hljs-string">&quot;limit,omitempty&quot;</span><br>protobuf:<span class="hljs-string">&quot;varint,7,opt,name=limit&quot;</span>`<br>   Continue <span class="hljs-built_in">string</span> `json:<span class="hljs-string">&quot;continue,omitempty&quot;</span><br>protobuf:<span class="hljs-string">&quot;bytes,8,opt,name=continue&quot;</span>`<br>&#125;<br><br></code></pre></td></tr></table></figure><p>了解完kube-apiserver的分页特性后，我们接着往下看Continue字段具体含义，以及它是如何影响etcd查询结果的。</p><p>我们知道etcd分页是通过范围查询和Limit实现，ListOption中的Limit对应etcd查询接口中的Limit参数。你可以大胆猜测下，Continue字段是不是跟查询的范围起始key相关呢？</p><p>Continue字段的确包含查询范围的起始key，它本质上是个结构体，还包含APIVersion和ResourceVersion。你之所以看到的是一个奇怪字符串，那是因为kube-apiserver使用base64库对其进行了URL编码，下面是它的原始结构体。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">type</span> continueToken <span class="hljs-keyword">struct</span> &#123;<br>   APIVersion      <span class="hljs-type">string</span> <span class="hljs-string">`json:&quot;v&quot;`</span><br>   ResourceVersion <span class="hljs-type">int64</span>  <span class="hljs-string">`json:&quot;rv&quot;`</span><br>   StartKey        <span class="hljs-type">string</span> <span class="hljs-string">`json:&quot;start&quot;`</span><br>&#125;<br><br></code></pre></td></tr></table></figure><p>当kube-apiserver收到带Continue的分页查询时，解析Continue，获取StartKey、ResourceVersion，etcd查询Range接口指定startKey，增加clienv3.WithRange、clientv3.WithLimit、clientv3.WithRev即可。</p><h3 id="资源按namespace拆分"><a href="#资源按namespace拆分" class="headerlink" title="资源按namespace拆分"></a>资源按namespace拆分</h3><p>通过分页特性提供机制避免一次拉取大量资源对象后，接下来就是业务最佳实践上要避免同namespace存储大量资源，尽量将资源对象拆分到不同namespace下。</p><p>为什么拆分到不同namespace下有助于提升性能呢?</p><p>Kubernetes资源对象存储在etcd中的key前缀包含namespace，因此它相当于是个高效的索引字段。etcd treeIndex模块从B-tree中匹配前缀时，可快速过滤出符合条件的key-value数据。</p><p>Kubernetes社区承诺 <a href="https://github.com/kubernetes/community/blob/master/sig-scalability/slos/slos.md">SLO</a> 达标的前提是，你在使用Kubernetes集群过程中必须合理配置集群和使用扩展特性，并遵循 <a href="https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md">一系列条件限制</a>（比如同namespace下的Service数量不超过5000个）。</p><h3 id="Informer机制"><a href="#Informer机制" class="headerlink" title="Informer机制"></a>Informer机制</h3><p>各组件启动发起一轮List操作加载完初始状态数据后，就进入了控制器的一致性协调逻辑。在一致性协调逻辑中，Kubernetes使用的是Watch特性来获取数据变化通知，而不是List定时轮询，这也是减少List操作一大核心策略。</p><p>Kubernetes社区在client-go项目中提供了一个通用的Informer组件来负责client与kube-apiserver进行资源和事件同步，显著降低了开发者使用Kubernetes API、开发高性能Kubernetes扩展组件的复杂度。</p><p>Informer机制的Reflector封装了Watch、List操作，结合本地Cache、Indexer，实现了控制器加载完初始状态数据后，接下来的其他操作都只需要从本地缓存读取，极大降低了kube-apiserver和etcd的压力。</p><p>下面是Kubernetes社区给出的一个控制器使用Informer机制的架构图。黄色部分是控制器相关基础组件，蓝色部分是client-go的Informer机制的组件，它由Reflector、Queue、Informer、Indexer、Thread safe store(Local Cache)组成。</p><p><img src="https://static001.geekbang.org/resource/image/fb/99/fb7caaa37a6a860422825d2199217899.png?wh=1058*794" alt="img"></p><p>Informer机制的基本工作流程如下：</p><ul><li>client启动或与kube-apiserver出现连接中断再次Watch时，报”too old resource version”等错误后，通过Reflector组件的List操作，从kube-apiserver获取初始状态数据，随后通过Watch机制实时监听数据变化。</li><li>收到事件后添加到Delta FIFO队列，由Informer组件进行处理。</li><li>Informer将delta FIFO队列中的事件转发给Indexer组件，Indexer组件将事件持久化存储在本地的缓存中。</li><li>控制器开发者可通过Informer组件注册Add、Update、Delete事件的回调函数。Informer组件收到事件后会回调业务函数，比如典型的控制器使用场景，一般是将各个事件添加到WorkQueue中，控制器的各个协调goroutine从队列取出消息，解析key，通过key从Informer机制维护的本地Cache中读取数据。</li></ul><p>通过以上流程分析，你可以发现除了启动、连接中断等场景才会触发List操作，其他时候都是从本地Cache读取。</p><p><strong>那连接中断等场景为什么触发client List操作呢？</strong></p><h3 id="Watch-bookmark机制"><a href="#Watch-bookmark机制" class="headerlink" title="Watch bookmark机制"></a>Watch bookmark机制</h3><p>要搞懂这个问题，你得了解kube-apiserver Watch特性的原理。</p><p>接下来介绍下Kubernetes的Watch特性。我们知道Kubernetes通过全局递增的Resource Version来实现增量数据同步逻辑，尽量避免连接中断等异常场景下client发起全量List同步操作。</p><p>那么在什么场景下会触发全量List同步操作呢？这就取决于client请求的Resource Version以及kube-apiserver中是否还保存了相关的历史版本数据。</p><p>在 Watch特性中，提到实现历史版本数据存储两大核心机制，滑动窗口和MVCC。与etcd v3使用MVCC机制不一样的是，Kubernetes采用的是滑动窗口机制。</p><p>kube-apiserver的滑动窗口机制是如何实现的呢?</p><p>它通过为每个类型资源（Pod,Node等）维护一个cyclic buffer，来存储最近的一系列变更事件实现。</p><p>下面Kubernetes核心的watchCache结构体中的cache数组、startIndex、endIndex就是用来实现cyclic buffer的。滑动窗口中的第一个元素就是cache[startIndex%capacity]，最后一个元素则是cache[endIndex%capacity]。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// watchCache is a &quot;sliding window&quot; (with a limited capacity) of objects</span><br><span class="hljs-comment">// observed from a watch.</span><br><span class="hljs-keyword">type</span> watchCache <span class="hljs-keyword">struct</span> &#123;<br>   sync.RWMutex<br><br>   <span class="hljs-comment">// Condition on which lists are waiting for the fresh enough</span><br>   <span class="hljs-comment">// resource version.</span><br>   cond *sync.Cond<br><br>   <span class="hljs-comment">// Maximum size of history window.</span><br>   capacity <span class="hljs-type">int</span><br><br>   <span class="hljs-comment">// upper bound of capacity since event cache has a dynamic size.</span><br>   upperBoundCapacity <span class="hljs-type">int</span><br><br>   <span class="hljs-comment">// lower bound of capacity since event cache has a dynamic size.</span><br>   lowerBoundCapacity <span class="hljs-type">int</span><br><br>   <span class="hljs-comment">// cache is used a cyclic buffer - its first element (with the smallest</span><br>   <span class="hljs-comment">// resourceVersion) is defined by startIndex, its last element is defined</span><br>   <span class="hljs-comment">// by endIndex (if cache is full it will be startIndex + capacity).</span><br>   <span class="hljs-comment">// Both startIndex and endIndex can be greater than buffer capacity -</span><br>   <span class="hljs-comment">// you should always apply modulo capacity to get an index in cache array.</span><br>   cache      []*watchCacheEvent<br>   startIndex <span class="hljs-type">int</span><br>   endIndex   <span class="hljs-type">int</span><br><br>   <span class="hljs-comment">// store will effectively support LIST operation from the &quot;end of cache</span><br>   <span class="hljs-comment">// history&quot; i.e. from the moment just after the newest cached watched event.</span><br>   <span class="hljs-comment">// It is necessary to effectively allow clients to start watching at now.</span><br>   <span class="hljs-comment">// <span class="hljs-doctag">NOTE:</span> We assume that &lt;store&gt; is thread-safe.</span><br>   store cache.Indexer<br><br>   <span class="hljs-comment">// ResourceVersion up to which the watchCache is propagated.</span><br>   resourceVersion <span class="hljs-type">uint64</span><br>&#125;<br><br></code></pre></td></tr></table></figure><p>下面以Pod资源的历史事件滑动窗口为例，聊聊它在什么场景可能会触发client全量List同步操作。</p><p>如下图所示，kube-apiserver启动后，通过List机制，加载初始Pod状态数据，随后通过Watch机制监听最新Pod数据变化。当你不断对Pod资源进行增加、删除、修改后，携带新Resource Version（简称RV）的Pod事件就会不断被加入到cyclic buffer。假设cyclic buffer容量为100，RV1是最小的一个Watch事件的Resource Version，RV 100是最大的一个Watch事件的Resource Version。</p><p>当版本号为RV101的Pod事件到达时，RV1就会被淘汰，kube-apiserver维护的Pod最小版本号就变成了RV2。然而在Kubernetes集群中，不少组件都只关心cyclic buffer中与自己相关的事件。</p><p><img src="/images/348633/29deb02b3724edef274ce71d6a758b29.png"></p><p>比如图中的kubelet只关注运行在自己节点上的Pod，假设只有RV1是它关心的Pod事件版本号，在未实现Watch bookmark特性之前，其他RV2到RV101的事件是不会推送给它的，因此它内存中维护的Resource Version依然是RV1。</p><p>若此kubelet随后与kube-apiserver连接出现异常，它将使用版本号RV1发起Watch重连操作。但是kube-apsierver cyclic buffer中的Pod最小版本号已是RV2，因此会返回”too old resource version”错误给client，client只能发起List操作，在获取到最新版本号后，才能重新进入监听逻辑。</p><p><strong>那么能否定时将最新的版本号推送给各个client来解决以上问题呢?</strong></p><p>是的，这就是Kubernetes的Watch bookmark机制核心思想。即使队列中无client关注的更新事件，Informer机制的Reflector组件中Resource Version也需要更新。</p><p>Watch bookmark机制通过新增一个bookmark类型的事件来实现的。kube-apiserver会通过定时器将各类型资源最新的Resource Version推送给kubelet等client，在client与kube-apiserver网络异常重连等场景，大大降低了client重建Watch的开销，减少了relist expensive request。</p><h3 id="更高效的Watch恢复机制"><a href="#更高效的Watch恢复机制" class="headerlink" title="更高效的Watch恢复机制"></a>更高效的Watch恢复机制</h3><p>虽然Kubernetes社区通过Watch bookmark机制缓解了client与kube-apiserver重连等场景下可能导致的relist expensive request操作，然而在kube-apiserver重启、滚动更新时，它依然还是有可能导致大量的relist操作，这是为什么呢？ 如何进一步减少kube-apiserver重启场景下的List操作呢？</p><p>如下图所示，在kube-apiserver重启后，kubelet等client会立刻带上Resource Version发起重建Watch的请求。问题就在kube-apiserver重启后，watchCache中的cyclic buffer是空的，此时watchCache中的最小Resource Version(listResourceVersion)是etcd的最新全局版本号，也就是图中的RV200。</p><p><img src="/images/348633/e1694c3dce75b310b9950f3e3yydd2d2.png"></p><p>在不少场景下，client请求重建Watch的Resource Version是可能小于listResourceVersion的。</p><p>比如在上面的这个案例图中，集群内Pod稳定运行未发生变化，kubelet假设收到了最新的RV100事件。然而这个集群其他资源如ConfigMap，被管理员不断的修改，它就会导致导致etcd版本号新增，ConfigMap滑动窗口也会不断存储变更事件，从图中可以看到，它记录最大版本号为RV200。</p><p>因此kube-apiserver重启后，client请求重建Pod Watch的Resource Version是RV100，而Pod watchCache中的滑动窗口最小Resource Version是RV200。很显然，RV100不在Pod watchCache所维护的滑动窗口中，kube-apiserver就会返回”too old resource version”错误给client，client只能发起relist expensive request操作同步最新数据。</p><p>为了进一步降低kube-apiserver重启对client Watch中断的影响，Kubernetes在1.20版本中又进一步实现了 <a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1904-efficient-watch-resumption">更高效的Watch恢复机制</a>。它通过etcd Watch机制的<strong>Notify</strong>特性，实现了将etcd最新的版本号定时推送给kube-apiserver。kube-apiserver在将其转换成ResourceVersion后，再通过bookmark机制推送给client，避免了kube-apiserver重启后client可能发起的List操作。</p><h2 id="如何控制db-size"><a href="#如何控制db-size" class="headerlink" title="如何控制db size"></a>如何控制db size</h2><p>分析完Kubernetes如何减少expensive request，再看看Kubernetes是如何控制db size的。</p><p>首先，Kubernetes的kubelet组件会每隔10秒上报一次心跳给kube-apiserver。</p><p>其次，Node资源对象因为包含若干个镜像、数据卷等信息，导致Node资源对象会较大，一次心跳消息可能高达15KB以上。</p><p>最后，etcd是基于COW(Copy-on-write)机制实现的MVCC数据库，每次修改都会产生新的key-value，若大量写入会导致db size持续增长。</p><p>早期Kubernetes集群由于以上原因，当节点数成千上万时，kubelet产生的大量写请求就较容易造成db大小达到配额，无法写入。</p><p>那么如何解决呢？</p><p>本质上还是Node资源对象大的问题。实际上我们需要更新的仅仅是Node资源对象的心跳状态，而在etcd中我们存储的是整个Node资源对象，并未将心跳状态拆分出来。</p><p>因此Kuberentes的解决方案就是将Node资源进行拆分，把心跳状态信息从Node对象中剥离出来，通过下面的Lease对象来描述它。</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">// Lease defines a lease concept.<br><span class="hljs-keyword">type</span> Lease struct &#123;<br>   metav1.TypeMeta `<span class="hljs-type">json</span>:&quot;,inline&quot;`<br>   metav1.ObjectMeta `<span class="hljs-type">json</span>:&quot;metadata,omitempty&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;`<br>   Spec LeaseSpec `<span class="hljs-type">json</span>:&quot;spec,omitempty&quot; protobuf:&quot;bytes,2,opt,name=spec&quot;`<br>&#125;<br><br>// LeaseSpec <span class="hljs-keyword">is</span> a specification <span class="hljs-keyword">of</span> a Lease.<br><span class="hljs-keyword">type</span> LeaseSpec struct &#123;<br>   HolderIdentity *string `<span class="hljs-type">json</span>:&quot;holderIdentity,omitempty&quot; protobuf:&quot;bytes,1,opt,name=holderIdentity&quot;`<br>   LeaseDurationSeconds *int32 `<span class="hljs-type">json</span>:&quot;leaseDurationSeconds,omitempty&quot; protobuf:&quot;varint,2,opt,name=leaseDurationSeconds&quot;`<br>   AcquireTime *metav1.MicroTime `<span class="hljs-type">json</span>:&quot;acquireTime,omitempty&quot; protobuf:&quot;bytes,3,opt,name=acquireTime&quot;`<br>   RenewTime *metav1.MicroTime `<span class="hljs-type">json</span>:&quot;renewTime,omitempty&quot; protobuf:&quot;bytes,4,opt,name=renewTime&quot;`<br>   LeaseTransitions *int32 `<span class="hljs-type">json</span>:&quot;leaseTransitions,omitempty&quot; protobuf:&quot;varint,5,opt,name=leaseTransitions&quot;`<br>&#125;<br><br></code></pre></td></tr></table></figure><p>因为<strong>Lease对象非常小，更新的代价远小于Node对象</strong>，所以这样显著降低了kube-apiserver的CPU开销、etcd db size，Kubernetes 1.14版本后已经默认启用Node心跳切换到Lease API。</p><h2 id="如何优化key-value大小"><a href="#如何优化key-value大小" class="headerlink" title="如何优化key-value大小"></a>如何优化key-value大小</h2><p>最后，再看看Kubernetes是如何解决etcd key-value大小限制的。</p><p>在成千上万个节点的集群中，一个服务可能背后有上万个Pod。而服务对应的Endpoints资源含有大量的独立的endpoints信息，这会导致Endpoints资源大小达到etcd的value大小限制，etcd拒绝更新。</p><p>另外，kube-proxy等组件会实时监听Endpoints资源，一个endpoint变化就会产生较大的流量，导致kube-apiserver等组件流量超大、出现一系列性能瓶颈。</p><p>如何解决以上Endpoints资源过大的问题呢？</p><p>答案依然是<strong>拆分、化大为小</strong>。Kubernetes社区设计了EndpointSlice概念，每个EndpointSlice最大支持保存100个endpoints，成功解决了key-value过大、变更同步导致流量超大等一系列瓶颈。</p><h2 id="etcd优化"><a href="#etcd优化" class="headerlink" title="etcd优化"></a>etcd优化</h2><p>Kubernetes社区在解决大集群的挑战的同时，etcd社区也在不断优化、新增特性，提升etcd在Kubernetes场景下的稳定性和性能。这里简单列举两个，一个是etcd并发读特性，一个是Watch特性的Notify机制。</p><h3 id="并发读特性"><a href="#并发读特性" class="headerlink" title="并发读特性"></a>并发读特性</h3><p>通过以上介绍的各种机制、策略，虽然Kubernetes能大大缓解expensive read request问题，但是它并不是从本质上来解决问题的。</p><p>为什么etcd无法支持大量的read expensive request呢？</p><p>除了一直强调的容易导致OOM、大流量导致丢包外，etcd根本性瓶颈是在etcd 3.4版本之前，expensive read request会长时间持有MVCC模块的buffer读锁RLock。而写请求执行完后，需升级锁至Lock，expensive request导致写事务阻塞在升级锁过程中，最终导致写请求超时。</p><p>为了解决此问题，etcd 3.4版本实现了并发读特性。核心解决方案是去掉了读写锁，每个读事务拥有一个buffer。在收到读请求创建读事务对象时，全量拷贝写事务维护的buffer到读事务buffer中。</p><p>通过并发读特性，显著降低了List Pod和CRD等expensive read request对写性能的影响，延时不再突增、抖动。</p><h3 id="改善Watch-Notify机制"><a href="#改善Watch-Notify机制" class="headerlink" title="改善Watch Notify机制"></a>改善Watch Notify机制</h3><p>为了配合Kubernetes社区实现更高效的Watch恢复机制，etcd改善了Watch Notify机制，早期Notify消息发送间隔是固定的10分钟。</p><p>在etcd 3.4.11版本中，新增了–experimental-watch-progress-notify-interval参数使Notify间隔时间可配置，最小支持为100ms，满足了Kubernetes业务场景的诉求。</p><p>最后，要注意的是，默认通过clientv3 Watch API创建的watcher是不会开启此特性的。需要创建Watcher的时候，设置clientv3.WithProgressNotify选项，这样etcd server就会定时发送提醒消息给client，消息中就会携带etcd当前最新的全局版本号。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>首先剖析了大集群核心问题，即expensive request、db size、key-value大小。</p><p>针对expensive request，阐述了Kubernetes的分页机制、资源按namespace拆分部署策略、核心的Informer机制、优化client与kube-apiserver连接异常后的Watch恢复效率的bookmark机制、以及进一步优化kube-apiserver重建场景下Watch恢复效率的Notify机制。从这个问题优化思路中可以看到，优化无止境。从大方向到边界问题，Kubernetes社区一步步将expensive request降低到极致。</p><p>针对db size和key-value大小，Kubernetes社区的解决方案核心思想是拆分，通过Lease和EndpointSlice资源对象成功解决了大规模集群过程遇到db size和key-value瓶颈。</p><p>最后etcd社区也在努力提升、优化相关特性，etcd 3.4版本中的并发读特性和可配置化的Watch Notify间隔时间就是最典型的案例。自从etcd被redhat捐赠给CNCF后，etcd核心就围绕着Kubernetes社区展开工作，努力打造更快、更稳的etcd。</p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>19.Kubernetes基础应用：创建一个Pod背后etcd发生了什么？</title>
    <link href="/2022/10/15/19-Kubernetes%E5%9F%BA%E7%A1%80%E5%BA%94%E7%94%A8%EF%BC%9A%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AAPod%E8%83%8C%E5%90%8Eetcd%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F/"/>
    <url>/2022/10/15/19-Kubernetes%E5%9F%BA%E7%A1%80%E5%BA%94%E7%94%A8%EF%BC%9A%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AAPod%E8%83%8C%E5%90%8Eetcd%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="19-Kubernetes基础应用：创建一个Pod背后etcd发生了什么？"><a href="#19-Kubernetes基础应用：创建一个Pod背后etcd发生了什么？" class="headerlink" title="19.Kubernetes基础应用：创建一个Pod背后etcd发生了什么？"></a>19.Kubernetes基础应用：创建一个Pod背后etcd发生了什么？</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><p>今天我们通过在Kubernetes集群中创建一个Pod的案例，分析etcd在其中发挥的作用，深入了解Kubernetes是如何使用etcd的。</p><h2 id="Kubernetes基础架构"><a href="#Kubernetes基础架构" class="headerlink" title="Kubernetes基础架构"></a>Kubernetes基础架构</h2><p>在详细了解etcd在Kubernetes里的应用之前，先简单介绍下Kubernetes集群的整体架构，搞清楚etcd在Kubernetes集群中扮演的角色与作用。</p><p>下图是Kubernetes集群的架构图（ <a href="https://kubernetes.io/docs/concepts/overview/components/">引用自Kubernetes官方文档</a>），从图中你可以看到，它由Master节点和Node节点组成。</p><p><img src="https://static001.geekbang.org/resource/image/b1/c0/b13d665a0e5be852c050d09c8602e4c0.png?wh=1920*831" alt="img"></p><p>控制面Master节点主要包含以下组件：</p><ul><li>kube-apiserver，负责对外提供集群各类资源的增删改查及Watch接口，它是Kubernetes集群中各组件数据交互和通信的枢纽。kube-apiserver在设计上可水平扩展，高可用Kubernetes集群中一般多副本部署。当收到一个创建Pod写请求时，它的基本流程是对请求进行认证、限速、授权、准入机制等检查后，写入到etcd即可。</li><li>kube-scheduler是调度器组件，负责集群Pod的调度。基本原理是通过监听kube-apiserver获取待调度的Pod，然后基于一系列筛选和评优算法，为Pod分配最佳的Node节点。</li><li>kube-controller-manager包含一系列的控制器组件，比如Deployment、StatefulSet等控制器。控制器的核心思想是监听、比较资源实际状态与期望状态是否一致，若不一致则进行协调工作使其最终一致。</li><li>etcd组件，Kubernetes的元数据存储。</li></ul><p>Node节点主要包含以下组件：</p><ul><li>kubelet，部署在每个节点上的Agent的组件，负责Pod的创建运行。基本原理是通过监听APIServer获取分配到其节点上的Pod，然后根据Pod的规格详情，调用运行时组件创建pause和业务容器等。</li><li>kube-proxy，部署在每个节点上的网络代理组件。基本原理是通过监听APIServer获取Service、Endpoint等资源，基于Iptables、IPVS等技术实现数据包转发等功能。</li></ul><p>从Kubernetes基础架构介绍中可以看到，kube-apiserver是唯一直接与etcd打交道的组件，各组件都通过kube-apiserver实现数据交互，它们极度依赖kube-apiserver提供的资源变化 <strong>监听机制</strong>。而kube-apiserver对外提供的监听机制，也正是etcd <strong>Watch特性</strong> 提供的底层支持。</p><h2 id="创建Pod案例"><a href="#创建Pod案例" class="headerlink" title="创建Pod案例"></a>创建Pod案例</h2><p>接下来就以在Kubernetes集群中创建一个nginx服务为例，通过这个案例来详细分析etcd在Kubernetes集群创建Pod背后是如何工作的。</p><p>下面是创建一个nginx服务的YAML文件，Workload是Deployment，期望的副本数是1。</p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">apiVersion</span><span class="hljs-punctuation">:</span> <span class="hljs-string">apps/v1</span><br><span class="hljs-attribute">kind</span><span class="hljs-punctuation">:</span> <span class="hljs-string">Deployment</span><br><span class="hljs-attribute">metadata</span><span class="hljs-punctuation">:</span><br>  <span class="hljs-attribute">name</span><span class="hljs-punctuation">:</span> <span class="hljs-string">nginx-deployment</span><br>  <span class="hljs-attribute">labels</span><span class="hljs-punctuation">:</span><br>    <span class="hljs-attribute">app</span><span class="hljs-punctuation">:</span> <span class="hljs-string">nginx</span><br><span class="hljs-attribute">spec</span><span class="hljs-punctuation">:</span><br>  <span class="hljs-attribute">replicas</span><span class="hljs-punctuation">:</span> <span class="hljs-string">1</span><br>  <span class="hljs-attribute">selector</span><span class="hljs-punctuation">:</span><br>    <span class="hljs-attribute">matchLabels</span><span class="hljs-punctuation">:</span><br>      <span class="hljs-attribute">app</span><span class="hljs-punctuation">:</span> <span class="hljs-string">nginx</span><br>  <span class="hljs-attribute">template</span><span class="hljs-punctuation">:</span><br>    <span class="hljs-attribute">metadata</span><span class="hljs-punctuation">:</span><br>      <span class="hljs-attribute">labels</span><span class="hljs-punctuation">:</span><br>        <span class="hljs-attribute">app</span><span class="hljs-punctuation">:</span> <span class="hljs-string">nginx</span><br>    <span class="hljs-attribute">spec</span><span class="hljs-punctuation">:</span><br>      <span class="hljs-attribute">containers</span><span class="hljs-punctuation">:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">name: nginx</span><br>        <span class="hljs-attribute">image</span><span class="hljs-punctuation">:</span> <span class="hljs-string">nginx:1.14.2</span><br>        <span class="hljs-attribute">ports</span><span class="hljs-punctuation">:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">containerPort: 80</span><br><br></code></pre></td></tr></table></figure><p>假设此YAML文件名为nginx.yaml，首先通过如下的kubectl create -f nginx.yml命令创建Deployment资源。</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs powershell"><span class="hljs-variable">$</span> kubectl create <span class="hljs-operator">-f</span> nginx.yml<br>deployment.apps/nginx<span class="hljs-literal">-deployment</span> created<br><br></code></pre></td></tr></table></figure><p>创建之后，我们立刻通过如下命令，带标签查询Pod，输出如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs routeros">$ kubectl <span class="hljs-built_in">get</span> po -l <span class="hljs-attribute">app</span>=nginx<br>NAME                                READY   STATUS    RESTARTS   AGE<br>nginx-deployment-756d9fd5f9-fkqnf   1/1     Running   0          8s<br><br></code></pre></td></tr></table></figure><p>那么在kubectl create命令发出，nginx Deployment资源成功创建的背后，kube-apiserver是如何与etcd打交道的呢？ 它是通过什么接口 <strong>安全写入</strong> 资源到etcd的？</p><p>同时，使用kubectl带标签查询Pod背后，kube-apiserver是直接从 <strong>缓存读取</strong> 还是向etcd发出一个 <strong>线性读</strong> 或 <strong>串行读</strong> 请求呢？ 若同namespace下存在大量的Pod，此操作性能又是怎样的呢?</p><p>接下来聊聊kube-apiserver收到创建和查询请求后，是如何与etcd交互的。</p><h2 id="kube-apiserver请求执行链路"><a href="#kube-apiserver请求执行链路" class="headerlink" title="kube-apiserver请求执行链路"></a>kube-apiserver请求执行链路</h2><p>kube-apiserver作为Kubernetes集群交互的枢纽、对外提供API供用户访问的组件，因此保障集群安全、保障本身及后端etcd的稳定性的等重任也是非它莫属。比如校验创建请求发起者是否合法、是否有权限操作相关资源、是否出现Bug产生大量写和读请求等。</p><p><a href="https://speakerdeck.com/sttts/kubernetes-api-codebase-tour?slide=18">下图是kube-apiserver的请求执行链路</a>（引用自sttts分享的PDF），当收到一个请求后，它主要经过以下处理链路来完成以上若干职责后，才能与etcd交互。</p><p>核心链路如下：</p><ul><li>认证模块，校验发起的请求的用户身份是否合法。支持多种方式，比如x509客户端证书认证、静态token认证、webhook认证等。</li><li>限速模块，对请求进行简单的限速，默认读400&#x2F;s写200&#x2F;s，不支持根据请求类型进行分类、按优先级限速，存在较多问题。Kubernetes 1.19后已新增Priority and Fairness特性取代它，它支持将请求重要程度分类进行限速，支持多租户，可有效保障Leader选举之类的高优先级请求得到及时响应，能防止一个异常client导致整个集群被限速。</li><li>审计模块，可记录用户对资源的详细操作行为。</li><li>授权模块，检查用户是否有权限对其访问的资源进行相关操作。支持多种方式，RBAC(Role-based access control)、ABAC(Attribute-based access control)、Webhhook等。Kubernetes 1.12版本后，默认授权机制使用的RBAC。</li><li>准入控制模块，提供在访问资源前拦截请求的静态和动态扩展能力，比如要求镜像的拉取策略始终为AlwaysPullImages。</li></ul><p><img src="https://static001.geekbang.org/resource/image/56/bc/561f38086df49d17ee4e12ec3c5220bc.png?wh=1920*1078" alt="img"></p><p>经过上面一系列的模块检查后，这时kube-apiserver就开始与etcd打交道了。在了解kube-apiserver如何将我们创建的Deployment资源写入到etcd前，先介绍下Kubernetes的资源是如何组织、存储在etcd中。</p><h2 id="Kubernetes资源存储格式"><a href="#Kubernetes资源存储格式" class="headerlink" title="Kubernetes资源存储格式"></a>Kubernetes资源存储格式</h2><p>etcd仅仅是个key-value存储，但是在Kubernetes中存在各种各样的资源，并提供了以下几种灵活的资源查询方式：</p><ul><li>按具体资源名称查询，比如PodName、kubectl get po&#x2F;PodName。</li><li>按namespace查询，获取一个namespace下的所有Pod，比如kubectl get po -n kube-system。</li><li>按标签名，标签是极度灵活的一种方式，你可以为你的Kubernetes资源打上各种各样的标签，比如上面案例中的kubectl get po -l app&#x3D;nginx。</li></ul><p>你知道以上这几种查询方式它们的性能优劣吗？假设你是Kubernetes开发者，你会如何设计存储格式来满足以上功能点？</p><p>首先是按具体资源名称查询。它本质就是个key-value查询，只需要写入etcd的key名称与资源key一致即可。</p><p>其次是按namespace查询。这种查询也并不难。因为我们知道etcd支持范围查询，若key名称前缀包含namespace、资源类型，查询的时候指定namespace和资源类型的组合的最小开始区间、最大结束区间即可。</p><p>最后是标签名查询。这种查询方式非常灵活，业务可随时添加、删除标签，各种标签可相互组合。实现标签查询的办法主要有以下两种：</p><ul><li>方案一，在etcd中存储标签数据，实现通过标签可快速定位（时间复杂度O(1)）到具体资源名称。然而一个标签可能容易实现，但是在Kubernetes集群中，它支持按各个标签组合查询，各个标签组合后的数量相当庞大。在etcd中维护各种标签组合对应的资源列表，会显著增加kube-apiserver的实现复杂度，导致更频繁的etcd写入。</li><li>方案二，在etcd中不存储标签数据，而是由kube-apiserver通过范围遍历etcd获取原始数据，然后基于用户指定标签，来筛选符合条件的资源返回给client。此方案优点是实现简单，但是大量标签查询可能会导致etcd大流量等异常情况发生。</li></ul><p>那么Kubernetes集群选择的是哪种实现方式呢?</p><p>下面是一个Kubernetes集群中的coredns一系列资源在etcd中的存储格式：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/registry/</span>clusterrolebindings/system:coredns<br><span class="hljs-regexp">/registry/</span>clusterroles/system:coredns<br><span class="hljs-regexp">/registry/</span>configmaps<span class="hljs-regexp">/kube-system/</span>coredns<br><span class="hljs-regexp">/registry/</span>deployments<span class="hljs-regexp">/kube-system/</span>coredns<br><span class="hljs-regexp">/registry/</span>events<span class="hljs-regexp">/kube-system/</span>coredns-<span class="hljs-number">7</span>fcc6d65dc-<span class="hljs-number">6</span>njlg.<span class="hljs-number">1662</span>c287aabf742b<br><span class="hljs-regexp">/registry/</span>events<span class="hljs-regexp">/kube-system/</span>coredns-<span class="hljs-number">7</span>fcc6d65dc-<span class="hljs-number">6</span>njlg.<span class="hljs-number">1662</span>c288232143ae<br><span class="hljs-regexp">/registry/</span>pods<span class="hljs-regexp">/kube-system/</span>coredns-<span class="hljs-number">7</span>fcc6d65dc-jvj26<br><span class="hljs-regexp">/registry/</span>pods<span class="hljs-regexp">/kube-system/</span>coredns-<span class="hljs-number">7</span>fcc6d65dc-mgvtb<br><span class="hljs-regexp">/registry/</span>pods<span class="hljs-regexp">/kube-system/</span>coredns-<span class="hljs-number">7</span>fcc6d65dc-whzq9<br><span class="hljs-regexp">/registry/</span>replicasets<span class="hljs-regexp">/kube-system/</span>coredns-<span class="hljs-number">7</span>fcc6d65dc<br><span class="hljs-regexp">/registry/</span>secrets<span class="hljs-regexp">/kube-system/</span>coredns-token-hpqbt<br><span class="hljs-regexp">/registry/</span>serviceaccounts<span class="hljs-regexp">/kube-system/</span>coredns<br><br></code></pre></td></tr></table></figure><p>从中我们可以看到，一方面Kubernetes资源在etcd中的存储格式由prefix + “&#x2F;“ + 资源类型 + “&#x2F;“ + namespace + “&#x2F;“ + 具体资源名组成，基于etcd提供的范围查询能力，非常简单地支持了按具体资源名称查询和namespace查询。</p><p>kube-apiserver提供了如下参数给你配置etcd prefix，并支持将资源存储在多个etcd集群。</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver"><span class="hljs-comment">--etcd-prefix string     Default: &quot;/registry&quot;</span><br>The prefix <span class="hljs-built_in">to</span> prepend <span class="hljs-built_in">to</span> all resource paths <span class="hljs-keyword">in</span> etcd.<br><span class="hljs-comment">--etcd-servers stringSlice</span><br>List <span class="hljs-keyword">of</span> etcd servers <span class="hljs-built_in">to</span> connect <span class="hljs-keyword">with</span> (scheme://ip:port), <span class="hljs-literal">comma</span> separated.<br><span class="hljs-comment">--etcd-servers-overrides stringSlice</span><br>Per-resource etcd servers overrides, <span class="hljs-literal">comma</span> separated. The individual override <span class="hljs-built_in">format</span>: group/resource<span class="hljs-comment">#servers, where servers are URLs,</span><br>semicolon separated.<br><br></code></pre></td></tr></table></figure><p>另一方面，我们未看到任何标签相关的key。Kubernetes实现标签查询的方式显然是方案二，即由kube-apiserver通过范围遍历etcd获取原始数据，然后基于用户指定标签，来筛选符合条件的资源返回给client（资源key的value中记录了资源YAML文件内容等，如标签）。</p><p>也就是当执行”kubectl get po -l app&#x3D;nginx”命令，按标签查询Pod时，它会向etcd发起一个范围遍历整个default namespace下的Pod操作。</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs awk">$ kubectl get po -l app=nginx -v <span class="hljs-number">8</span><br>I0301 <span class="hljs-number">23</span>:<span class="hljs-number">45</span>:<span class="hljs-number">25.597465</span>   <span class="hljs-number">32411</span> loader.go:<span class="hljs-number">359</span>] Config loaded from file <span class="hljs-regexp">/root/</span>.kube/config<br>I0301 <span class="hljs-number">23</span>:<span class="hljs-number">45</span>:<span class="hljs-number">25.603182</span>   <span class="hljs-number">32411</span> round_trippers.go:<span class="hljs-number">416</span>] GET https:<span class="hljs-regexp">//i</span>p:port<span class="hljs-regexp">/api/</span>v1<span class="hljs-regexp">/namespaces/</span>default/pods?<br>labelSelector=app%<span class="hljs-number">3</span>Dnginx&amp;limit=<span class="hljs-number">500</span><br><br></code></pre></td></tr></table></figure><p>etcd收到的请求日志如下，由此可见当一个namespace存在大量Pod等资源时，若频繁通过kubectl，使用标签查询Pod等资源，后端etcd将出现较大的压力。</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs elixir">&#123;<br>    <span class="hljs-string">&quot;level&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;debug&quot;</span></span>,<br>    <span class="hljs-string">&quot;ts&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;2021-03-01T23:45:25.609+0800&quot;</span></span>,<br>    <span class="hljs-string">&quot;caller&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;v3rpc/interceptor.go:181&quot;</span></span>,<br>    <span class="hljs-string">&quot;msg&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;request stats&quot;</span></span>,<br>    <span class="hljs-string">&quot;start time&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;2021-03-01T23:45:25.608+0800&quot;</span></span>,<br>    <span class="hljs-string">&quot;time spent&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;1.414135ms&quot;</span></span>,<br>    <span class="hljs-string">&quot;remote&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;127.0.0.1:44664&quot;</span></span>,<br>    <span class="hljs-string">&quot;response type&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;/etcdserverpb.KV/Range&quot;</span></span>,<br>    <span class="hljs-string">&quot;request count&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">0</span>,<br>    <span class="hljs-string">&quot;request size&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">61</span>,<br>    <span class="hljs-string">&quot;response count&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">11</span>,<br>    <span class="hljs-string">&quot;response size&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">81478</span>,<br>    <span class="hljs-string">&quot;request content&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;key:&quot;</span>/registry/pods/default/<span class="hljs-string">&quot; range_end:&quot;</span>/registry/pods/default0<span class="hljs-string">&quot; limit:500 &quot;</span></span><br>&#125;<br><br></code></pre></td></tr></table></figure><p>了解完Kubernetes资源的存储格式后，我们再看看nginx Deployment资源是如何由kube-apiserver写入etcd的。</p><h2 id="通用存储模块"><a href="#通用存储模块" class="headerlink" title="通用存储模块"></a>通用存储模块</h2><p>kube-apiserver启动的时候，会将每个资源的APIGroup、Version、Resource Handler注册到路由上。当请求经过认证、限速、授权、准入控制模块检查后，请求就会被转发到对应的资源逻辑进行处理。</p><p>同时，kube-apiserver实现了类似数据库ORM机制的通用资源存储机制，提供了对一个资源创建、更新、删除前后的hook能力，将其封装成策略接口。当你新增一个资源时，你只需要编写相应的创建、更新、删除等策略即可，不需要写任何etcd的API。</p><p>下面是kube-apiserver通用存储模块的创建流程图：</p><p><img src="https://static001.geekbang.org/resource/image/4d/09/4d8fa0f1d6afd89cf6463cf22c56b709.png?wh=1920*1178" alt="img"></p><p>从图中可以看到，创建一个资源主要由BeforeCreate、Storage.Create以及AfterCreate三大步骤组成。</p><p>当收到创建nginx Deployment请求后，通用存储模块首先会回调各个资源自定义实现的BeforeCreate策略，为资源写入etcd做一些初始化工作。</p><p>下面是Deployment资源的创建策略实现，它会进行将deployment.Generation设置为1等操作。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-comment">// PrepareForCreate clears fields that are not allowed to be set by end users on creation.</span><br>func (deploymentStrategy) <span class="hljs-built_in">PrepareForCreate</span>(ctx context<span class="hljs-selector-class">.Context</span>, obj runtime.Object) &#123;<br>   deployment := obj.(*apps.Deployment)<br>   deployment<span class="hljs-selector-class">.Status</span> = apps.DeploymentStatus&#123;&#125;<br>   deployment<span class="hljs-selector-class">.Generation</span> = <span class="hljs-number">1</span><br><br>   pod<span class="hljs-selector-class">.DropDisabledTemplateFields</span>(&amp;deployment<span class="hljs-selector-class">.Spec</span><span class="hljs-selector-class">.Template</span>, nil)<br>&#125;<br><br></code></pre></td></tr></table></figure><p>执行完BeforeCreate策略后，它就会执行Storage.Create接口，也就是由它真正开始调用底层存储模块etcd3，将nginx Deployment资源对象写入etcd。</p><p>那么Kubernetes是使用etcd Put接口写入资源key-value的吗？如果是，那要如何防止同名资源并发创建被覆盖的问题？</p><h3 id="资源安全创建及更新"><a href="#资源安全创建及更新" class="headerlink" title="资源安全创建及更新"></a>资源安全创建及更新</h3><p>我们知道etcd提供了Put和Txn接口给业务添加key-value数据，但是Put接口在并发场景下若收到key相同的资源创建，就会导致被覆盖。</p><p>因此Kubernetes很显然无法直接通过etcd Put接口来写入数据。</p><p>etcd的事务接口Txn，它正是为了多key原子更新、并发操作安全性等而诞生的，它提供了丰富的冲突检查机制。</p><p>Kubernetes集群使用的正是事务Txn接口来防止并发创建、更新被覆盖等问题。当执行完BeforeCreate策略后，这时kube-apiserver就会调用Storage的模块的Create接口写入资源。1.6版本后的Kubernete集群默认使用的存储是etcd3，它的创建接口简要实现如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-comment">// Create implements storage.Interface.Create.</span><br>func (s *store) <span class="hljs-built_in">Create</span>(ctx context<span class="hljs-selector-class">.Context</span>, key string, obj, out runtime<span class="hljs-selector-class">.Object</span>, ttl uint64) error &#123;<br>   ......<br>   key = path<span class="hljs-selector-class">.Join</span>(s<span class="hljs-selector-class">.pathPrefix</span>, key)<br><br>   opts, err := s<span class="hljs-selector-class">.ttlOpts</span>(ctx, <span class="hljs-built_in">int64</span>(ttl))<br>   <span class="hljs-keyword">if</span> err != nil &#123;<br>      return err<br>   &#125;<br><br>   newData, err := s<span class="hljs-selector-class">.transformer</span><span class="hljs-selector-class">.TransformToStorage</span>(data, <span class="hljs-built_in">authenticatedDataString</span>(key))<br>   <span class="hljs-keyword">if</span> err != nil &#123;<br>      return storage<span class="hljs-selector-class">.NewInternalError</span>(err<span class="hljs-selector-class">.Error</span>())<br>   &#125;<br><br>   startTime := <span class="hljs-selector-tag">time</span><span class="hljs-selector-class">.Now</span>()<br>   txnResp, err := s<span class="hljs-selector-class">.client</span><span class="hljs-selector-class">.KV</span><span class="hljs-selector-class">.Txn</span>(ctx)<span class="hljs-selector-class">.If</span>(<br>      <span class="hljs-built_in">notFound</span>(key),<br>   )<span class="hljs-selector-class">.Then</span>(<br>      clientv3<span class="hljs-selector-class">.OpPut</span>(key, <span class="hljs-built_in">string</span>(newData), opts...),<br>   )<span class="hljs-selector-class">.Commit</span><br><br></code></pre></td></tr></table></figure><p>从上面的代码片段中，我们可以得出首先它会按照我们介绍的Kubernetes资源存储格式拼接key。</p><p>然后若TTL非0，它会根据TTL从leaseManager获取可复用的Lease ID。Kubernetes集群默认若不同key（如Kubernetes的Event资源对象）的TTL差异在1分钟内，可复用同一个Lease ID，避免大量Lease影响etcd性能和稳定性。</p><p>其次若开启了数据加密，在写入etcd前数据还将按加密算法进行转换工作。</p><p>最后就是使用etcd的Txn接口，向etcd发起一个创建deployment资源的Txn请求。</p><p>那么etcd收到kube-apiserver的请求是长什么样子的呢？</p><p>下面是etcd收到创建nginx deployment资源的请求日志：</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs elixir">&#123;<br>    <span class="hljs-string">&quot;level&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;debug&quot;</span></span>,<br>    <span class="hljs-string">&quot;ts&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;2021-02-11T09:55:45.914+0800&quot;</span></span>,<br>    <span class="hljs-string">&quot;caller&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;v3rpc/interceptor.go:181&quot;</span></span>,<br>    <span class="hljs-string">&quot;msg&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;request stats&quot;</span></span>,<br>    <span class="hljs-string">&quot;start time&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;2021-02-11T09:55:45.911+0800&quot;</span></span>,<br>    <span class="hljs-string">&quot;time spent&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;2.697925ms&quot;</span></span>,<br>    <span class="hljs-string">&quot;remote&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;127.0.0.1:44822&quot;</span></span>,<br>    <span class="hljs-string">&quot;response type&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;/etcdserverpb.KV/Txn&quot;</span></span>,<br>    <span class="hljs-string">&quot;request count&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">1</span>,<br>    <span class="hljs-string">&quot;request size&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">479</span>,<br>    <span class="hljs-string">&quot;response count&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">0</span>,<br>    <span class="hljs-string">&quot;response size&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">44</span>,<br>    <span class="hljs-string">&quot;request content&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;compare:&lt;target:MOD key:&quot;</span>/registry/deployments/default/nginx-deployment<span class="hljs-string">&quot; mod_revision:0 &gt; success:&lt;request_put:&lt;key:&quot;</span>/registry/deployments/default/nginx-deployment<span class="hljs-string">&quot; value_size:421 &gt;&gt; failure:&lt;&gt;&quot;</span></span><br>&#125;<br><br></code></pre></td></tr></table></figure><p>从这个请求日志中，你可以得到以下信息：</p><ul><li>请求的模块和接口，KV&#x2F;Txn；</li><li>key路径，&#x2F;registry&#x2F;deployments&#x2F;default&#x2F;nginx-deployment，由prefix + “&#x2F;“ + 资源类型 + “&#x2F;“ + namespace + “&#x2F;“ + 具体资源名组成；</li><li>安全的并发创建检查机制，mod_revision为0时，也就是此key不存在时，才允许执行put更新操作。</li></ul><p>通过Txn接口成功将数据写入到etcd后，kubectl create -f nginx.yml命令就执行完毕，返回给client了。在以上介绍中你可以看到，kube-apiserver并没有任何逻辑去真正创建Pod，但是为什么我们可以马上通过kubectl get命令查询到新建并成功运行的Pod呢？</p><p>这就涉及到了基础架构图中的控制器、调度器、Kubelet等组件。下面我就为你浅析它们是如何基于etcd提供的Watch机制工作，最终实现创建Pod、调度Pod、运行Pod的。</p><h2 id="Watch机制在Kubernetes中应用"><a href="#Watch机制在Kubernetes中应用" class="headerlink" title="Watch机制在Kubernetes中应用"></a>Watch机制在Kubernetes中应用</h2><p>正如基础架构中所介绍的，kube-controller-manager组件中包含一系列WorkLoad的控制器。Deployment资源就由其中的Deployment控制器来负责的，那么它又是如何感知到新建Deployment资源，最终驱动ReplicaSet控制器创建出Pod的呢？</p><p>获取数据变化的方案，主要有轮询和推送两种方案组成。轮询会产生大量expensive request，并且存在高延时。而etcd Watch机制提供的流式推送能力，赋予了kube-apiserver对外提供数据监听能力。</p><p>我们知道在etcd中版本号是个逻辑时钟，随着client对etcd的增、删、改操作而全局递增，它被广泛应用在MVCC、事务、Watch特性中。</p><p>尤其是在Watch特性中，版本号是数据增量同步的核心。当client因网络等异常出现连接闪断后，它就可以通过版本号从etcd server中快速获取异常后的事件，无需全量同步。</p><p>那么在Kubernetes集群中，它提供了什么概念来实现增量监听逻辑呢？</p><p>答案是<strong>Resource Version</strong>。</p><h3 id="Resource-Version与etcd版本号"><a href="#Resource-Version与etcd版本号" class="headerlink" title="Resource Version与etcd版本号"></a>Resource Version与etcd版本号</h3><p>Resource Version是Kubernetes API中非常重要的一个概念，顾名思义，它是一个Kubernetes资源的内部版本字符串，client可通过它来判断资源是否发生了变化。同时，你可以在Get、List、Watch接口中，通过指定Resource Version值来满足你对数据一致性、高性能等诉求。</p><p>那么Resource Version有哪些值呢？跟etcd版本号是什么关系？</p><p>下面我分别以Get和Watch接口中的Resource Version参数值为例，为你剖析它与etcd的关系。</p><p>在Get请求查询案例中，ResourceVersion主要有以下这三种取值：</p><p>第一种是未指定ResourceVersion，默认空字符串。kube-apiserver收到一个此类型的读请求后，它会向etcd发出共识读&#x2F;线性读请求获取etcd集群最新的数据。</p><p>第二种是设置ResourceVersion&#x3D;”0”，赋值字符串0。kube-apiserver收到此类请求时，它可能会返回任意资源版本号的数据，但是优先返回较新版本。一般情况下它直接从kube-apiserver缓存中获取数据返回给client，有可能读到过期的数据，适用于对数据一致性要求不高的场景。</p><p>第三种是设置ResourceVersion为一个非0的字符串。kube-apiserver收到此类请求时，它会保证Cache中的最新ResourceVersion大于等于你传入的ResourceVersion，然后从Cache中查找你请求的资源对象key，返回数据给client。基本原理是kube-apiserver为各个核心资源（如Pod）维护了一个Cache，通过etcd的Watch机制来实时更新Cache。当你的Get请求中携带了非0的ResourceVersion，它会等待缓存中最新ResourceVersion大于等于你Get请求中的ResoureVersion，若满足条件则从Cache中查询数据，返回给client。若不满足条件，它最多等待3秒，若超过3秒，Cache中的最新ResourceVersion还小于Get请求中的ResourceVersion，就会返回ResourceVersionTooLarge错误给client。</p><p>你要注意的是，若你使用的Get接口，那么kube-apiserver会取资源key的ModRevision字段填充Kubernetes资源的ResourceVersion字段（v1.meta&#x2F;ObjectMeta.ResourceVersion）。若你使用的是List接口，kube-apiserver会在查询时，使用etcd当前版本号填充ListMeta.ResourceVersion字段（v1.meta&#x2F;ListMeta.ResourceVersion）。</p><p>那么当我们执行kubectl get po查询案例时，它的ResouceVersion是什么取值呢? 查询的是kube-apiserver缓存还是etcd最新共识数据?</p><p>如下所示，可以通过指定kubectl日志级别为6，观察它向kube-apiserver发出的请求参数。从下面请求日志里你可以看到，默认是未指定Resource Version，也就是会发出一个共识读&#x2F;线性读请求给etcd，获取etcd最新共识数据。</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs elixir">kubectl get po -l app=nginx -v <span class="hljs-number">6</span><br><span class="hljs-number">4410</span> <span class="hljs-symbol">loader.go:</span><span class="hljs-number">359</span>] <span class="hljs-title class_">Config</span> loaded from file /root/.kube/config<br><span class="hljs-number">4410</span> <span class="hljs-symbol">round_trippers.go:</span><span class="hljs-number">438</span>] <span class="hljs-title class_">GET</span> <span class="hljs-symbol">https:</span>//*.*.*.*<span class="hljs-symbol">:*/api/v1/namespaces/default/pods?labelSelector=app%</span><span class="hljs-number">3</span>Dnginx&amp;limit=<span class="hljs-number">500</span> <span class="hljs-number">200</span> <span class="hljs-title class_">OK</span> <span class="hljs-keyword">in</span> <span class="hljs-number">8</span> milliseconds<br><br></code></pre></td></tr></table></figure><p>这里需要注意，在规模较大的集群中，尽量不要使用kubectl频繁查询资源。正如上面所分析的，它会直接查询etcd数据，可能会产生大量的expensive request请求，之前我就有见过业务这样用，然后导致了集群不稳定。</p><p>介绍完查询案例后，我们再看看Watch案例中，它的不同取值含义是怎样的呢?</p><p>它同样含有查询案例中的三种取值，官方定义的含义分别如下：</p><ul><li>未指定ResourceVersion，默认空字符串。一方面为了帮助client建立初始状态，它会将当前已存在的资源通过Add事件返回给client。另一方面，它会从etcd当前版本号开始监听，后续新增写请求导致数据变化时可及时推送给client。</li><li>设置ResourceVersion&#x3D;”0”，赋值字符串0。它同样会帮助client建立初始状态，但是它会从任意版本号开始监听（当前kube-apiserver的实现指定ResourceVersion&#x3D;0和不指定行为一致，在获取初始状态后，都会从cache最新的ResourceVersion开始监听），这种场景可能会导致集群返回陈旧的数据。</li><li>设置ResourceVersion为一个非0的字符串。从精确的版本号开始监听数据，它只会返回大于等于精确版本号的变更事件。</li></ul><p>Kubernetes的控制器组件就基于以上的Watch特性，在快速感知到新建Deployment资源后，进入一致性协调逻辑，创建ReplicaSet控制器，整体交互流程如下所示。</p><p><img src="https://static001.geekbang.org/resource/image/89/54/89c610a5e5bc2bf5eda466a5a0e18e54.png?wh=1740*1456" alt="img"></p><p>Deployment控制器创建ReplicaSet资源对象的日志如下所示。</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs elixir">&#123;<br>    <span class="hljs-string">&quot;level&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;debug&quot;</span></span>,<br>    <span class="hljs-string">&quot;ts&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;2021-02-11T09:55:45.923+0800&quot;</span></span>,<br>    <span class="hljs-string">&quot;caller&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;v3rpc/interceptor.go:181&quot;</span></span>,<br>    <span class="hljs-string">&quot;msg&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;request stats&quot;</span></span>,<br>    <span class="hljs-string">&quot;start time&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;2021-02-11T09:55:45.917+0800&quot;</span></span>,<br>    <span class="hljs-string">&quot;time spent&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;5.922089ms&quot;</span></span>,<br>    <span class="hljs-string">&quot;remote&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;127.0.0.1:44828&quot;</span></span>,<br>    <span class="hljs-string">&quot;response type&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;/etcdserverpb.KV/Txn&quot;</span></span>,<br>    <span class="hljs-string">&quot;request count&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">1</span>,<br>    <span class="hljs-string">&quot;request size&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">766</span>,<br>    <span class="hljs-string">&quot;response count&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">0</span>,<br>    <span class="hljs-string">&quot;response size&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">44</span>,<br>    <span class="hljs-string">&quot;request content&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;compare:&lt;target:MOD key:&quot;</span>/registry/replicasets/default/nginx-deployment-</span><span class="hljs-number">756</span>d9fd5f9<span class="hljs-string">&quot; mod_revision:0 &gt; success:&lt;request_put:&lt;key:&quot;</span>/registry/replicasets/default/nginx-deployment<span class="hljs-number">-756</span>d9fd5f9<span class="hljs-string">&quot; value_size:697 &gt;&gt; failure:&lt;&gt;&quot;</span><br>&#125;<br><br></code></pre></td></tr></table></figure><p>真正创建Pod则是由ReplicaSet控制器负责，它同样基于Watch机制感知到新的RS资源创建后，发起请求创建Pod，确保实际运行Pod数与期望一致。</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs elixir">&#123;<br>    <span class="hljs-string">&quot;level&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;debug&quot;</span></span>,<br>    <span class="hljs-string">&quot;ts&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;2021-02-11T09:55:46.023+0800&quot;</span></span>,<br>    <span class="hljs-string">&quot;caller&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;v3rpc/interceptor.go:181&quot;</span></span>,<br>    <span class="hljs-string">&quot;msg&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;request stats&quot;</span></span>,<br>    <span class="hljs-string">&quot;start time&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;2021-02-11T09:55:46.019+0800&quot;</span></span>,<br>    <span class="hljs-string">&quot;time spent&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;3.519326ms&quot;</span></span>,<br>    <span class="hljs-string">&quot;remote&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;127.0.0.1:44664&quot;</span></span>,<br>    <span class="hljs-string">&quot;response type&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;/etcdserverpb.KV/Txn&quot;</span></span>,<br>    <span class="hljs-string">&quot;request count&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">1</span>,<br>    <span class="hljs-string">&quot;request size&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">822</span>,<br>    <span class="hljs-string">&quot;response count&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">0</span>,<br>    <span class="hljs-string">&quot;response size&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">44</span>,<br>    <span class="hljs-string">&quot;request content&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;compare:&lt;target:MOD key:&quot;</span>/registry/pods/default/nginx-deployment-</span><span class="hljs-number">756</span>d9fd5f9-x6r6q<span class="hljs-string">&quot; mod_revision:0 &gt; success:&lt;request_put:&lt;key:&quot;</span>/registry/pods/default/nginx-deployment<span class="hljs-number">-756</span>d9fd5f9-x6r6q<span class="hljs-string">&quot; value_size:754 &gt;&gt; failure:&lt;&gt;&quot;</span><br>&#125;<br><br></code></pre></td></tr></table></figure><p>在这过程中也产生了若干Event，下面是etcd收到新增Events资源的请求，你可以看到Event事件key关联了Lease，这个Lease正是由我上面所介绍的leaseManager所负责创建。</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs elixir">&#123;<br>    <span class="hljs-string">&quot;level&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;debug&quot;</span></span>,<br>    <span class="hljs-string">&quot;ts&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;2021-02-11T09:55:45.930+0800&quot;</span></span>,<br>    <span class="hljs-string">&quot;caller&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;v3rpc/interceptor.go:181&quot;</span></span>,<br>    <span class="hljs-string">&quot;msg&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;request stats&quot;</span></span>,<br>    <span class="hljs-string">&quot;start time&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;2021-02-11T09:55:45.926+0800&quot;</span></span>,<br>    <span class="hljs-string">&quot;time spent&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;3.259966ms&quot;</span></span>,<br>    <span class="hljs-string">&quot;remote&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;127.0.0.1:44632&quot;</span></span>,<br>    <span class="hljs-string">&quot;response type&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;/etcdserverpb.KV/Txn&quot;</span></span>,<br>    <span class="hljs-string">&quot;request count&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">1</span>,<br>    <span class="hljs-string">&quot;request size&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">449</span>,<br>    <span class="hljs-string">&quot;response count&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">0</span>,<br>    <span class="hljs-string">&quot;response size&quot;</span><span class="hljs-symbol">:</span><span class="hljs-number">44</span>,<br>    <span class="hljs-string">&quot;request content&quot;</span><span class="hljs-symbol">:<span class="hljs-string">&quot;compare:&lt;target:MOD key:&quot;</span>/registry/events/default/nginx-deployment</span>.<span class="hljs-number">16628</span>eb9f79e0ab0<span class="hljs-string">&quot; mod_revision:0 &gt; success:&lt;request_put:&lt;key:&quot;</span>/registry/events/default/nginx-deployment.<span class="hljs-number">16628</span>eb9f79e0ab0<span class="hljs-string">&quot; value_size:369 lease:5772338802590698925 &gt;&gt; failure:&lt;&gt;&quot;</span><br>&#125;<br><br></code></pre></td></tr></table></figure><p>Pod创建出来后，这时kube-scheduler监听到待调度的Pod，于是为其分配Node，通过kube-apiserver的Bind接口，将调度后的节点IP绑定到Pod资源上。kubelet通过同样的Watch机制感知到新建的Pod后，发起Pod创建流程即可。</p><p>以上就是当我们在Kubernetes集群中创建一个Pod后，Kubernetes和etcd之间交互的简要分析。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>最后首先解读了Kubernetes集群的etcd存储格式，每个资源的保存路径为prefix + “&#x2F;“ + 资源类型 + “&#x2F;“ + namespace + “&#x2F;“ + 具体资源名组成。结合etcd3的范围查询，可快速实现按namesapace、资源名称查询。按标签查询则是通过kube-apiserver遍历指定namespace下的资源实现的，若未从kube-apiserver的Cache中查询，请求较频繁，很可能导致etcd流量较大，出现不稳定。</p><p>随后介绍了kube-apiserver的通用存储模块，它通过在创建、查询、删除、更新操作前增加一系列的Hook机制，实现了新增任意资源只需编写相应的Hook策略即可。我还重点和你介绍了创建接口，它主要由拼接key、获取Lease ID、数据转换、写入etcd组成，重点是它通过使用事务接口实现了资源的安全创建及更新。</p><p>最后讲解了Resoure Version在Kubernetes集群中的大量应用，重点分析了Get和Watch请求案例中的Resource Version含义，帮助你了解Resource Version本质，让你能根据业务场景和对一致性的容忍度，正确的使用Resource Version以满足业务诉求。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><p>有哪些原因可能会导致kube-apiserver报“too old Resource Version”错误呢？</p><p>informer watch请求的resource version比kube-apiserver缓存中保存的最小resource version还小，kube-apiserver就会返回“too old Resource Version”，然后触发informer进行list全量数据，导致expensive request</p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>Kubernetes</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>18.实战：如何基于Raft从0到1构建一个支持多存储引擎分布式KV服务？</title>
    <link href="/2022/10/12/18-%E5%AE%9E%E6%88%98%EF%BC%9A%E5%A6%82%E4%BD%95%E5%9F%BA%E4%BA%8ERaft%E4%BB%8E0%E5%88%B01%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%94%AF%E6%8C%81%E5%A4%9A%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E%E5%88%86%E5%B8%83%E5%BC%8FKV%E6%9C%8D%E5%8A%A1%EF%BC%9F/"/>
    <url>/2022/10/12/18-%E5%AE%9E%E6%88%98%EF%BC%9A%E5%A6%82%E4%BD%95%E5%9F%BA%E4%BA%8ERaft%E4%BB%8E0%E5%88%B01%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%94%AF%E6%8C%81%E5%A4%9A%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E%E5%88%86%E5%B8%83%E5%BC%8FKV%E6%9C%8D%E5%8A%A1%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="18-实战：如何基于Raft从0到1构建一个支持多存储引擎分布式KV服务？"><a href="#18-实战：如何基于Raft从0到1构建一个支持多存储引擎分布式KV服务？" class="headerlink" title="18.实战：如何基于Raft从0到1构建一个支持多存储引擎分布式KV服务？"></a>18.实战：如何基于Raft从0到1构建一个支持多存储引擎分布式KV服务？</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><p>今天聊聊如何实现一个类etcd、支持多存储引擎的KV服务，基于etcd自带的 <a href="https://github.com/etcd-io/etcd/tree/v3.4.9/contrib/raftexample">raftexample</a> 项目快速构建它。</p><p>为了方便后面描述，把它命名为metcd（表示微型的etcd），它是raftexample的加强版。希望通过metcd这个小小的实战项目，能够帮助进一步理解etcd乃至分布式存储服务的核心架构、原理、典型问题解决方案。</p><h2 id="整体架构设计"><a href="#整体架构设计" class="headerlink" title="整体架构设计"></a>整体架构设计</h2><p>在和你深入聊代码细节之前，首先我和你从整体上介绍下系统架构。</p><p>下面是我给你画的metcd整体架构设计，它由API层、Raft层的共识模块、逻辑层及存储层组成的状态机组成。</p><p>接下来，简要分析下API设计及复制状态机。</p><p><img src="https://static001.geekbang.org/resource/image/5e/03/5e9f6882a6f6e357e5c2c5yyffda4e03.png?wh=1920*1166" alt="img"></p><h3 id="API设计"><a href="#API设计" class="headerlink" title="API设计"></a>API设计</h3><p>API是软件系统对外的语言，它是应用编程接口的缩写，由一组接口定义和协议组成。</p><p>在设计API的时候，往往会考虑以下几个因素：</p><ul><li><strong>性能</strong>。如etcd v2使用的是简单的HTTP&#x2F;1.x，性能上无法满足大规模Kubernetes集群等场景的诉求，因此etcd v3使用的是基于HTTP&#x2F;2的gRPC协议。</li><li><strong>易用性、可调试性</strong>。如有的内部高并发服务为了满足性能等诉求，使用的是UDP协议。相比HTTP协议，UDP协议显然在易用性、可调试性上存在一定的差距。</li><li><strong>开发效率、跨平台、可移植性</strong>。相比基于裸UDP、TCP协议设计的接口，如果你使用Protobuf等IDL语言，它支持跨平台、代码自动自动生成，开发效率更高。</li><li><strong>安全性</strong>。如相比HTTP协议，使用HTTPS协议可对通信数据加密更安全，可适用于不安全的网络环境（比如公网传输）。</li><li><strong>接口幂等性</strong>。幂等性简单来说，就是同样一个接口请求一次与多次的效果一样。若你的接口对外保证幂等性，则可降低使用者的复杂度。</li></ul><p>因为我们场景的是POC(Proof of concept)、Demo开发，因此在metcd项目中，我们优先考虑点是易用性、可调试性，选择HTTP&#x2F;1.x协议，接口上为了满足key-value操作，支持Get和Put接口即可。</p><p>假设metcd项目使用3379端口，Put和Get接口，如下所示。</p><ul><li>Put接口，设置key-value</li></ul><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">curl -L http:<span class="hljs-regexp">//</span><span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span>:<span class="hljs-number">3379</span>/hello -XPUT -d world<br><br></code></pre></td></tr></table></figure><ul><li>Get接口，查询key-value</li></ul><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">curl -L http:<span class="hljs-regexp">//</span><span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span>:<span class="hljs-number">3379</span>/hello<br>world<br><br></code></pre></td></tr></table></figure><h3 id="复制状态机"><a href="#复制状态机" class="headerlink" title="复制状态机"></a>复制状态机</h3><p>了解完API设计，那最核心的复制状态机是如何工作的呢？</p><p>我们知道etcd是基于下图复制状态机实现的分布式KV服务，复制状态机由共识模块、日志模块、状态机组成。</p><p><img src="https://static001.geekbang.org/resource/image/5c/4f/5c7a3079032f90120a6b309ee401fc4f.png?wh=605*319" alt="img"></p><p>实战项目metcd，也正是使用与之一样的模型，并且使用etcd项目中实现的Raft算法库作为共识模块，此算法库已被广泛应用在etcd、cockroachdb、dgraph等开源项目中。</p><p>以下是复制状态机的写请求流程：</p><ul><li>client发起一个写请求（put hello &#x3D; world）；</li><li>server向Raft共识模块提交请求，共识模块生成一个写提案日志条目。若server是Leader，则把日志条目广播给其他节点，并持久化日志条目到WAL中；</li><li>当一半以上节点持久化日志条目后，Leader的共识模块将此日志条目标记为已提交（committed），并通知其他节点提交；</li><li>server从共识模块获取已经提交的日志条目，异步应用到状态机存储中（boltdb&#x2F;leveldb&#x2F;memory），然后返回给client。</li></ul><h3 id="多存储引擎"><a href="#多存储引擎" class="headerlink" title="多存储引擎"></a>多存储引擎</h3><p>了解完复制状态机模型后，再深入介绍下状态机。状态机中最核心模块当然是存储引擎，那要如何同时支持多种存储引擎呢？</p><p>metcd项目将基于etcd本身自带的raftexample项目进行快速开发，而raftexample本身只支持内存存储。</p><p>因此通过将KV存储接口进行<strong>抽象化设计</strong>，实现支持多存储引擎。KVStore interface的定义如下所示。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-keyword">type</span> KVStore interface &#123;<br>   <span class="hljs-comment">// LookUp get key value</span><br>   <span class="hljs-constructor">Lookup(<span class="hljs-params">key</span> <span class="hljs-params">string</span>)</span> (<span class="hljs-built_in">string</span>, <span class="hljs-built_in">bool</span>)<br><br>   <span class="hljs-comment">// Propose propose kv request into raft state machine</span><br>   <span class="hljs-constructor">Propose(<span class="hljs-params">k</span>, <span class="hljs-params">v</span> <span class="hljs-params">string</span>)</span><br><br>   <span class="hljs-comment">// ReadCommits consume entry from raft state machine into KvStore map until error</span><br>   <span class="hljs-constructor">ReadCommits(<span class="hljs-params">commitC</span> &lt;-<span class="hljs-params">chan</span> <span class="hljs-operator">*</span><span class="hljs-params">string</span>, <span class="hljs-params">errorC</span> &lt;-<span class="hljs-params">chan</span> <span class="hljs-params">error</span>)</span><br><br>   <span class="hljs-comment">// Snapshot return KvStore snapshot</span><br>   <span class="hljs-constructor">Snapshot()</span> (<span class="hljs-literal">[]</span>byte, error)<br><br>   <span class="hljs-comment">// RecoverFromSnapshot recover data from snapshot</span><br>   <span class="hljs-constructor">RecoverFromSnapshot(<span class="hljs-params">snapshot</span> []<span class="hljs-params">byte</span>)</span> error<br><br>   <span class="hljs-comment">// Close close backend databases</span><br>   <span class="hljs-constructor">Close()</span> err<br>&#125;<br><br></code></pre></td></tr></table></figure><p><strong>基于KV接口抽象化的设计，只需要针对具体的存储引擎，实现对应的操作即可。</strong></p><p>我们期望支持三种存储引擎，分别是内存map、boltdb、leveldb，并做一系列简化设计。一组metcd实例，通过metcd启动时的配置来决定使用哪种存储引擎。不同业务场景不同实例，比如读多写少的存储引擎可使用boltdb，写多读少的可使用leveldb。</p><p>接下来重点介绍下存储引擎的选型及原理。</p><h4 id="boltdb"><a href="#boltdb" class="headerlink" title="boltdb"></a>boltdb</h4><p>boltdb是一个基于B+ tree实现的存储引擎库，在 之前文章详细介绍过原理。</p><p>boltdb为什么适合读多写少？</p><p><strong>对于读请求而言，一般情况下它可直接从内存中基于B+ tree遍历，快速获取数据返回给client，不涉及经过磁盘I&#x2F;O。</strong></p><p>对于写请求，它基于B+ tree查找写入位置，更新key-value。事务提交时，写请求包括B+ tree重平衡、分裂、持久化ditry page、持久化freelist、持久化meta page流程。同时，ditry page可能分布在文件的各个位置，它发起的是随机写磁盘I&#x2F;O。</p><p>因此在boltdb中，完成一个写请求的开销相比读请求是大很多的。一个3节点的8核16G空集群，线性读性能可以达到19万QPS，而写QPS仅为5万。</p><h4 id="leveldb"><a href="#leveldb" class="headerlink" title="leveldb"></a>leveldb</h4><p>那要如何设计适合写多读少的存储引擎呢?</p><p>最简单的思路当然是写内存最快。可是内存有限的，无法支撑大容量的数据存储，不持久化数据会丢失。</p><p>那能否直接将数据顺序追加到文件末尾（AOF）呢？因为磁盘的特点是顺序写性能比较快。</p><p>当然可以。 <a href="https://en.wikipedia.org/wiki/Bitcask">Bitcask</a> 存储模型就是采用AOF模式，把写请求顺序追加到文件。Facebook的图片存储 <a href="https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Beaver.pdf">Haystack</a> 根据其论文介绍，也是使用类似的方案来解决大规模写入痛点。</p><p>那在AOF写入模型中如何实现查询数据呢？</p><p>很显然通过遍历文件一个个匹配key是可以的，但是它的性能是极差的。为了实现高性能的查询，最理想的解决方案从直接从内存中查询，但是内存是有限的，那么我们能否通过内存索引来记录一个key-value数据在文件中的偏移量，实现从磁盘快速读取呢？</p><p>是的，这正是 <a href="https://en.wikipedia.org/wiki/Bitcask">Bitcask</a> 存储模型的查询的实现，它通过内存哈希表维护各个key-value数据的索引，实现了快速查找key-value数据。不过，内存中虽然只保存key索引信息，但是当key较多的时候，其对内存要求依然比较高。</p><p>快速了解完存储引擎提升写性能的核心思路（随机写转化为顺序写）之后，那leveldb它的原理是怎样的呢？与Bitcask存储模型有什么不一样？</p><p>leveldb是基于**LSM tree(log-structured merge-tree)**实现的key-value存储，它的架构如下图所示（ <a href="https://microsoft.github.io/MLOS/notebooks/LevelDbTuning/">引用自微软博客</a>）。</p><p>它提升写性能的核心思路同样是将随机写转化为顺序写磁盘WAL文件和内存，结合了我们上面讨论的写内存和磁盘两种方法。数据持久化到WAL文件是为了确保机器crash后数据不丢失。</p><p><img src="https://static001.geekbang.org/resource/image/05/50/05f01951fe5862a62624b81e2ceea150.png?wh=992*677" alt="img"></p><p>那么它要如何解决内存不足和查询的痛点问题呢？</p><p>核心解决方案是分层的设计和基于一系列对象的转换和压缩。接下来分析一下上面架构图写流程和后台compaction任务：</p><ul><li>首先写请求顺序写入Log文件(WAL)；</li><li>更新内存的Memtable。leveldb Memtable后端数据结构实现是skiplist，skiplist相比平衡二叉树，实现简单却同样拥有高性能的读写；</li><li>当Memtable达到一定的阈值时，转换成不可变的Memtable，也就是只读不可写；</li><li>leveldb后台Compact任务会将不可变的Memtable生成SSTable文件，它有序地存储一系列key-value数据。注意SST文件按写入时间进行了分层，Level层次越小数据越新。Manifest文件记录了各个SSTable文件处于哪个层级、它的最小与最大key范围；</li><li>当某个level下的SSTable文件数目超过一定阈值后，Compact任务会从这个level的SSTable中选择一个文件（level&gt;0），将其和高一层级的level+1的SSTable文件合并；</li><li>注意level 0是由Immutable直接生成的，因此level 0 SSTable文件中的key-value存在相互重叠。而level &gt; 0时，在和更高一层SSTable合并过程中，参与的SSTable文件是多个，leveldb会确保各个SSTable中的key-value不重叠。</li></ul><p>了解完写流程，读流程也就简单了，核心步骤如下：</p><ul><li>从Memtable跳跃表中查询key；</li><li>未找到则从Immutable中查找；</li><li>Immutable仍未命中，则按照leveldb的分层属性，因level 0 SSTable文件是直接从Immutable生成的，level 0存在特殊性，因此你需要从level 0遍历SSTable查找key；</li><li>level 0中若未命中，则从level 1乃至更高的层次查找。level大于0时，各个SSTable中的key是不存在相互重叠的。根据manifest记录的key-value范围信息，可快递定位到具体的SSTable。同时leveldb基于 <a href="https://en.wikipedia.org/wiki/Bloom_filter">bloom filter</a> 实现了快速筛选SSTable，因此查询效率较高。</li></ul><p>更详细原理可以参考一下 <a href="https://github.com/google/leveldb">leveldb</a> 源码。</p><h2 id="实现分析"><a href="#实现分析" class="headerlink" title="实现分析"></a>实现分析</h2><p>从API设计、复制状态机、多存储引擎支持等几个方面你介绍了metcd架构设计后，接下来我就和你重点介绍下共识模块、状态机支持多存储引擎模块的核心实现要点。</p><h3 id="Raft算法库"><a href="#Raft算法库" class="headerlink" title="Raft算法库"></a>Raft算法库</h3><p>共识模块使用的是etcd <a href="https://github.com/etcd-io/etcd/tree/v3.4.9/raft">Raft算法库</a>，它是一个经过大量业务生产环境检验、具备良好可扩展性的共识算法库。</p><p>它提供了哪些接口给你使用? 如何提交一个提案，并且获取Raft共识模块输出结果呢？</p><h4 id="Raft-API"><a href="#Raft-API" class="headerlink" title="Raft API"></a>Raft API</h4><p>Raft作为一个库，它对外最核心的对象是一个名为 <a href="https://github.com/etcd-io/etcd/blob/v3.4.9/raft/node.go#L125:L203">Node</a> 的数据结构。Node表示Raft集群中的一个节点，它的输入与输出接口如下图所示，下面重点介绍它的几个接口功能：</p><ul><li>Campaign，状态转换成Candidate，发起新一轮Leader选举；</li><li>Propose，提交提案接口；</li><li>Ready，Raft状态机输出接口，它的返回是一个输出Ready数据结构类型的管道，应用需要监听此管道，获取Ready数据，处理其中的各个消息（如持久化未提交的日志条目到WAL中，发送消息给其他节点等）；</li><li>Advance，通知Raft状态机，应用已处理上一个输出的Ready数据，等待发送下一个Ready数据；</li><li>TransferLeaderShip，尝试将Leader转移到某个节点；</li><li>Step，向Raft状态机提交收到的消息，比如当Leader广播完MsgApp消息给Follower节点后，Leader收到Follower节点回复的MsgAppResp消息时，就通过Step接口将此消息提交给Raft状态机驱动其工作；</li><li>ReadIndex，用于实现线性读。</li></ul><p><img src="https://static001.geekbang.org/resource/image/a7/39/a79a97f8cc8294dcb93f9552fb638f39.png?wh=1920*787" alt="img"></p><p>上面提到的Raft状态机的输出 <a href="https://github.com/etcd-io/etcd/blob/v3.4.9/raft/node.go#L52:L90">Ready结构</a> 含有哪些信息呢? 下图是其详细字段，含义如下：</p><ul><li>SoftState，软状态。包括集群Leader和节点状态，不需要持久化到WAL；</li><li>pb.HardState，硬状态。与软状态相反，包括了节点当前Term、Vote等信息，需要持久化到WAL中；</li><li>ReadStates，用于线性一致性读；</li><li>Entries，在向其他节点发送消息之前需持久化到WAL中；</li><li>Messages，持久化Entries后，发送给其他节点的消息；</li><li>Committed Entries，已提交的日志条目，需要应用到存储状态机中；</li><li>Snapshot，快照需保存到持久化存储中；</li><li>MustSync，HardState和Entries是否要持久化到WAL中；</li></ul><p>了解完API后，接下来继续看看代码如何使用Raft的Node API。</p><p>etcd Raft库的设计抽象了网络、Raft日志存储等模块，它本身并不会进行网络、存储相关的操作，上层应用需结合自己业务场景选择内置的模块或自定义实现网络、存储、日志等模块。</p><p>因此在使用Raft库时，需要先自定义好相关网络、存储等模块，再结合上面介绍的Raft Node API，就可以完成一个Node的核心操作了。其数据结构定义如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// A key-value stream backed by raft</span><br><span class="hljs-keyword">type</span> raftNode <span class="hljs-keyword">struct</span> &#123;<br>   proposeC    &lt;-<span class="hljs-keyword">chan</span> <span class="hljs-type">string</span>            <span class="hljs-comment">// proposed messages (k,v)</span><br>   confChangeC &lt;-<span class="hljs-keyword">chan</span> raftpb.ConfChange <span class="hljs-comment">// proposed cluster config changes</span><br>   commitC     <span class="hljs-keyword">chan</span>&lt;- *<span class="hljs-type">string</span>           <span class="hljs-comment">// entries committed to log (k,v)</span><br>   errorC      <span class="hljs-keyword">chan</span>&lt;- <span class="hljs-type">error</span>             <span class="hljs-comment">// errors from raft session</span><br>   id          <span class="hljs-type">int</span>      <span class="hljs-comment">// client ID for raft session</span><br>   ......<br>   node        raft.Node<br>   raftStorage *raft.MemoryStorage<br>   wal         *wal.WAL<br>   transport *rafthttp.Transport<br>&#125;<br><br></code></pre></td></tr></table></figure><p>这个数据结构名字叫raftNode，它表示Raft集群中的一个节点。它是由我们业务应用层设计的一个组合结构。从结构体定义中你可以看到它包含了Raft核心数据结构Node(raft.Node)、Raft日志条目内存存储模块(raft.MemoryStorage）、WAL持久化模块(wal.WAL)以及网络模块(rafthttp.Transport)。</p><p>同时，它提供了三个核心的管道与业务逻辑模块、存储状态机交互：</p><ul><li>proposeC，它用来接收client发送的写请求提案消息；</li><li>confChangeC，它用来接收集群配置变化消息；</li><li>commitC，它用来输出Raft共识模块已提交的日志条目消息。</li></ul><p>在metcd项目中因为是直接基于raftexample定制开发，因此日志持久化存储、网络都使用的是etcd自带的WAL和rafthttp模块。</p><p><a href="https://github.com/etcd-io/etcd/blob/v3.4.9/wal/wal.go">WAL</a> 模块中提供了核心的保存未持久化的日志条目和快照功能接口。</p><p><a href="https://github.com/etcd-io/etcd/tree/v3.4.9/etcdserver/api/rafthttp">rafthttp</a> 模块基于HTTP协议提供了各个节点间的消息发送能力，metcd使用如下：</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">rc.transport = &amp;rafthttp.Transport&#123;<br>   Logger:      zap.<span class="hljs-constructor">NewExample()</span>,<br>   ID:          types.<span class="hljs-constructor">ID(<span class="hljs-params">rc</span>.<span class="hljs-params">id</span>)</span>,<br>   ClusterID:   <span class="hljs-number">0x1000</span>,<br>   Raft:        rc,<br>   ServerStats: stats.<span class="hljs-constructor">NewServerStats(<span class="hljs-string">&quot;&quot;</span>, <span class="hljs-string">&quot;&quot;</span>)</span>,<br>   LeaderStats: stats.<span class="hljs-constructor">NewLeaderStats(<span class="hljs-params">strconv</span>.Itoa(<span class="hljs-params">rc</span>.<span class="hljs-params">id</span>)</span>),<br>   ErrorC:      make(chan error),<br>&#125;<br><br></code></pre></td></tr></table></figure><p>搞清楚Raft模块的输入、输出API，设计好raftNode结构，复用etcd的WAL、网络等模块后，接下来就只需要实现如下两个循环逻辑，处理业务层发送给proposeC和confChangeC消息、将Raft的Node输出Ready结构进行相对应的处理即可。精简后的代码如下所示：</p><figure class="highlight roboconf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs roboconf">func (rc *raftNode) serveChannels() &#123;<br>   // <span class="hljs-attribute">send proposals over raft</span><br><span class="hljs-attribute">   go func() &#123;</span><br><span class="hljs-attribute">      confChangeCount</span> := uint64(0)<br>      for rc<span class="hljs-variable">.proposeC</span> != nil &amp;&amp; rc<span class="hljs-variable">.confChangeC</span> != nil &#123;<br>         select &#123;<br>         case prop, ok := &lt;-rc<span class="hljs-variable">.proposeC</span>:<br>            if !ok &#123;<br>               rc<span class="hljs-variable">.proposeC</span> = nil<br>            &#125; else &#123;<br>               // blocks until accepted by raft state machine<br>               rc<span class="hljs-variable">.node</span><span class="hljs-variable">.Propose</span>(context<span class="hljs-variable">.TODO</span>(), []byte(prop))<br>            &#125;<br><br>         case cc, ok := &lt;-rc<span class="hljs-variable">.confChangeC</span>:<br>            if !ok &#123;<br>               rc<span class="hljs-variable">.confChangeC</span> = nil<br>            &#125; else &#123;<br>               confChangeCount++<br>               cc<span class="hljs-variable">.ID</span> = confChangeCount<br>               rc<span class="hljs-variable">.node</span><span class="hljs-variable">.ProposeConfChange</span>(context<span class="hljs-variable">.TODO</span>(), cc)<br>            &#125;<br>         &#125;<br>      &#125;<br>   &#125;()<br><br>   // event loop on raft state machine updates<br>   for &#123;<br>      select &#123;<br>      case &lt;-ticker<span class="hljs-variable">.C</span>:<br>         rc<span class="hljs-variable">.node</span><span class="hljs-variable">.Tick</span>()<br><br>      // store raft entries to wal, then publish over commit channel<br>      case rd := &lt;-rc<span class="hljs-variable">.node</span><span class="hljs-variable">.Ready</span>():<br>         rc<span class="hljs-variable">.wal</span><span class="hljs-variable">.Save</span>(rd<span class="hljs-variable">.HardState</span>, rd<span class="hljs-variable">.Entries</span>)<br>         if !raft<span class="hljs-variable">.IsEmptySnap</span>(rd<span class="hljs-variable">.Snapshot</span>) &#123;<br>            rc<span class="hljs-variable">.saveSnap</span>(rd<span class="hljs-variable">.Snapshot</span>)<br>            rc<span class="hljs-variable">.raftStorage</span><span class="hljs-variable">.ApplySnapshot</span>(rd<span class="hljs-variable">.Snapshot</span>)<br>            rc<span class="hljs-variable">.publishSnapshot</span>(rd<span class="hljs-variable">.Snapshot</span>)<br>         &#125;<br>         rc<span class="hljs-variable">.raftStorage</span><span class="hljs-variable">.Append</span>(rd<span class="hljs-variable">.Entries</span>)<br>         rc<span class="hljs-variable">.transport</span><span class="hljs-variable">.Send</span>(rd<span class="hljs-variable">.Messages</span>)<br>         if ok := rc<span class="hljs-variable">.publishEntries</span>(rc<span class="hljs-variable">.entriesToApply</span>(rd<span class="hljs-variable">.CommittedEntries</span>)); !<span class="hljs-attribute">ok &#123;</span><br><span class="hljs-attribute">            rc.stop()</span><br><span class="hljs-attribute">            return</span><br><span class="hljs-attribute">         &#125;</span><br><span class="hljs-attribute">         rc.maybeTriggerSnapshot()</span><br><span class="hljs-attribute">         rc.node.Advance()</span><br><span class="hljs-attribute">      &#125;</span><br><span class="hljs-attribute">   &#125;</span><br><span class="hljs-attribute">&#125;</span><br><span class="hljs-attribute"></span><br></code></pre></td></tr></table></figure><p>代码简要分析如下：</p><ul><li>从proposeC中取出提案消息，通过raft.Node.Propose API提交提案；</li><li>从confChangeC取出配置变更消息，通过raft.Node.ProposeConfChange API提交配置变化消息；</li><li>从raft.Node中获取Raft算法状态机输出到Ready结构中，将rd.Entries和rd.HardState通过WAL模块持久化，将rd.Messages通过rafthttp模块，发送给其他节点。将rd.CommittedEntries应用到业务存储状态机。</li></ul><p>以上就是Raft实现的核心流程，接下来聊聊业务存储状态机。</p><h3 id="支持多存储引擎"><a href="#支持多存储引擎" class="headerlink" title="支持多存储引擎"></a>支持多存储引擎</h3><p>在整体架构设计时，介绍了为了使metcd项目能支撑多存储引擎，将KVStore进行了抽象化设计，因此只需要实现各个存储引擎相对应的API即可。</p><p>这里以Put接口为案例，分别介绍下各个存储引擎的实现。</p><h4 id="boltdb-1"><a href="#boltdb-1" class="headerlink" title="boltdb"></a>boltdb</h4><p>首先是boltdb存储引擎，它的实现如下。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(s *boltdbKVStore)</span></span> Put(key, value <span class="hljs-type">string</span>) <span class="hljs-type">error</span> &#123;<br>   s.mu.Lock()<br>   <span class="hljs-keyword">defer</span> s.mu.Unlock()<br>   <span class="hljs-comment">// Start a writable transaction.</span><br>   tx, err := s.db.Begin(<span class="hljs-literal">true</span>)<br>   <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>      <span class="hljs-keyword">return</span> err<br>   &#125;<br>   <span class="hljs-keyword">defer</span> tx.Rollback()<br><br>   <span class="hljs-comment">// Use the transaction...</span><br>   bucket, err := tx.CreateBucketIfNotExists([]<span class="hljs-type">byte</span>(<span class="hljs-string">&quot;keys&quot;</span>))<br>   <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>      log.Printf(<span class="hljs-string">&quot;failed to put key %s, value %s, err is %v&quot;</span>, key, value, err)<br>      <span class="hljs-keyword">return</span> err<br>   &#125;<br>   err = bucket.Put([]<span class="hljs-type">byte</span>(key), []<span class="hljs-type">byte</span>(value))<br>   <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>      log.Printf(<span class="hljs-string">&quot;failed to put key %s, value %s, err is %v&quot;</span>, key, value, err)<br>      <span class="hljs-keyword">return</span> err<br>   &#125;<br><br>   <span class="hljs-comment">// Commit the transaction and check for error.</span><br>   <span class="hljs-keyword">if</span> err := tx.Commit(); err != <span class="hljs-literal">nil</span> &#123;<br>      log.Printf(<span class="hljs-string">&quot;failed to commit transaction, key %s, err is %v&quot;</span>, key, err)<br>      <span class="hljs-keyword">return</span> err<br>   &#125;<br>   log.Printf(<span class="hljs-string">&quot;backend:%s,put key:%s,value:%s succ&quot;</span>, s.config.backend, key, value)<br>   <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br><br></code></pre></td></tr></table></figure><h4 id="leveldb-1"><a href="#leveldb-1" class="headerlink" title="leveldb"></a>leveldb</h4><p>其次是leveldb，我们使用的是 <a href="https://github.com/syndtr/goleveldb">goleveldb</a>，它基于Google开源的c++ <a href="https://github.com/google/leveldb">leveldb</a> 版本实现。它提供的常用API如下所示。</p><ul><li>通过OpenFile API创建或打开一个leveldb数据库。</li></ul><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stata"><span class="hljs-keyword">db</span>, <span class="hljs-keyword">err</span> := leveldb.OpenFile(<span class="hljs-string">&quot;path/to/db&quot;</span>, nil)<br>...<br>defer <span class="hljs-keyword">db</span>.<span class="hljs-keyword">Close</span>()<br><br></code></pre></td></tr></table></figure><ul><li>通过DB.Get&#x2F;Put&#x2F;Delete API操作数据。</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs stylus">data, err := db<span class="hljs-selector-class">.Get</span>(<span class="hljs-selector-attr">[]</span><span class="hljs-built_in">byte</span>(<span class="hljs-string">&quot;key&quot;</span>), nil)<br>...<br>err = db<span class="hljs-selector-class">.Put</span>(<span class="hljs-selector-attr">[]</span><span class="hljs-built_in">byte</span>(<span class="hljs-string">&quot;key&quot;</span>), <span class="hljs-selector-attr">[]</span><span class="hljs-built_in">byte</span>(<span class="hljs-string">&quot;value&quot;</span>), nil)<br>...<br>err = db<span class="hljs-selector-class">.Delete</span>(<span class="hljs-selector-attr">[]</span><span class="hljs-built_in">byte</span>(<span class="hljs-string">&quot;key&quot;</span>), nil)<br>...<br><br></code></pre></td></tr></table></figure><p>了解其接口后，通过goleveldb的库，client调用就非常简单了，下面是metcd项目中，leveldb存储引擎Put接口的实现。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(s *leveldbKVStore)</span></span> Put(key, value <span class="hljs-type">string</span>) <span class="hljs-type">error</span> &#123;<br>   err := s.db.Put([]<span class="hljs-type">byte</span>(key), []<span class="hljs-type">byte</span>(value), <span class="hljs-literal">nil</span>)<br>   <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>      log.Printf(<span class="hljs-string">&quot;failed to put key %s, value %s, err is %v&quot;</span>, key, value, err)<br>      <span class="hljs-keyword">return</span> err<br>   &#125;<br>   log.Printf(<span class="hljs-string">&quot;backend:%s,put key:%s,value:%s succ&quot;</span>, s.config.backend, key, value)<br>   <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br><br></code></pre></td></tr></table></figure><h3 id="读写流程"><a href="#读写流程" class="headerlink" title="读写流程"></a>读写流程</h3><p>介绍完在metcd项目中如何使用Raft共识模块、支持多存储引擎后，再从整体上介绍下在metcd中写入和读取一个key-value的流程。</p><h4 id="写流程"><a href="#写流程" class="headerlink" title="写流程"></a>写流程</h4><p>当通过如下curl命令发起一个写操作时，写流程如下面架构图序号所示:</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">curl -L http:<span class="hljs-regexp">//</span><span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span>:<span class="hljs-number">3379</span>/hello -XPUT -d world<br><br></code></pre></td></tr></table></figure><ul><li>client通过curl发送HTTP PUT请求到server；</li><li>server收到后，将消息写入到KVStore的ProposeC管道；</li><li>raftNode循环逻辑将消息通过Raft模块的Propose接口提交；</li><li>Raft模块输出Ready结构，server将日志条目持久化后，并发送给其他节点；</li><li>集群多数节点持久化此日志条目后，这个日志条目被提交给存储状态机KVStore执行；</li><li>KVStore根据启动的backend存储引擎名称，调用对应的Put接口即可。</li></ul><p><img src="https://static001.geekbang.org/resource/image/9b/c1/9b84a7e312165de46749e1c4046fc9c1.png?wh=1920*1135" alt="img"></p><h4 id="读流程"><a href="#读流程" class="headerlink" title="读流程"></a>读流程</h4><p>当通过curl命令发起一个读操作时，读流程如下面架构图序号所示：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">curl -L http:<span class="hljs-regexp">//</span><span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span>:<span class="hljs-number">3379</span>/hello<br>world<br><br></code></pre></td></tr></table></figure><ul><li>client通过curl发送HTTP Get请求到server；</li><li>server收到后，根据KVStore的存储引擎，从后端查询出对应的key-value数据。</li></ul><p><img src="https://static001.geekbang.org/resource/image/17/b2/1746fbd9e9435d8607e44bea2d2c39b2.png?wh=1920*1187" alt="img"></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>如何基于Raft从0到1构建一个支持多存储引擎的分布式key-value数据库。</p><p>在整体架构设计上，介绍了API设计核心因素，它们分别是<strong>性能、易用性、开发效率、安全性、幂等性</strong>。其次介绍了复制状态机的原理，它由共识模块、日志模块、存储状态机模块组成。最后深入分析了多存储引擎设计，重点介绍了leveldb原理，它将随机写转换为顺序写日志和内存，通过一系列分层、创新的设计实现了优异的写性能，适合读少写多。</p><p>在实现分析上，重点介绍了Raft算法库的核心对象Node API。对于一个库而言，我们重点关注的是其输入、输出接口，业务逻辑层可通过Propose接口提交提案，通过Ready结构获取Raft算法状态机的输出内容。其次介绍了Raft算法库如何与WAL模块、Raft日志存储模块、网络模块协作完成一个写请求。</p><p>最后为了支持多存储引擎，分别基于boltdb、leveldb实现了KVStore相关接口操作，并通过读写流程图，从整体上介绍了一个读写请求在metcd中是如何工作的。</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p> <a href="https://github.com/etcd-io/etcd/tree/v3.4.9/contrib/raftexample">raftexample</a> 启动的时候是如何工作的吗？它的存储引擎内存map是如何保证数据不丢失的呢？</p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>17.性能及稳定性（下）：如何优化及扩展etcd写性能?</title>
    <link href="/2022/10/09/17-%E6%80%A7%E8%83%BD%E5%8F%8A%E7%A8%B3%E5%AE%9A%E6%80%A7%EF%BC%88%E4%B8%8B%EF%BC%89%EF%BC%9A%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96%E5%8F%8A%E6%89%A9%E5%B1%95etcd%E5%86%99%E6%80%A7%E8%83%BD/"/>
    <url>/2022/10/09/17-%E6%80%A7%E8%83%BD%E5%8F%8A%E7%A8%B3%E5%AE%9A%E6%80%A7%EF%BC%88%E4%B8%8B%EF%BC%89%EF%BC%9A%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96%E5%8F%8A%E6%89%A9%E5%B1%95etcd%E5%86%99%E6%80%A7%E8%83%BD/</url>
    
    <content type="html"><![CDATA[<h1 id="17-性能及稳定性（下）：如何优化及扩展etcd写性能"><a href="#17-性能及稳定性（下）：如何优化及扩展etcd写性能" class="headerlink" title="17.性能及稳定性（下）：如何优化及扩展etcd写性能?"></a>17.性能及稳定性（下）：如何优化及扩展etcd写性能?</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><h2 id="写性能分析链路"><a href="#写性能分析链路" class="headerlink" title="写性能分析链路"></a>写性能分析链路</h2><p>为什么写入大量key-value数据的时候，会遇到Too Many Request限速错误呢？ 是写流程中的哪些环节出现了瓶颈？</p><p>和读请求类似，总结一个开启鉴权场景的写性能瓶颈及稳定性分析链路图，并在每个核心步骤数字旁边标识了影响性能、稳定性的关键因素。</p><p><img src="https://static001.geekbang.org/resource/image/14/0a/14ac1e7f1936f2def67b7fa24914070a.png?wh=1920*1167" alt="img"></p><h2 id="db-quota"><a href="#db-quota" class="headerlink" title="db quota"></a>db quota</h2><p>首先是流程一。在etcd v3.4.9版本中，client会通过clientv3库的Round-robin负载均衡算法，从endpoint列表中轮询选择一个endpoint访问，发起gRPC调用。</p><p>然后进入流程二。etcd收到gRPC写请求后，首先经过的是Quota模块，它会影响写请求的稳定性，若db大小超过配额就无法写入。</p><p><img src="https://static001.geekbang.org/resource/image/89/e8/89c9ccbf210861836cc3b5929b7ebae8.png?wh=1920*1227" alt="img"></p><p>etcd是个小型的元数据存储，默认db quota大小是2G，超过2G就只读无法写入。因此需要根据业务场景，适当调整db quota大小，并配置的合适的压缩策略。</p><p>etcd支持按时间周期性压缩、按版本号压缩两种策略，建议压缩策略不要配置得过于频繁。比如如果按时间周期压缩，一般情况下5分钟以上压缩一次比较合适，因为压缩过程中会加一系列锁和删除boltdb数据，过于频繁的压缩会对性能有一定的影响。</p><p>一般情况下db大小尽量不要超过8G，过大的db文件和数据量对集群稳定性各方面都会有一定的影响，详细你可以参考 <a href="https://blog.longpi1.com/2022/10/07/13-db%E5%A4%A7%E5%B0%8F%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88etcd%E7%A4%BE%E5%8C%BA%E5%BB%BA%E8%AE%AEdb%E5%A4%A7%E5%B0%8F%E4%B8%8D%E8%B6%85%E8%BF%878G%EF%BC%9F/">13</a>。</p><h2 id="限速"><a href="#限速" class="headerlink" title="限速"></a>限速</h2><p>通过流程二的Quota模块后，请求就进入流程三KVServer模块。在KVServer模块里，影响写性能的核心因素是限速。</p><p><img src="https://static001.geekbang.org/resource/image/78/14/78062ff5b8c5863d8802bdfacf32yy14.png?wh=1920*1233" alt="img"></p><p>KVServer模块的写请求在提交到Raft模块前，会进行限速判断。如果Raft模块已提交的日志索引（committed index）比已应用到状态机的日志索引（applied index）超过了5000，那么它就返回一个”etcdserver: too many requests”错误给client。</p><p>那么哪些情况可能会导致committed Index远大于applied index呢?</p><p>首先是long expensive read request导致写阻塞。比如etcd 3.4版本之前长读事务会持有较长时间的buffer读锁，而写事务又需要升级锁更新buffer，因此出现写阻塞乃至超时。最终导致etcd server应用已提交的Raft日志命令到状态机缓慢。堆积过多时，则会触发限速。</p><p>其次etcd定时批量将boltdb写事务提交的时候，需要对B+ tree进行重平衡、分裂，并将freelist、dirty page、meta page持久化到磁盘。此过程需要持有boltdb事务锁，若磁盘随机写性能较差、瞬间大量写入，则也容易写阻塞，应用已提交的日志条目缓慢。</p><p>最后执行defrag等运维操作时，也会导致写阻塞，它们会持有相关锁，导致写性能下降。</p><h2 id="心跳及选举参数优化"><a href="#心跳及选举参数优化" class="headerlink" title="心跳及选举参数优化"></a>心跳及选举参数优化</h2><p>写请求经过KVServer模块后，则会提交到流程四的Raft模块。我们知道etcd写请求需要转发给Leader处理，因此影响此模块性能和稳定性的核心因素之一是集群Leader的稳定性。</p><p><img src="https://static001.geekbang.org/resource/image/66/2c/660a03c960cd56610e3c43e15c14182c.png?wh=1920*1247" alt="img"></p><p>那如何判断Leader的稳定性呢?</p><p>答案是日志和metrics。</p><p>一方面，在使用etcd过程中，你很可能见过如下Leader发送心跳超时的警告日志，你可以通过此日志判断集群是否有频繁切换Leader的风险。</p><p>另一方面，可以通过etcd_server_leader_changes_seen_total metrics来观察已发生Leader切换的次数。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">21</span>:<span class="hljs-number">30</span>:<span class="hljs-number">27</span> etcd3 | &#123;<span class="hljs-string">&quot;level&quot;</span>:<span class="hljs-string">&quot;warn&quot;</span>,<span class="hljs-string">&quot;ts&quot;</span>:<span class="hljs-string">&quot;2021-02-23T21:30:27.255+0800&quot;</span>,<span class="hljs-string">&quot;caller&quot;</span>:<span class="hljs-string">&quot;wal/wal.go:782&quot;</span>,<span class="hljs-string">&quot;msg&quot;</span>:<span class="hljs-string">&quot;slow fdatasync&quot;</span>,<span class="hljs-string">&quot;took&quot;</span>:<span class="hljs-string">&quot;3.259857956s&quot;</span>,<span class="hljs-string">&quot;expected-duration&quot;</span>:<span class="hljs-string">&quot;1s&quot;</span>&#125;<br><span class="hljs-attribute">21</span>:<span class="hljs-number">30</span>:<span class="hljs-number">30</span> etcd3 | &#123;<span class="hljs-string">&quot;level&quot;</span>:<span class="hljs-string">&quot;warn&quot;</span>,<span class="hljs-string">&quot;ts&quot;</span>:<span class="hljs-string">&quot;2021-02-23T21:30:30.396+0800&quot;</span>,<span class="hljs-string">&quot;caller&quot;</span>:<span class="hljs-string">&quot;etcdserver/raft.go:390&quot;</span>,<span class="hljs-string">&quot;msg&quot;</span>:<span class="hljs-string">&quot;leader failed to send out heartbeat on time; took too long, leader is overloaded likely from slow disk&quot;</span>,<span class="hljs-string">&quot;to&quot;</span>:<span class="hljs-string">&quot;91bc3c398fb3c146&quot;</span>,<span class="hljs-string">&quot;heartbeat-interval&quot;</span>:<span class="hljs-string">&quot;100ms&quot;</span>,<span class="hljs-string">&quot;expected-duration&quot;</span>:<span class="hljs-string">&quot;200ms&quot;</span>,<span class="hljs-string">&quot;exceeded-duration&quot;</span>:<span class="hljs-string">&quot;827.162111ms&quot;</span>&#125;<br><br></code></pre></td></tr></table></figure><p>那么哪些因素会导致此日志产生以及发生Leader切换呢?</p><p>首先，etcd是基于Raft协议实现数据复制和高可用的，各节点会选出一个Leader，然后Leader将写请求同步给各个Follower节点。而Follower节点如何感知Leader异常，发起选举，正是依赖Leader的心跳机制。</p><p>在etcd中，Leader节点会根据heartbeart-interval参数（默认100ms）定时向Follower节点发送心跳。如果两次发送心跳间隔超过2*heartbeart-interval，就会打印此警告日志。超过election timeout（默认1000ms），Follower节点就会发起新一轮的Leader选举。</p><p>哪些原因会导致心跳超时呢？</p><p>一方面可能是磁盘IO比较慢。因为etcd从Raft的Ready结构获取到相关待提交日志条目后，它需要将此消息写入到WAL日志中持久化。可以通过观察etcd_wal_fsync_durations_seconds_bucket指标来确定写WAL日志的延时。若延时较大，可以使用SSD硬盘解决。</p><p>另一方面也可能是CPU使用率过高和网络延时过大导致。CPU使用率较高可能导致发送心跳的goroutine出现饥饿。若etcd集群跨地域部署，节点之间RTT延时大，也可能会导致此问题。</p><p>最后应该如何调整心跳相关参数，以避免频繁Leader选举呢？</p><p>etcd默认心跳间隔是100ms，较小的心跳间隔会导致发送频繁的消息，消耗CPU和网络资源。而较大的心跳间隔，又会导致检测到Leader故障不可用耗时过长，影响业务可用性。一般情况下，为了避免频繁Leader切换，可以根据实际部署环境、业务场景，将心跳间隔时间调整到100ms到400ms左右，选举超时时间要求至少是心跳间隔的10倍。</p><h2 id="网络和磁盘IO延时"><a href="#网络和磁盘IO延时" class="headerlink" title="网络和磁盘IO延时"></a>网络和磁盘IO延时</h2><p>当集群Leader稳定后，就可以进入Raft日志同步流程。</p><p>假设收到写请求的节点就是Leader，写请求通过Propose接口提交到Raft模块后，Raft模块会输出一系列消息。</p><p>etcd server的raftNode goroutine通过Raft模块的输出接口Ready，获取到待发送给Follower的日志条目追加消息和待持久化的日志条目。</p><p>raftNode goroutine首先通过HTTP协议将日志条目追加消息广播给各个Follower节点，也就是流程五。</p><p><img src="https://static001.geekbang.org/resource/image/8d/eb/8dd9d414eb4ef3ba9a7603fayy991aeb.png?wh=1920*1254" alt="img"></p><p>流程五涉及到各个节点之间网络通信，因此节点之间RTT延时对其性能有较大影响。跨可用区、跨地域部署时性能会出现一定程度下降，建议你结合实际网络环境使用benchmark工具测试一下。etcd Raft网络模块在实现上，也会通过流式发送和pipeline等技术优化来降低延时、提高网络性能。</p><p>同时，raftNode goroutine也会将待持久化的日志条目追加到WAL中，它可以防止进程crash后数据丢失，也就是流程六。注意此过程需要同步等待数据落地，因此磁盘顺序写性能决定着性能优异。</p><p>为了提升写吞吐量，etcd会将一批日志条目批量持久化到磁盘。etcd是个对磁盘IO延时非常敏感的服务，如果服务对性能、稳定性有较大要求，建议你使用SSD盘。</p><p>那使用SSD盘的etcd集群和非SSD盘的etcd集群写性能差异有多大呢？</p><p>下面是SSD盘集群，执行如下benchmark命令的压测结果，写QPS 51298，平均延时189ms。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs routeros">benchmark <span class="hljs-attribute">--endpoints</span>=addr <span class="hljs-attribute">--conns</span>=100 <span class="hljs-attribute">--clients</span>=1000 \<br>    put <span class="hljs-attribute">--key-size</span>=8 --sequential-keys <span class="hljs-attribute">--total</span>=10000000 --<br><span class="hljs-attribute">val-size</span>=256<br><br></code></pre></td></tr></table></figure><p><img src="https://static001.geekbang.org/resource/image/91/14/913e9875ef32df415426a3e5e7cff814.png?wh=1424*1364" alt="img"></p><p>下面是非SSD盘集群，执行同样benchmark命令的压测结果，写QPS 35255，平均延时279ms。</p><p><img src="https://static001.geekbang.org/resource/image/17/2f/1758a57804be463228e6431a388c552f.png?wh=1432*1354" alt="img"></p><h2 id="快照参数优化"><a href="#快照参数优化" class="headerlink" title="快照参数优化"></a>快照参数优化</h2><p>在Raft模块中，正常情况下，Leader可快速地将key-value写请求同步给其他Follower节点。但是某Follower节点若数据落后太多，Leader内存中的Raft日志已经被compact了，那么Leader只能发送一个快照给Follower节点重建恢复。</p><p>在快照较大的时候，发送快照可能会消耗大量的CPU、Memory、网络资源，那么它就会影响读写性能，也就是图中的流程七。</p><p><img src="https://static001.geekbang.org/resource/image/1a/38/1ab7a084e61d84f44b893a0fbbdc0138.png?wh=1920*1262" alt="img"></p><p>一方面， etcd Raft模块引入了流控机制，来解决日志同步过程中可能出现的大量资源开销、导致集群不稳定的问题。</p><p>另一方面，可以通过快照参数优化，去降低Follower节点通过Leader快照重建的概率，使其尽量能通过增量的日志同步保持集群的一致性。</p><p>etcd提供了一个名为–snapshot-count的参数来控制快照行为。它是指收到多少个写请求后就触发生成一次快照，并对Raft日志条目进行压缩。为了帮助slower Follower赶上Leader进度，etcd在生成快照，压缩日志条目的时候也会至少保留5000条日志条目在内存中。</p><p>那snapshot-count参数设置多少合适呢?</p><p>snapshot-count值过大它会消耗较多内存，可以参考15内存篇中Raft日志内存占用分析。过小则的话在某节点数据落后时，如果它请求同步的日志条目Leader已经压缩了，此时就不得不将整个db文件发送给落后节点，然后进行快照重建。</p><p>快照重建是极其昂贵的操作，对服务质量有较大影响，因此需要尽量避免快照重建。etcd 3.2版本之前snapshot-count参数值是1万，比较低，短时间内大量写入就较容易触发慢的Follower节点快照重建流程。etcd 3.2版本后将其默认值调大到10万，老版本升级的时候，需要注意配置文件是否写死固定的参数值。</p><h2 id="大value"><a href="#大value" class="headerlink" title="大value"></a>大value</h2><p>当写请求对应的日志条目被集群多数节点确认后，就可以提交到状态机执行了。etcd的raftNode goroutine就可通过Raft模块的输出接口Ready，获取到已提交的日志条目，然后提交到Apply模块的FIFO待执行队列。因为它是串行应用执行命令，任意请求在应用到状态机时阻塞都会导致写性能下降。</p><p>当Raft日志条目命令从FIFO队列取出执行后，它会首先通过授权模块校验是否有权限执行对应的写操作，对应图中的流程八。影响其性能因素是RBAC规则数和锁。</p><p><img src="https://static001.geekbang.org/resource/image/53/f6/5303f1b003480d2ddfe7dbd56b05b3f6.png?wh=1920*1179" alt="img"></p><p>然后通过权限检查后，写事务则会从treeIndex模块中查找key、更新的key版本号等信息，对应图中的流程九，影响其性能因素是key数和锁。</p><p>更新完索引后，就可以把新版本号作为boltdb key， 把用户key&#x2F;value、版本号等信息组合成一个value，写入到boltdb，对应图中的流程十，影响其性能因素是大value、锁。</p><p>如果你在应用中保存1Mb的value，这会给etcd稳定性带来哪些风险呢？</p><p>首先会导致读性能大幅下降、内存突增、网络带宽资源出现瓶颈等，上节课我已和你分享过一个1MB的key-value读性能压测结果，QPS从17万骤降到1100多。</p><p>那么写性能具体会下降到多少呢？</p><p>通过benchmark执行如下命令写入1MB的数据时候，集群几乎不可用（三节点8核16G，非SSD盘），事务提交P99延时高达4秒，如下图所示。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs routeros">benchmark <span class="hljs-attribute">--endpoints</span>=addr <span class="hljs-attribute">--conns</span>=100 <span class="hljs-attribute">--clients</span>=1000 \<br>put <span class="hljs-attribute">--key-size</span>=8 --sequential-keys <span class="hljs-attribute">--total</span>=500 --val-<br><span class="hljs-attribute">size</span>=1024000<br><br></code></pre></td></tr></table></figure><p><img src="https://static001.geekbang.org/resource/image/0c/bb/0c2635d617245f5d4084fbe48820e4bb.png?wh=1007*157" alt="img"></p><p>因此只能将写入的key-value大小调整为100KB。执行后得到如下结果，写入QPS 仅为1119&#x2F;S，平均延时高达324ms。</p><p><img src="https://static001.geekbang.org/resource/image/a7/63/a745af37d76208c08be147ac46018463.png?wh=1302*1324" alt="img"></p><p>其次etcd底层使用的boltdb存储，它是个基于COW(Copy-on-write)机制实现的嵌入式key-value数据库。较大的value频繁更新，因为boltdb的COW机制，会导致boltdb大小不断膨胀，很容易超过默认db quota值，导致无法写入。</p><p>那如何优化呢？</p><p>首先，如果业务已经使用了大key，拆分、改造存在一定客观的困难，那就从问题的根源之一的写入对症下药，尽量不要频繁更新大key，这样etcd db大小就不会快速膨胀。</p><p>可以从业务场景考虑，判断频繁的更新是否合理，能否做到增量更新。之前遇到一个case， 一个业务定时更新大量key，导致被限速，最后业务通过增量更新解决了问题。</p><p>如果写请求降低不了， 就必须进行精简、拆分数据结构了。将需要频繁更新的数据拆分成小key进行更新等，实现将value值控制在合理范围以内，才能让集群跑的更稳、更高效。</p><p>Kubernetes的Node心跳机制优化就是这块一个非常优秀的实践。早期kubelet会每隔10s上报心跳更新Node资源。但是此资源对象较大，导致db大小不断膨胀，无法支撑更大规模的集群。为了解决这个问题，社区做了数据拆分，将经常变更的数据拆分成非常细粒度的对象，实现了集群稳定性提升，支撑住更大规模的Kubernetes集群。</p><h2 id="boltdb锁"><a href="#boltdb锁" class="headerlink" title="boltdb锁"></a>boltdb锁</h2><p>了解完大value对集群性能的影响后，我们再看影响流程十的另外一个核心因素boltdb锁。</p><p>首先我们回顾下etcd读写性能优化历史，它经历了以下流程：</p><ul><li>3.0基于Raft log read实现线性读，线性读需要经过磁盘IO，性能较差；</li><li>3.1基于ReadIndex实现线性读，每个节点只需要向Leader发送ReadIndex请求，不涉及磁盘IO，提升了线性读性能；</li><li>3.2将访问boltdb的锁从互斥锁优化到读写锁，提升了并发读的性能；</li><li>3.4实现全并发读，去掉了buffer锁，长尾读几乎不再影响写。</li></ul><p>并发读特性的核心原理是创建读事务对象时，它会全量拷贝当前写事务未提交的buffer数据，并发的读写事务不再阻塞在一个buffer资源锁上，实现了全并发读。</p><p>最重要的是，写事务也不再因为expensive read request长时间阻塞，有效的降低了写请求的延时，详细测试结果可以参考 <a href="https://github.com/etcd-io/etcd/pull/10523">并发读特性实现PR</a>。</p><h2 id="扩展性能"><a href="#扩展性能" class="headerlink" title="扩展性能"></a>扩展性能</h2><p>当然有不少业务场景你即便用最高配的硬件配置，etcd可能还是无法解决你所面临的性能问题。etcd社区也考虑到此问题，提供了一个名为 <a href="https://etcd.io/docs/v3.4.0/op-guide/grpc_proxy/">gRPC proxy</a> 的组件，帮助你扩展读、扩展watch、扩展Lease性能的机制，如下图所示。</p><p><img src="https://static001.geekbang.org/resource/image/4a/b1/4a13ec9a4f93931e6e0656c600c2d3b1.png?wh=1920*1042" alt="img"></p><h3 id="扩展读"><a href="#扩展读" class="headerlink" title="扩展读"></a>扩展读</h3><p>如果client比较多，etcd集群节点连接数大于2万，或者想平行扩展串行读的性能，那么gRPC proxy就是良好一个解决方案。它是个无状态节点，提供高性能的读缓存的能力。可以根据业务场景需要水平扩容若干节点，同时通过连接复用，降低服务端连接数、负载。</p><p>它也提供了故障探测和自动切换能力，当后端etcd某节点失效后，会自动切换到其他正常节点，业务client可对此无感知。</p><h3 id="扩展Watch"><a href="#扩展Watch" class="headerlink" title="扩展Watch"></a>扩展Watch</h3><p>大量的watcher会显著增大etcd server的负载，导致读写性能下降。etcd为了解决这个问题，gRPC proxy组件里面提供了watcher合并的能力。如果多个client Watch同key或者范围（如上图三个client Watch同key）时，它会尝试将你的watcher进行合并，降低服务端的watcher数。</p><p>然后当它收到etcd变更消息时，会根据每个client实际Watch的版本号，将增量的数据变更版本，分发给你的多个client，实现watch性能扩展及提升。</p><h3 id="扩展Lease"><a href="#扩展Lease" class="headerlink" title="扩展Lease"></a>扩展Lease</h3><p>我们知道etcd Lease特性，提供了一种客户端活性检测机制。为了确保你的key不被淘汰，client需要定时发送keepalive心跳给server。当Lease非常多时，这就会导致etcd服务端的负载增加。在这种场景下，gRPC proxy提供了keepalive心跳连接合并的机制，来降低服务端负载。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>通过从上至下的写请求流程分析，介绍了各个流程中可能存在的瓶颈和优化方法、最佳实践。讨论的etcd性能优化、扩展问题分为了以下几类：</p><ul><li>业务应用层，etcd应用层的最佳实践；</li><li>etcd内核层，etcd参数最佳实践；</li><li>操作系统层，操作系统优化事项；</li><li>硬件及网络层，不同的硬件设备对etcd性能有着非常大的影响；</li><li>扩展性能，基于gRPC proxy扩展读、Watch、Lease的性能。</li></ul><p><img src="https://static001.geekbang.org/resource/image/92/87/928a4f1e66200531f5ee73aab000ce87.png?wh=1920*1091" alt="img"></p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>16.性能及稳定性（上）：如何优化及扩展etcd读性能？</title>
    <link href="/2022/10/09/16-%E6%80%A7%E8%83%BD%E5%8F%8A%E7%A8%B3%E5%AE%9A%E6%80%A7%EF%BC%88%E4%B8%8A%EF%BC%89%EF%BC%9A%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96%E5%8F%8A%E6%89%A9%E5%B1%95etcd%E8%AF%BB%E6%80%A7%E8%83%BD%EF%BC%9F/"/>
    <url>/2022/10/09/16-%E6%80%A7%E8%83%BD%E5%8F%8A%E7%A8%B3%E5%AE%9A%E6%80%A7%EF%BC%88%E4%B8%8A%EF%BC%89%EF%BC%9A%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96%E5%8F%8A%E6%89%A9%E5%B1%95etcd%E8%AF%BB%E6%80%A7%E8%83%BD%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="16-性能及稳定性（上）：如何优化及扩展etcd读性能？"><a href="#16-性能及稳定性（上）：如何优化及扩展etcd读性能？" class="headerlink" title="16.性能及稳定性（上）：如何优化及扩展etcd读性能？"></a>16.性能及稳定性（上）：如何优化及扩展etcd读性能？</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><h2 id="读性能分析链路"><a href="#读性能分析链路" class="headerlink" title="读性能分析链路"></a>读性能分析链路</h2><p>为什么在业务场景中读性能不如预期呢？ 是读流程中的哪一个环节出现了瓶颈？</p><p>在下图中，总结了一个开启密码鉴权场景的读性能瓶颈分析链路图，并在每个核心步骤数字旁边，标出了影响性能的关键因素。之所以选用密码鉴权的读请求为案例，是因为它使用较广泛并且请求链路覆盖最全，同时它也是最容易遇到性能瓶颈的场景。</p><p><img src="https://static001.geekbang.org/resource/image/7f/52/7f8c66ded3e151123b18768b880a2152.png?wh=1920*1253" alt="img"></p><p>接下来按照这张链路分析图，深入分析一个使用密码鉴权的线性读请求，一起看看影响它性能表现的核心因素以及最佳优化实践。</p><h2 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h2><p>首先是流程一负载均衡。在etcd 3.4以前，client为了节省与server节点的连接数，clientv3负载均衡器最终只会选择一个sever节点IP，与其建立一个长连接。</p><p>但是这可能会导致对应的server节点过载（如单节点流量过大，出现丢包）， 其他节点却是低负载，最终导致业务无法获得集群的最佳性能。在etcd 3.4后，引入了Round-robin负载均衡算法，它通过轮询的方式依次从endpoint列表中选择一个endpoint访问(长连接)，使server节点负载尽量均衡。</p><p>所以，如果使用的是etcd低版本，那么通过Load Balancer访问后端etcd集群。因为一方面Load Balancer一般支持配置各种负载均衡算法，如连接数、Round-robin等，可以使集群负载更加均衡，规避etcd client早期的固定连接缺陷，获得集群最佳性能。</p><p>另一方面，当集群节点需要替换、扩缩容集群节点的时候，不需要去调整各个client访问server的节点配置。</p><h2 id="选择合适的鉴权"><a href="#选择合适的鉴权" class="headerlink" title="选择合适的鉴权"></a>选择合适的鉴权</h2><p>client通过负载均衡算法为请求选择好etcd server节点后，client就可调用server的Range RPC方法，把请求发送给etcd server。在此过程中，如果server启用了鉴权，那么就会返回无权限相关错误给client。</p><p>如果server使用的是密码鉴权，你在创建client时，需指定用户名和密码。etcd clientv3库发现用户名、密码非空，就会先校验用户名和密码是否正确。</p><p>client是如何向sever请求校验用户名、密码正确性的呢？</p><p>client是通过向server发送Authenticate RPC鉴权请求实现密码认证的，也就是图中的流程二。</p><p><img src="https://static001.geekbang.org/resource/image/9e/61/9e1fb86567b351641db9586081c0e361.png?wh=1920*1267" alt="img"></p><p>根据之前介绍的密码认证原理，server节点收到鉴权请求后，它会从boltdb获取此用户密码对应的算法版本、salt、cost值，并基于用户的请求明文密码计算出一个hash值。</p><p>在得到hash值后，就可以对比db里保存的hash密码是否与其一致了。如果一致，就会返回一个token给client。 这个token是client访问server节点的通行证，后续server只需要校验“通行证”是否有效即可，无需每次发起昂贵的Authenticate RPC请求。</p><p>若你的业务在访问etcd过程中未复用token，每次访问etcd都发起一次Authenticate调用，这将是一个非常大的性能瓶颈和隐患。为了保证密码的安全性，密码认证（Authenticate）的开销非常昂贵，涉及到大量CPU资源。</p><p>那这个Authenticate接口究竟有多慢呢？</p><p>为了得到Authenticate接口的性能，一个测试：</p><ul><li>压测集群etcd节点配置是16核32G；</li><li>压测方式是通过修改etcd clientv3库、benchmark工具，使benchmark工具支持Authenticate接口压测；</li><li>然后设置不同的client和connection参数，运行多次，观察结果是否稳定，获取测试结果。</li></ul><p>最终的测试结果非常惊人。etcd v3.4.9之前的版本，Authenticate接口性能不到16 QPS，并且随着client和connection增多，该性能会继续恶化。</p><p>当client和connection的数量达到200个的时候，性能会下降到8 QPS，P99延时为18秒，如下图所示。</p><p><img src="https://static001.geekbang.org/resource/image/bc/c4/bc6336b93de53e6650bd7a5565ef8ec4.png?wh=1470*1284" alt="img"></p><p>由于导致Authenticate接口性能差的核心瓶颈，是在于密码鉴权使用了bcrpt计算hash值，因此Authenticate性能已接近极限。</p><p><img src="https://static001.geekbang.org/resource/image/44/aa/449bb47bef89a7cf1d2fbb1205a15faa.png?wh=1286*1362" alt="img"></p><p>最令人头疼的是，Auenticate的调用由clientv3库默默发起的，etcd中也没有任何日志记录其耗时等。当大家开启密码鉴权后，遇到读写接口超时的时候，未详细了解etcd的同学就会非常困惑，很难定位超时本质原因。</p><p>问题大都是由比较频繁的Authenticate调用导致，只要临时关闭鉴权或升级到etcd v3.4.9版本就可以恢复。</p><p>密码鉴权的性能如此差，可是业务又需要使用它，该怎么解决密码鉴权的性能问题呢？对此，三点建议。</p><p>第一，如果你的生产环境需要开启鉴权，并且读写QPS较大，不要图省事使用密码鉴权。最好使用证书鉴权，这样能完美避坑认证性能差、token过期等问题，性能几乎无损失。</p><p>第二，确保业务每次发起请求时有复用token机制，尽可能减少Authenticate RPC调用。</p><p>第三，如果使用密码鉴权时遇到性能瓶颈问题，可将etcd升级到3.4.9及以上版本，能适当提升密码鉴权的性能。</p><h2 id="选择合适的读模式"><a href="#选择合适的读模式" class="headerlink" title="选择合适的读模式"></a>选择合适的读模式</h2><p>client通过server的鉴权后，就可以发起读请求调用了，也就是图中的流程三。</p><p><img src="https://static001.geekbang.org/resource/image/58/9a/5832f5da0f916b941b1d832e9fe2e29a.png?wh=1920*1270" alt="img"></p><p>在这个步骤中，读模式对性能有着至关重要的影响。前面讲过etcd提供了串行读和线性读两种读模式。前者因为不经过ReadIndex模块，具有低延时、高吞吐量的特点；而后者在牺牲一点延时和吞吐量的基础上，实现了数据的强一致性读。这两种读模式分别为不同场景的读提供了解决方案。</p><p>关于串行读和线性读的性能对比，下图给出了一个测试结果，测试环境如下：</p><ul><li>机器配置client 16核32G，三个server节点8核16G、SSD盘，client与server节点都在同可用区；</li><li>各节点之间RTT在0.1ms到0.2ms之间；</li><li>etcd v3.4.9版本；</li><li>1000个client。</li></ul><p>执行如下串行读压测命令：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros">benchmark <span class="hljs-attribute">--endpoints</span>=addr <span class="hljs-attribute">--conns</span>=100 <span class="hljs-attribute">--clients</span>=1000 \<br>range hello <span class="hljs-attribute">--consistency</span>=s <span class="hljs-attribute">--total</span>=500000<br><br></code></pre></td></tr></table></figure><p>得到串行读压测结果如下，32万 QPS，平均延时2.5ms。</p><p><img src="https://static001.geekbang.org/resource/image/3d/9a/3d18aafb016a93e8d2f07a4193cb6b9a.png?wh=1426*1336" alt="img"></p><p>执行如下线性读压测命令：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros">benchmark <span class="hljs-attribute">--endpoints</span>=addr <span class="hljs-attribute">--conns</span>=100 <span class="hljs-attribute">--clients</span>=1000 \<br>range hello <span class="hljs-attribute">--consistency</span>=l <span class="hljs-attribute">--total</span>=500000<br><br></code></pre></td></tr></table></figure><p>得到线性读压测结果如下，19万 QPS，平均延时4.9ms。</p><p><img src="https://static001.geekbang.org/resource/image/83/0d/831338d142bc44999cc6c3b04147yy0d.png?wh=1474*1346" alt="img"></p><p>从两个压测结果图中你可以看到，在100个连接时，串行读性能比线性读性能高近11万&#x2F;s，串行读请求延时（2.5ms）比线性读延时约低一半（4.9ms）。</p><p><strong>需要注意的是，以上读性能数据是在1个key、没有任何写请求、同可用区的场景下压测出来的，实际的读性能会随着你的写请求增多而出现显著下降，这也是实际业务场景性能与社区压测结果存在非常大差距的原因之一。</strong> 建议使用etcd benchmark工具在etcd集群环境中自测一下，也可以参考下面的 <a href="https://etcd.io/docs/v3.4.0/op-guide/performance/">etcd社区压测结果</a>。</p><p><img src="https://static001.geekbang.org/resource/image/58/ca/58135ebf14a25e3f74004929369867ca.png?wh=1398*590" alt="img"></p><p>如果业务场景读QPS较大，但是又不想通过etcd proxy等机制来扩展性能，那可以进一步评估业务场景对数据一致性的要求高不高。如果可以容忍短暂的不一致，那你可以通过串行读来提升etcd的读性能，也可以部署Learner节点给可能会产生expensive read request的业务使用，实现cheap&#x2F;expensive read request隔离。</p><h2 id="线性读实现机制、网络延时"><a href="#线性读实现机制、网络延时" class="headerlink" title="线性读实现机制、网络延时"></a>线性读实现机制、网络延时</h2><p>了解完读模式对性能的影响后，我们继续往下分析。在我们这个密码鉴权读请求的性能分析案例中，读请求使用的是etcd默认线性读模式。线性读对应图中的流程四、流程五，其中流程四对应的是ReadIndex，流程五对应的是等待本节点数据追上Leader的进度（ApplyWait）。</p><p><img src="https://static001.geekbang.org/resource/image/f0/f1/f018b98629360e7c6eef6f9cfb0241f1.png?wh=1920*1162" alt="img"></p><p>在早期的etcd 3.0版本中，etcd线性读是基于Raft log read实现的。每次读请求要像写请求一样，生成一个Raft日志条目，然后提交给Raft一致性模块处理，基于Raft日志执行的有序性来实现线性读。因为该过程需要经过磁盘I&#x2F;O，所以性能较差。</p><p>为了解决Raft log read的线性读性能瓶颈，etcd 3.1中引入了ReadIndex。ReadIndex仅涉及到各个节点之间网络通信，因此节点之间的RTT延时对其性能有较大影响。虽然同可用区可获取到最佳性能，但是存在单可用区故障风险。如果你想实现高可用区容灾的话，那就必须牺牲一点性能了。</p><p>跨可用区部署时，各个可用区之间延时一般在2毫秒内。如果跨城部署，服务性能就会下降较大。所以一般场景下我不建议你跨城部署，你可以通过Learner节点实现异地容灾。如果异地的服务对数据一致性要求不高，那么你甚至可以通过串行读访问Learner节点，来实现就近访问，低延时。</p><p>各个节点之间的RTT延时，是决定流程四ReadIndex性能的核心因素之一。</p><h2 id="磁盘IO性能、写QPS"><a href="#磁盘IO性能、写QPS" class="headerlink" title="磁盘IO性能、写QPS"></a>磁盘IO性能、写QPS</h2><p>到了流程五，影响性能的核心因素就是磁盘IO延时和写QPS。</p><p><img src="https://static001.geekbang.org/resource/image/73/bc/732ec57338e1yy1d932e959ed776c0bc.png?wh=1920*1336" alt="img"></p><p>如下面代码所示，流程五是指节点从Leader获取到最新已提交的日志条目索引(rs.Index)后，它需要等待本节点当前已应用的Raft日志索引，大于等于Leader的已提交索引，确保能在本节点状态机中读取到最新数据。</p><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs fortran"><span class="hljs-keyword">if</span> ai := s.getAppliedIndex(); ai &lt; rs.<span class="hljs-built_in">Index</span> &#123;<br>   <span class="hljs-keyword">select</span> &#123;<br>   <span class="hljs-keyword">case</span> &lt;-s.applyWait.<span class="hljs-keyword">Wait</span>(rs.<span class="hljs-built_in">Index</span>):<br>   <span class="hljs-keyword">case</span> &lt;-s.stopping:<br>      <span class="hljs-keyword">return</span><br>   &#125;<br>&#125;<br>// unblock <span class="hljs-built_in">all</span> l-reads requested at indices before rs.<span class="hljs-built_in">Index</span><br>nr.notify(nil)<br><br></code></pre></td></tr></table></figure><p>而应用已提交日志条目到状态机的过程中又涉及到随机写磁盘，详情可参考我们 <a href="https://time.geekbang.org/column/article/336766">03</a> 中介绍过etcd的写请求原理。</p><p>因此可以知道， <strong>etcd是一个对磁盘IO性能非常敏感的存储系统，磁盘IO性能不仅会影响Leader稳定性、写性能表现，还会影响读性能。线性读性能会随着写性能的增加而快速下降。如果业务对性能、稳定性有较大要求，建议尽量使用SSD盘。</strong></p><p>下表给出了一个8核16G的三节点集群，在总key数只有一个的情况下，随着写请求增大，线性读性能下降的趋势总结（基于benchmark工具压测结果），可以直观感受下读性能是如何随着写性能下降。</p><p><img src="https://static001.geekbang.org/resource/image/40/5a/4069e72370942764ef4905715267c05a.jpg?wh=1475*620" alt="img"></p><p>当本节点已应用日志条目索引大于等于Leader已提交的日志条目索引后，读请求就会接到通知，就可通过MVCC模块获取数据。</p><h2 id="RBAC规则数、Auth锁"><a href="#RBAC规则数、Auth锁" class="headerlink" title="RBAC规则数、Auth锁"></a>RBAC规则数、Auth锁</h2><p>读请求到了MVCC模块后，首先要通过鉴权模块判断此用户是否有权限访问请求的数据路径，也就是流程六。影响流程六的性能因素是你的RBAC规则数和锁。</p><p>首先是RBAC规则数，为了解决快速判断用户对指定key范围是否有权限，etcd为每个用户维护了读写权限区间树。基于区间树判断用户访问的范围是否在用户的读写权限区间内，时间复杂度仅需要O(logN)。</p><p>另外一个因素则是AuthStore的锁。在etcd 3.4.9之前的，校验密码接口可能会占用较长时间的锁，导致授权接口阻塞。etcd 3.4.9之后合入了缩小锁范围的PR，可一定程度降低授权接口被阻塞的问题。</p><h2 id="expensive-request、treeIndex锁"><a href="#expensive-request、treeIndex锁" class="headerlink" title="expensive request、treeIndex锁"></a>expensive request、treeIndex锁</h2><p>通过流程六的授权后，则进入流程七，从treeIndex中获取整个查询涉及的key列表版本号信息。在这个流程中，影响其性能的关键因素是treeIndex的总key数、查询的key数、获取treeIndex锁的耗时。</p><p><img src="https://static001.geekbang.org/resource/image/9d/da/9dfe22355a9fd841943fb1c4556db9da.png?wh=1920*1272" alt="img"></p><p>首先，treeIndex中总key数过多会适当增大遍历的耗时。</p><p>其次，若要访问treeIndex我们必须获取到锁，但是可能其他请求如compact操作也会获取锁。早期的时候，它需要遍历所有索引，然后进行数据压缩工作。这就会导致其他请求阻塞，进而增大延时。</p><p>为了解决这个性能问题，优化方案是compact的时候会将treeIndex克隆一份，以空间来换时间，尽量降低锁阻塞带来的超时问题。</p><p>接下来重点介绍查询key数较多等expensive read request时对性能的影响。</p><p>假设链路分析图中的请求是查询一个Kubernetes集群所有Pod，当Pod数一百以内的时候可能对etcd影响不大，但是当你Pod数千甚至上万的时候， 流程七、八就会遍历大量的key，导致请求耗时突增、内存上涨、性能急剧下降。</p><p>如果业务就是有这种expensive read request逻辑，我们该如何应对呢？</p><p>首先可以尽量减少expensive read request次数，在程序启动的时候，只List一次全量数据，然后通过etcd Watch机制去获取增量变更数据。比如Kubernetes的Informer机制，就是典型的优化实践。</p><p>其次，在设计上评估是否能进行一些数据分片、拆分等，不同场景使用不同的etcd prefix前缀。比如在Kubernetes中，不要把Pod全部都部署在default命名空间下，尽量根据业务场景按命名空间拆分部署。即便每个场景全量拉取，也只需要遍历自己命名空间下的资源，数据量上将下降一个数量级。</p><p>再次，如果觉得Watch改造大、数据也无法分片，开发麻烦，你可以通过分页机制按批拉取，尽量减少一次性拉取数万条数据。</p><p>最后，如果以上方式都不起作用的话，还可以通过引入cache实现缓存expensive read request的结果，不过应用需维护缓存数据与etcd的一致性。</p><h2 id="大key-value、boltdb锁"><a href="#大key-value、boltdb锁" class="headerlink" title="大key-value、boltdb锁"></a>大key-value、boltdb锁</h2><p>从流程七获取到key列表及版本号信息后，我们就可以访问boltdb模块，获取key-value信息了。在这个流程中，影响其性能表现的，除了我们上面介绍的expensive read request，还有大key-value和锁。</p><p>首先是大key-value。我们知道etcd设计上定位是个小型的元数据存储，它没有数据分片机制，默认db quota只有2G，实践中往往不会超过8G，并且针对每个key-value大小，它也进行了大小限制，默认是1.5MB。</p><p>大key-value非常容易导致etcd OOM、server 节点出现丢包、性能急剧下降等。</p><p>那么当我们往etcd集群写入一个1MB的key-value时，它的线性读性能会从17万QPS具体下降到多少呢?</p><p>我们可以执行如下benchmark命令：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros">benchmark <span class="hljs-attribute">--endpoints</span>=addr <span class="hljs-attribute">--conns</span>=100 <span class="hljs-attribute">--clients</span>=1000 \<br>range key <span class="hljs-attribute">--consistency</span>=l <span class="hljs-attribute">--total</span>=10000<br><br></code></pre></td></tr></table></figure><p>得到其结果如下，从下图可以看到，读取一个1MB的key-value，线性读性能QPS下降到1163，平均延时上升到818ms，可见大key-value对性能的巨大影响。</p><p><img src="https://static001.geekbang.org/resource/image/a0/c7/a0735af4c2efd4156d392f75yyf132c7.png?wh=1296*1324" alt="img"></p><p>同时，从下面的etcd监控图上你也可以看到内存出现了突增，若存在大量大key-value时，可想而知，etcd内存肯定暴涨，大概率会OOM。</p><p><img src="https://static001.geekbang.org/resource/image/95/78/9599ec869c1496e8f9a8e5e54acb5b78.png?wh=1326*412" alt="img"></p><p>其次是锁，etcd为了提升boltdb读的性能，从etcd 3.1到etcd 3.4版本，分别进行过几次重大优化，在下一节中我将和你介绍。</p><p>以上就是一个开启密码鉴权场景，线性读请求的性能瓶颈分析过程。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>优化读性能的核心思路是首先通过etcd clientv3自带的Round-robin负载均衡算法或者Load Balancer，尽量确保整个集群负载均衡。</p><p>然后，在开启鉴权场景时，建议尽量使用证书而不是密码认证，避免校验密码的昂贵开销。</p><p>其次，根据业务场景选择合适的读模式，串行读比线性度性能提高30%以上，延时降低一倍。线性读性能受节点之间RTT延时、磁盘IO延时、当前写QPS等多重因素影响。</p><p>最容易被大家忽视的就是写QPS对读QPS的影响，我通过一系列压测数据，整理成一个表格，让你更直观感受写QPS对读性能的影响。多可用区部署会导致节点RTT延时增高，读性能下降。因此你需要在高可用和高性能上做取舍和平衡。</p><p>最后在访问数据前，读性能还可能会受授权性能、expensive read request、treeIndex及boltdb的锁等影响。你需要遵循最佳实践，避免一个请求查询大量key、大key-value等，否则会导致读性能剧烈下降。</p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>15.内存：为什么你的etcd内存占用那么高？</title>
    <link href="/2022/10/07/15-%E5%86%85%E5%AD%98%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E7%9A%84etcd%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E9%82%A3%E4%B9%88%E9%AB%98%EF%BC%9F/"/>
    <url>/2022/10/07/15-%E5%86%85%E5%AD%98%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E7%9A%84etcd%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E9%82%A3%E4%B9%88%E9%AB%98%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="15-内存：为什么你的etcd内存占用那么高？"><a href="#15-内存：为什么你的etcd内存占用那么高？" class="headerlink" title="15.内存：为什么你的etcd内存占用那么高？"></a>15.内存：为什么你的etcd内存占用那么高？</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><p>在使用etcd的过程中，你是否被异常内存占用等现象困扰过？比如etcd中只保存了1个1MB的key-value，但是经过若干次修改后，最终etcd内存可能达到数G。它是由什么原因导致的？如何分析呢？</p><p>今天要分享的主题：etcd的内存。 帮助你掌握etcd内存抖动、异常背后的常见原因和分析方法，当遇到类似问题时，能独立定位、解决。同时，帮助在实际业务场景中，为集群节点配置充足的内存资源，遵循最佳实践，尽量减少expensive request，避免etcd内存出现突增，导致OOM。</p><h2 id="分析整体思路"><a href="#分析整体思路" class="headerlink" title="分析整体思路"></a>分析整体思路</h2><p>当遇到etcd内存占用较高的案例时，你脑海中第一反应是什么呢？</p><p>下图以etcd写请求流程为例，总结的可能导致etcd内存占用较高的核心模块与其数据结构。</p><p><img src="https://static001.geekbang.org/resource/image/c2/49/c2673ebb2db4b555a9fbe229ed1bda49.png?wh=1920*1056" alt="img"></p><p>从图中可以看到，当etcd收到一个写请求后，gRPC Server会建立连接。连接数越多，会导致etcd进程的fd、goroutine等资源上涨，因此会使用越来越多的内存。</p><p>其次，基于Raft知识背景，它需要将此请求的日志条目保存在raftLog里面。etcd raftLog后端实现是内存存储，核心就是数组。因此raftLog使用的内存与其保存的日志条目成正比，它也是内存分析过程中最容易被忽视的一个数据结构。</p><p>然后当此日志条目被集群多数节点确认后，在应用到状态机的过程中，会在内存treeIndex模块的B-tree中创建、更新key与版本号信息。 在这过程中treeIndex模块的B-tree使用的内存与key、历史版本号数量成正比。</p><p>更新完treeIndex模块的索引信息后，etcd将key-value数据持久化存储到boltdb。boltdb使用了mmap技术，将db文件映射到操作系统内存中。因此在未触发操作系统将db对应的内存page换出的情况下，etcd的db文件越大，使用的内存也就越大。</p><p>同时，在这个过程中还有两个注意事项。</p><p>一方面，其他client可能会创建若干watcher、监听这个写请求涉及的key， etcd也需要使用一定的内存维护watcher、推送key变化监听的事件。</p><p>另一方面，如果这个写请求的key还关联了Lease，Lease模块会在内存中使用数据结构Heap来快速淘汰过期的Lease，因此Heap也是一个占用一定内存的数据结构。</p><p>最后，不仅仅是写请求流程会占用内存，读请求本身也会导致内存上升。尤其是expensive request，当产生大包查询时，MVCC模块需要使用内存保存查询的结果，很容易导致内存突增。</p><p>基于以上读写流程图对核心数据结构使用内存的分析，我们定位问题时就有线索、方法可循了。那如何确定是哪个模块、场景导致的内存异常呢？</p><p>接下来通过一个实际案例，深入介绍下内存异常的分析方法。</p><h2 id="一个key使用数G内存的案例"><a href="#一个key使用数G内存的案例" class="headerlink" title="一个key使用数G内存的案例"></a>一个key使用数G内存的案例</h2><p>我们通过goreman启动一个3节点etcd集群(linux&#x2F;etcd v3.4.9)，db quota为6G，执行如下的命令并观察etcd内存占用情况：</p><ul><li>执行1000次的put同一个key操作，value为1MB；</li><li>更新完后并进行compact、defrag操作；</li></ul><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># put同一个key，执行1000次</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> &#123;1<span class="hljs-built_in">..</span>1000&#125;; <span class="hljs-keyword">do</span> dd <span class="hljs-attribute">if</span>=/dev/urandom <span class="hljs-attribute">bs</span>=1024<br><span class="hljs-attribute">count</span>=1024  | <span class="hljs-attribute">ETCDCTL_API</span>=3 etcdctl put key  || break; done<br><br><span class="hljs-comment"># 获取最新revision，并压缩</span><br>etcdctl compact `(etcdctl endpoint status <span class="hljs-attribute">--write-out</span>=<span class="hljs-string">&quot;json&quot;</span> | egrep -o <span class="hljs-string">&#x27;&quot;revision&quot;:[0-9]*&#x27;</span> | egrep -o <span class="hljs-string">&#x27;[0-9].*&#x27;</span>)`<br><br><span class="hljs-comment"># 对集群所有节点进行碎片整理</span><br>etcdctl defrag --cluster<br><br></code></pre></td></tr></table></figure><p>在执行操作前，空集群etcd db size 20KB，etcd进程内存36M左右，分别如下图所示。</p><p><img src="https://static001.geekbang.org/resource/image/c1/e6/c1fb89ae1d6218a66cf1db30c41d9be6.png?wh=1164*358" alt="img"></p><p><img src="https://static001.geekbang.org/resource/image/6c/6d/6ce074583f39cd9a19bdcb392133426d.png?wh=1318*420" alt="img"></p><p>预测执行1000次同样key更新后，etcd进程占用了多少内存呢? 约37M？ 1G？ 2G？3G？ 还是其他呢？</p><p>执行1000次的put操作后，db大小和etcd内存占用分别如下图所示。</p><p><img src="https://static001.geekbang.org/resource/image/d6/45/d6dc86f76f52dfed73ab1771ebbbf545.png?wh=1302*348" alt="img"></p><p><img src="https://static001.geekbang.org/resource/image/9d/70/9d97762851c18a0c4cd89aa5a7bb0270.png?wh=1310*404" alt="img"></p><p>当执行compact、defrag命令后，如下图所示，db大小只有1M左右，但是会发现etcd进程实际却仍占用了2G左右内存。</p><p><img src="https://static001.geekbang.org/resource/image/93/bd/937c3fb0bf12595928e8ae4b05b7a5bd.png?wh=1260*354" alt="img"></p><p><img src="https://static001.geekbang.org/resource/image/8d/58/8d2d9fb3c0193745d80fe68b0cb4a758.png?wh=1276*406" alt="img"></p><p>整个集群只有一个key，为什么etcd占用了这么多的内存呢？是etcd发生了内存泄露吗？接下来进行原因分析</p><h2 id="raftLog"><a href="#raftLog" class="headerlink" title="raftLog"></a>raftLog</h2><p>当发起一个put请求的时候，etcd需通过Raft模块将此请求同步到其他节点，详细流程可结合下图再次了解下。</p><p><img src="https://static001.geekbang.org/resource/image/df/2c/df9yy18a1e28e18295cfc15a28cd342c.png?wh=1920*1328" alt="img"></p><p>从图中可以看到，Raft模块的输入是一个消息&#x2F;Msg，输出统一为Ready结构。etcd会把此请求封装成一个消息，提交到Raft模块。</p><p>Raft模块收到此请求后，会把此消息追加到raftLog的unstable存储的entry内存数组中（图中流程2），并且将待持久化的此消息封装到Ready结构内，通过管道通知到etcdserver（图中流程3）。</p><p>etcdserver取出消息，持久化到WAL中，并追加到raftLog的内存存储storage的entry数组中（图中流程5）。</p><p>下面是 <a href="https://github.com/etcd-io/etcd/blob/v3.4.9/raft/log.go#L24:L45">raftLog</a> 的核心数据结构，它由storage、unstable、committed、applied等组成。storage存储已经持久化到WAL中的日志条目，unstable存储未持久化的条目和快照，一旦持久化会及时删除日志条目，因此不存在过多内存占用的问题。</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">type</span> raftLog struct &#123;<br>   // <span class="hljs-keyword">storage</span> contains <span class="hljs-keyword">all</span> <span class="hljs-keyword">stable</span> entries since the last <span class="hljs-keyword">snapshot</span>.<br>   <span class="hljs-keyword">storage</span> <span class="hljs-keyword">Storage</span><br><br>   // unstable contains <span class="hljs-keyword">all</span> unstable entries <span class="hljs-keyword">and</span> <span class="hljs-keyword">snapshot</span>.<br>   // they will be saved <span class="hljs-keyword">into</span> <span class="hljs-keyword">storage</span>.<br>   unstable unstable<br><br>   // <span class="hljs-keyword">committed</span> <span class="hljs-keyword">is</span> the highest <span class="hljs-keyword">log</span> position that <span class="hljs-keyword">is</span> known <span class="hljs-keyword">to</span> be <span class="hljs-keyword">in</span><br>   // <span class="hljs-keyword">stable</span> <span class="hljs-keyword">storage</span> <span class="hljs-keyword">on</span> a quorum <span class="hljs-keyword">of</span> nodes.<br>   <span class="hljs-keyword">committed</span> uint64<br>   // applied <span class="hljs-keyword">is</span> the highest <span class="hljs-keyword">log</span> position that the application has<br>   // been instructed <span class="hljs-keyword">to</span> apply <span class="hljs-keyword">to</span> its state machine.<br>   // Invariant: applied &lt;= <span class="hljs-keyword">committed</span><br>   applied uint64<br>&#125;<br><br></code></pre></td></tr></table></figure><p>从上面raftLog结构体中，可以看到，存储稳定的日志条目的storage类型是Storage，Storage定义了存储Raft日志条目的核心API接口，业务应用层可根据实际场景进行定制化实现。etcd使用的是Raft算法库本身提供的MemoryStorage，其定义如下，核心是使用了一个数组来存储已经持久化后的日志条目。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-comment">// MemoryStorage implements the Storage interface backed</span><br><span class="hljs-comment">// by an in-memory array.</span><br>type MemoryStorage struct &#123;<br>   <span class="hljs-comment">// Protects access to all fields. Most methods of MemoryStorage are</span><br>   <span class="hljs-comment">// run on the raft goroutine， but Append() is run on an application</span><br>   <span class="hljs-comment">// goroutine.</span><br>   sync<span class="hljs-selector-class">.Mutex</span><br><br>   hardState pb<span class="hljs-selector-class">.HardState</span><br>   snapshot  pb<span class="hljs-selector-class">.Snapshot</span><br>   <span class="hljs-comment">// ents[i] has raftLog position i+snapshot.Metadata.Index</span><br>   ents <span class="hljs-selector-attr">[]</span>pb<span class="hljs-selector-class">.Entry</span><br>&#125;<br><br></code></pre></td></tr></table></figure><p>那么随着写请求增多，内存中保留的Raft日志条目会越来越多，如何防止etcd出现OOM呢？</p><p>etcd提供了<strong>快照和压缩</strong>功能来解决这个问题。</p><p>首先可以通过调整–snapshot-count参数来控制生成快照的频率，其值默认是100000（etcd v3.4.9，早期etcd版本是10000），也就是每10万个写请求触发一次快照生成操作。</p><p>快照生成完之后，etcd会通过压缩来删除旧的日志条目。</p><p>那么是全部删除日志条目还是保留一小部分呢？</p><p>答案是保留一小部分Raft日志条目。数量由DefaultSnapshotCatchUpEntries参数控制，默认5000，目前不支持自定义配置。</p><p>保留一小部分日志条目其实是为了帮助慢的Follower以较低的开销向Leader获取Raft日志条目，以尽快追上Leader进度。若raftLog中不保留任何日志条目，就只能发送快照给慢的Follower，这开销就非常大了。</p><p>通过以上分析可知，如果你的请求key-value比较大，比如上面我们的案例中是1M，1000次修改，那么etcd raftLog至少会消耗1G的内存。这就是为什么内存随着写请求修改次数不断增长的原因。</p><p>除了raftLog占用内存外，MVCC模块的treeIndex&#x2F;boltdb模块又是如何使用内存的呢？</p><h2 id="treeIndex"><a href="#treeIndex" class="headerlink" title="treeIndex"></a>treeIndex</h2><p>一个put写请求的日志条目被集群多数节点确认提交后，这时etcdserver就会从Raft模块获取已提交的日志条目，应用到MVCC模块的treeIndex和boltdb。</p><p>我们知道treeIndex是基于google内存btree库实现的一个索引管理模块，在etcd中每个key都会在treeIndex中保存一个索引项(keyIndex)，记录你的key和版本号等信息，如下面的数据结构所示。</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs gauss"><span class="hljs-built_in">type</span> keyIndex <span class="hljs-keyword">struct</span> &#123;<br>   <span class="hljs-built_in">key</span>         []byte<br>   modified    revision <span class="hljs-comment">// the main rev of the last modification</span><br>   generations []generation<br>&#125;<br><br></code></pre></td></tr></table></figure><p>同时，每次对key的修改、删除操作都会在key的索引项中追加一条修改记录(revision)。因此，随着修改次数的增加，etcd内存会一直增加。那么如何清理旧版本，防止过多的内存占用呢？</p><p>答案也是压缩。当你执行compact命令时，etcd会遍历treeIndex中的各个keyIndex，清理历史版本号记录与已删除的key，释放内存。</p><p>从上面的keyIndex数据结构我们可知，一个key的索引项内存开销跟你的key大小、保存的历史版本数、compact策略有关。为了避免内存索引项占用过多的内存，key的长度不应过长，同时需要配置好合理的压缩策略。</p><h2 id="boltdb"><a href="#boltdb" class="headerlink" title="boltdb"></a>boltdb</h2><p>在treeIndex模块中创建、更新完keyIndex数据结构后，你的key-value数据、各种版本号、lease等相关信息会保存到如下的一个mvccpb.keyValue结构体中。它是boltdb的value，key则是treeIndex中保存的版本号，然后通过boltdb的写接口保存到db文件中。</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs dts">kv := mvccpb.KeyValue<span class="hljs-punctuation">&#123;</span><br><span class="hljs-symbol">   Key:</span>            key，<br><span class="hljs-symbol">   Value:</span>          value，<br><span class="hljs-symbol">   CreateRevision:</span> c，<br><span class="hljs-symbol">   ModRevision:</span>    rev，<br><span class="hljs-symbol">   Version:</span>        ver，<br><span class="hljs-symbol">   Lease:</span>          int64(leaseID)，<br><span class="hljs-punctuation">&#125;</span><br><br></code></pre></td></tr></table></figure><p>前面介绍boltdb时，提到过etcd在启动时会通过mmap机制，将etcd db文件映射到etcd进程地址空间，并设置mmap的MAP_POPULATE flag，它会告诉Linux内核预读文件，让Linux内核将文件内容拷贝到物理内存中。</p><p>在节点内存足够的情况下，后续读请求可直接从内存中获取。相比read系统调用，mmap少了一次从page cache拷贝到进程内存地址空间的操作，因此具备更好的性能。</p><p>若etcd节点内存不足，可能会导致db文件对应的内存页被换出。当读请求命中的页未在内存中时，就会产生缺页异常，导致读过程中产生磁盘IO。这样虽然避免了etcd进程OOM，但是此过程会产生较大的延时。</p><p>从以上boltdb的key-value和mmap机制介绍中我们可知，我们应控制boltdb文件大小，优化key-value大小，配置合理的压缩策略，回收旧版本，避免过多内存占用。</p><h2 id="watcher"><a href="#watcher" class="headerlink" title="watcher"></a>watcher</h2><p>在写入key的时候，其他client还可通过etcd的Watch监听机制，获取到key的变化事件。</p><p>那创建一个watcher耗费的内存跟哪些因素有关呢?</p><p>在Watch机制设计与实现分析中，介绍过创建watcher的整体流程与架构，如下图所示。当创建一个watcher时，client与server建立连接后，会创建一个gRPC Watch Stream，随后通过这个gRPC Watch Stream发送创建watcher请求。</p><p>每个gRPC Watch Stream中etcd WatchServer会分配两个goroutine处理，一个是sendLoop，它负责Watch事件的推送。一个是recvLoop，负责接收client的创建、取消watcher请求消息。</p><p>同时对每个watcher来说，etcd的WatchableKV模块需将其保存到相应的内存管理数据结构中，实现可靠的Watch事件推送。</p><p><img src="https://static001.geekbang.org/resource/image/42/bf/42575d8d0a034e823b8e48d4ca0a49bf.png?wh=1920*1075" alt="img"></p><p>因此watch监听机制耗费的内存跟client连接数、gRPC Stream、watcher数(watching)有关，如下面公式所示：</p><ul><li>c1表示每个连接耗费的内存；</li><li>c2表示每个gRPC Stream耗费的内存；</li><li>c3表示每个watcher耗费的内存。</li></ul><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs armasm"><span class="hljs-symbol">memory</span> = <span class="hljs-built_in">c1</span> * number_of_conn + <span class="hljs-built_in">c2</span> *<br><span class="hljs-symbol">avg_number_of_stream_per_conn</span> + <span class="hljs-built_in">c3</span> *<br><span class="hljs-symbol">avg_number_of_watch_stream</span><br><br></code></pre></td></tr></table></figure><p>根据etcd社区的 <a href="https://etcd.io/docs/v3.4.0/benchmarks/etcd-3-watch-memory-benchmark/">压测报告</a>，大概估算出Watch机制中c1、c2、c3占用的内存分别如下：</p><ul><li>每个client连接消耗大约17kb的内存(c1)；</li><li>每个gRPC Stream消耗大约18kb的内存(c2)；</li><li>每个watcher消耗大约350个字节(c3)；</li></ul><p>当业务场景大量使用watcher的时候，应提前估算下内存容量大小，选择合适的内存配置节点。</p><p>注意以上估算并不包括watch事件堆积的开销。变更事件较多，服务端、客户端高负载，网络阻塞等情况都可能导致事件堆积。</p><p>在etcd 3.4.9版本中，每个watcher默认buffer是1024。buffer内保存watch响应结果，如watchID、watch事件（watch事件包含key、value）等。</p><p>若大量事件堆积，将产生较高昂的内存的开销。你可以通过etcd_debugging_mvcc_pending_events_total指标监控堆积的事件数，etcd_debugging_slow_watcher_total指标监控慢的watcher数，来及时发现异常。</p><h2 id="expensive-request"><a href="#expensive-request" class="headerlink" title="expensive request"></a>expensive request</h2><p>当写入比较大的key-value后，如果client频繁查询它，也会产生高昂的内存开销。</p><p>假设写入了100个这样1M大小的key， 通过Range接口一次查询100个key， 那么boltdb遍历、反序列化过程将花费至少100MB的内存。如下面代码所示，它会遍历整个key-value，将key-value保存到数组kvs中。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs go">kvs := <span class="hljs-built_in">make</span>([]mvccpb.KeyValue， limit)<br>revBytes := newRevBytes()<br><span class="hljs-keyword">for</span> i， revpair := <span class="hljs-keyword">range</span> revpairs[:<span class="hljs-built_in">len</span>(kvs)] &#123;<br>   revToBytes(revpair， revBytes)<br>   _， vs := tr.tx.UnsafeRange(keyBucketName， revBytes， <span class="hljs-literal">nil</span>， <span class="hljs-number">0</span>)<br>   <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(vs) != <span class="hljs-number">1</span> &#123;<br>        ......<br>   &#125;<br>   <span class="hljs-keyword">if</span> err := kvs[i].Unmarshal(vs[<span class="hljs-number">0</span>]); err != <span class="hljs-literal">nil</span> &#123;<br>        .......<br>   &#125;<br><br></code></pre></td></tr></table></figure><p>也就是说，一次查询就耗费了至少100MB的内存、产生了至少100MB的流量，随着QPS增大后，很容易OOM、网卡出现丢包。</p><p>count-only、limit查询在key百万级以上时，也会产生非常大的内存开销。因为它们在遍历treeIndex的过程中，会将相关key保存在数组里面。当key多时，此开销不容忽视。</p><h2 id="etcd-v2-x2F-goroutines-x2F-bug"><a href="#etcd-v2-x2F-goroutines-x2F-bug" class="headerlink" title="etcd v2&#x2F;goroutines&#x2F;bug"></a>etcd v2&#x2F;goroutines&#x2F;bug</h2><p>除了以上介绍的核心模块、expensive request场景可能导致较高的内存开销外，还有以下场景也会导致etcd内存使用较高。</p><p>首先是 <strong>etcd中使用了v2的API写入了大量的key-value数据</strong>，这会导致内存飙高。etcd v2的key-value都是存储在内存树中的，同时v2的watcher不支持多路复用，内存开销相比v3多了一个数量级。</p><p>在etcd 3.4版本之前，etcd默认同时支持etcd v2&#x2F;v3 API，etcd 3.4版本默认关闭了v2 API。 可以通过etcd v2 API和etcd v2内存存储模块的metrics前缀etcd_debugging_store，观察集群中是否有v2数据导致的内存占用高。</p><p>其次是 <strong>goroutines泄露</strong> 导致内存占用高。此问题可能会在容器化场景中遇到。etcd在打印日志的时候，若出现阻塞则可能会导致goroutine阻塞并持续泄露，最终导致内存泄露。可以通过观察、监控go_goroutines来发现这个问题。</p><p>最后是 <strong>etcd bug</strong> 导致的内存泄露。当基本排除以上场景导致的内存占用高后，则很可能是etcd bug导致的内存泄露。</p><p>比如早期etcd clientv3的lease keepalive租约频繁续期bug，它会导致Leader高负载、内存泄露，此bug已在3.2.24&#x2F;3.3.9版本中修复。</p><p>若内存泄露并不是已知的etcd bug导致，可以开启pprof， 尝试复现，通过分析pprof heap文件来确定消耗大量内存的模块和数据结构。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>通过一个写入1MB key的实际案例，介绍了可能导致etcd内存占用高的核心数据结构、场景，将可能导致内存占用较高的因素总结为了下面这幅图。</p><p><img src="https://static001.geekbang.org/resource/image/aa/90/aaf7b4f5f6f568dc70c1a0964fb92790.png?wh=1920*684" alt="img"></p><p>首先是raftLog。为了帮助slow Follower同步数据，它至少要保留5000条最近收到的写请求在内存中。若key非常大，更新5000次会产生较大的内存开销。</p><p>其次是treeIndex。 每个key-value会在内存中保留一个索引项。索引项的开销跟key长度、保留的历史版本有关，可以通过compact命令压缩。</p><p>然后是boltdb。etcd启动的时候，会通过mmap系统调用，将文件映射到虚拟内存中。可以通过compact命令回收旧版本，defrag命令进行碎片整理。</p><p>接着是watcher。它的内存占用跟连接数、gRPC Watch Stream数、watcher数有关。watch机制一个不可忽视的内存开销其实是事件堆积的占用缓存，可以通过相关metrics及时发现堆积的事件以及slow watcher。</p><p>最后介绍了一些典型的场景导致的内存异常，如大包查询等expensive request，etcd中存储了v2 API写入的key， goroutines泄露以及etcd lease bug等。</p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>14.延时：为什么你的etcd请求会出现超时？</title>
    <link href="/2022/10/07/14-%E5%BB%B6%E6%97%B6%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E7%9A%84etcd%E8%AF%B7%E6%B1%82%E4%BC%9A%E5%87%BA%E7%8E%B0%E8%B6%85%E6%97%B6%EF%BC%9F/"/>
    <url>/2022/10/07/14-%E5%BB%B6%E6%97%B6%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E7%9A%84etcd%E8%AF%B7%E6%B1%82%E4%BC%9A%E5%87%BA%E7%8E%B0%E8%B6%85%E6%97%B6%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="14-延时：为什么你的etcd请求会出现超时？"><a href="#14-延时：为什么你的etcd请求会出现超时？" class="headerlink" title="14.延时：为什么你的etcd请求会出现超时？"></a>14.延时：为什么你的etcd请求会出现超时？</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><p>在使用etcd的过程中，经常会被日志中的”apply request took too long”和“etcdserver: request timed out”等高延时现象困扰过？它们是由什么原因导致的呢？我们应该如何来分析这些问题？</p><p>这就是要分享的主题：etcd延时。掌握etcd延时抖动、超时背后的常见原因和分析方法，当遇到类似问题时，能独立定位、解决。同时，在实际业务场景中，合理配置集群，遵循最佳实践，尽量减少expensive request，避免etcd请求出现超时。</p><h2 id="分析思路及工具"><a href="#分析思路及工具" class="headerlink" title="分析思路及工具"></a>分析思路及工具</h2><p>首先，当面对一个高延时的请求案例后，如何梳理问题定位思路呢？</p><p>知彼知己，方能百战不殆，定位问题也是类似。首先我们得弄清楚产生问题的原理、流程，读写请求的核心链路。其次是熟练掌握相关工具，借助它们，可以帮助我们快速攻破疑难杂症。</p><p>Leader收到一个写请求，将一个日志条目复制到集群多数节点并应用到存储状态机的流程（如下图所示），通过此图看看写流程上哪些地方可能会导致请求超时呢？</p><p><img src="https://static001.geekbang.org/resource/image/df/2c/df9yy18a1e28e18295cfc15a28cd342c.png?wh=1920*1328" alt="img"></p><p>首先是流程四，一方面，Leader需要并行将消息通过网络发送给各Follower节点，依赖网络性能。另一方面，Leader需持久化日志条目到WAL，依赖磁盘I&#x2F;O顺序写入性能。</p><p>其次是流程八，应用日志条目到存储状态机时，etcd后端key-value存储引擎是boltdb。正如之前 所介绍的，它是一个基于B+ tree实现的存储引擎，当你写入数据，提交事务时，它会将dirty page持久化到磁盘中。在这过程中boltdb会产生磁盘随机I&#x2F;O写入，因此事务提交性能依赖磁盘I&#x2F;O随机写入性能。</p><p>最后，在整个写流程处理过程中，etcd节点的CPU、内存、网络带宽资源应充足，否则肯定也会影响性能。</p><p>初步了解完可能导致延时抖动的瓶颈处之后，我给你总结了etcd问题定位过程中常用的工具，你可以参考下面这幅图。</p><p><img src="https://static001.geekbang.org/resource/image/b5/fc/b5bb69c8effda97f2ef78b067ab1aafc.png?wh=1920*1300" alt="img"></p><p>图的左边是读写请求链路中可能出现瓶颈或异常的点，比如上面流程分析中提到的磁盘、内存、CPU、网络资源。</p><p>图的右边是常用的工具，分别是metrics、trace日志、etcd其他日志、WAL及boltdb分析工具等。</p><p>接下来，基于读写请求的核心链路和其可能出现的瓶颈点，结合相关的工具，深入分析etcd延时抖动的定位方法和原因。</p><h2 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h2><p>首先看看流程图中第一个提到可能瓶颈点，网络模块。</p><p>在etcd中，各个节点之间需要通过2380端口相互通信，以完成Leader选举、日志同步等功能，因此底层网络质量（吞吐量、延时、稳定性）对上层etcd服务的性能有显著影响。</p><p>网络资源出现异常的常见表现是连接闪断、延时抖动、丢包等。那么我们要如何定位网络异常导致的延时抖动呢？</p><p>一方面，可以使用常规的ping&#x2F;traceroute&#x2F;mtr、ethtool、ifconfig&#x2F;ip、netstat、tcpdump网络分析工具等命令，测试网络的连通性、延时，查看网卡的速率是否存在丢包等错误，确认etcd进程的连接状态及数量是否合理，抓取etcd报文分析等。</p><p>另一方面，etcd应用层提供了节点之间网络统计的metrics指标，分别如下：</p><ul><li>etcd_network_active_peer，表示peer之间活跃的连接数；</li><li>etcd_network_peer_round_trip_time_seconds，表示peer之间RTT延时；</li><li>etcd_network_peer_sent_failures_total，表示发送给peer的失败消息数；</li><li>etcd_network_client_grpc_sent_bytes_total，表示server发送给client的总字节数，通过这个指标我们可以监控etcd出流量；</li><li>etcd_network_client_grpc_received_bytes_total，表示server收到client发送的总字节数，通过这个指标可以监控etcd入流量。</li></ul><p>client入流量监控如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/26/ff/26617a4c08e7c1e155c4332058451cff.png?wh=866*356" alt="img"></p><p>client出流量如下图监控所示。 从图中你可以看到，峰值接近140MB&#x2F;s(1.12Gbps)，这是非常不合理的，说明业务中肯定有大量expensive read request操作。若etcd集群读写请求开始出现超时，你可以用ifconfig等命令查看是否出现丢包等错误。</p><p><img src="https://static001.geekbang.org/resource/image/4c/0b/4c8659e621305200b8f761b1e319460b.png?wh=856*356" alt="img"></p><p>etcd metrics指标名由namespace和subsystem、name组成。namespace为etcd， subsystem是模块名（比如network、name具体的指标名）。你可以在Prometheus里搜索etcd_network找到所有network相关的metrics指标名。</p><p>下面是一个集群中某节点异常后的metrics指标：</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs abnf">etcd_network_active_peers&#123;Local<span class="hljs-operator">=</span><span class="hljs-string">&quot;fd422379fda50e48&quot;</span>，Remote<span class="hljs-operator">=</span><span class="hljs-string">&quot;8211f1d0f64f3269&quot;</span>&#125; <span class="hljs-number">1</span><br>etcd_network_active_peers&#123;Local<span class="hljs-operator">=</span><span class="hljs-string">&quot;fd422379fda50e48&quot;</span>，Remote<span class="hljs-operator">=</span><span class="hljs-string">&quot;91bc3c398fb3c146&quot;</span>&#125; <span class="hljs-number">0</span><br>etcd_network_peer_sent_failures_total&#123;To<span class="hljs-operator">=</span><span class="hljs-string">&quot;91bc3c398fb3c146&quot;</span>&#125; <span class="hljs-number">47774</span><br>etcd_network_client_grpc_sent_bytes_total <span class="hljs-number">513207</span><br><br></code></pre></td></tr></table></figure><p>从以上metrics中，你可以看到91bc3c398fb3c146节点出现了异常。在etcd场景中，网络质量导致etcd性能下降主要源自两个方面：</p><p>一方面，expensive request中的大包查询会使网卡出现瓶颈，产生丢包等错误，从而导致etcd吞吐量下降、高延时。expensive request导致网卡丢包，出现超时，这在etcd中是非常典型且易发生的问题，它主要是因为业务没有遵循最佳实践，查询了大量key-value。</p><p>另一方面，在跨故障域部署的时候，故障域可能是可用区、城市。故障域越大，容灾级别越高，但各个节点之间的RTT越高，请求的延时更高。</p><h2 id="磁盘I-x2F-O"><a href="#磁盘I-x2F-O" class="headerlink" title="磁盘I&#x2F;O"></a>磁盘I&#x2F;O</h2><p>了解完网络问题的定位方法和导致网络性能下降的因素后，再看看最核心的磁盘I&#x2F;O。</p><p>正如Raft日志复制整体流程图中介绍的，在etcd中无论是Raft日志持久化还是boltdb事务提交，都依赖于磁盘I&#x2F;O的性能。</p><p><strong>当etcd请求延时出现波动时，我们往往首先关注disk相关指标是否正常。</strong> 我们可以通过etcd磁盘相关的metrics(etcd_disk_wal_fsync_duration_seconds和etcd_disk_backend_commit_duration_seconds)来观测应用层数据写入磁盘的性能。</p><p>etcd_disk_wal_fsync_duration_seconds（简称disk_wal_fsync）表示WAL日志持久化的fsync系统调用延时数据。一般本地SSD盘P99延时在10ms内，如下图所示。</p><p><img src="https://static001.geekbang.org/resource/image/9a/52/9a08490980abb23f90d8e59a83543e52.png?wh=1326*352" alt="img"></p><p>etcd_disk_backend_commit_duration_seconds（简称disk_backend_commit）表示后端boltdb事务提交的延时，一般P99在120ms内。</p><p><img src="https://static001.geekbang.org/resource/image/29/db/294600a0a144be38e9d7b69d9403f3db.png?wh=1338*356" alt="img"></p><p>这里需要注意的是，一般监控显示的磁盘延时都是P99，但实际上etcd对磁盘特别敏感，一次磁盘I&#x2F;O波动就可能产生Leader切换。如果遇到集群Leader出现切换、请求超时，但是磁盘指标监控显示正常，可以查看P100确认下是不是由于磁盘I&#x2F;O波动导致的。</p><p>同时etcd的WAL模块在fdatasync操作超过1秒时，也会在etcd中打印如下的日志，你可以结合日志进一步定位。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-keyword">if</span> took &gt; warnSyncDuration &#123;<br>   <span class="hljs-keyword">if</span> w<span class="hljs-selector-class">.lg</span> != nil &#123;<br>      w<span class="hljs-selector-class">.lg</span><span class="hljs-selector-class">.Warn</span>(<br>         <span class="hljs-string">&quot;slow fdatasync&quot;</span>,<br>         zap<span class="hljs-selector-class">.Duration</span>(<span class="hljs-string">&quot;took&quot;</span>, took),<br>         zap<span class="hljs-selector-class">.Duration</span>(<span class="hljs-string">&quot;expected-duration&quot;</span>, warnSyncDuration),<br>      )<br>   &#125; <span class="hljs-keyword">else</span> &#123;<br>      plog<span class="hljs-selector-class">.Warningf</span>(<span class="hljs-string">&quot;sync duration of %v, expected less than %v&quot;</span>, took, warnSyncDuration)<br>   &#125;<br>&#125;<br><br></code></pre></td></tr></table></figure><p>当disk_wal_fsync指标异常的时候，一般是底层硬件出现瓶颈或异常导致。当然也有可能是CPU高负载、cgroup blkio限制导致的，具体应该如何区分呢？</p><p>可以通过iostat、blktrace工具分析瓶颈是在应用层还是内核层、硬件层。其中blktrace是blkio层的磁盘I&#x2F;O分析利器，它可记录IO进入通用块层、IO请求生成插入请求队列、IO请求分发到设备驱动、设备驱动处理完成这一系列操作的时间，帮助你发现磁盘I&#x2F;O瓶颈发生的阶段。</p><p>当disk_backend_commit指标的异常时候，说明事务提交过程中的B+ tree树重平衡、分裂、持久化dirty page、持久化meta page等操作耗费了大量时间。</p><p>disk_backend_commit指标异常，能说明是磁盘I&#x2F;O发生了异常吗？</p><p>若disk_backend_commit较高、disk_wal_fsync却正常，说明瓶颈可能并非来自磁盘I&#x2F;O性能，也许是B+ tree的重平衡、分裂过程中的较高时间复杂度逻辑操作导致。比如etcd目前所有stable版本（etcd 3.2到3.4），从freelist中申请和回收若干连续空闲页的时间复杂度是O(N)，当db文件较大、空闲页碎片化分布的时候，则可能导致事务提交高延时。</p><p>那如何区分事务提交过程中各个阶段的耗时呢？</p><p>etcd还提供了disk_backend_commit_rebalance_duration和disk_backend_commit_spill_duration两个metrics，分别表示事务提交过程中B+ tree的重平衡和分裂操作耗时分布区间。</p><p>最后，需要注意disk_wal_fsync记录的是WAL文件顺序写入的持久化时间，disk_backend_commit记录的是整个事务提交的耗时。后者涉及的磁盘I&#x2F;O是随机的，为了保证你etcd集群的稳定性，建议使用SSD磁盘以确保事务提交的稳定性。</p><h2 id="expensive-request"><a href="#expensive-request" class="headerlink" title="expensive request"></a>expensive request</h2><p>若磁盘和网络指标都很正常，那么延时高还有可能是什么原因引起的呢？</p><p>从读请求链路我们可知，一个读写请求经过Raft模块处理后，最终会走到MVCC模块。那么在MVCC模块会有哪些场景导致延时抖动呢？时间耗在哪个处理流程上了？</p><p>etcd 3.4版本之前，在应用put&#x2F;txn等请求到状态机的apply和处理读请求range流程时，若一个请求执行超过100ms时，默认会在etcd log中打印一条”apply request took too long”的警告日志。通过此日志我们可以知道集群中apply流程产生了较慢的请求，但是不能确定具体是什么因素导致的。</p><p>比如在Kubernetes中，当集群Pod较多的时候，若你频繁执行List Pod，可能会导致etcd出现大量的”apply request took too long”警告日志。</p><p>因为对etcd而言，List Pod请求涉及到大量的key查询，会消耗较多的CPU、内存、网络资源，此类expensive request的QPS若较大，则很可能导致OOM、丢包。</p><p>当然，除了业务发起的expensive request请求导致延时抖动以外，也有可能是etcd本身的设计实现存在瓶颈。</p><p>比如在etcd 3.2和3.3版本写请求完成之前，需要更新MVCC的buffer，进行升级锁操作。然而此时若集群中出现了一个long expensive read request，则会导致写请求执行延时抖动。因为expensive read request事务会一直持有MVCC的buffer读锁，导致写请求事务阻塞在升级锁操作中。</p><p>在了解完expensive request对请求延时的影响后，接下来要如何解决请求延时较高问题的定位效率呢？</p><p>为了提高请求延时分布的可观测性、延时问题的定位效率，etcd社区在3.4版本后中实现了trace特性，详细记录了一个请求在各个阶段的耗时。若某阶段耗时流程超过默认的100ms，则会打印一条trace日志。</p><p>下面是我将trace日志打印的阈值改成1纳秒后读请求执行过程中的trace日志。从日志中你可以看到，trace日志记录了以下阶段耗时：</p><ul><li>agreement among raft nodes before linearized reading，此阶段读请求向Leader发起readIndex查询并等待本地applied index &gt;&#x3D; Leader的committed index， 但是你无法区分是readIndex慢还是等待本地applied index &gt; Leader的committed index慢。在etcd 3.5中新增了trace，区分了以上阶段；</li><li>get authentication metadata，获取鉴权元数据；</li><li>range keys from in-memory index tree，从内存索引B-tree中查找key列表对应的版本号列表；</li><li>range keys from bolt db，根据版本号列表从boltdb遍历，获得用户的key-value信息；</li><li>filter and sort the key-value pairs，过滤、排序key-value列表；</li><li>assemble the response，聚合结果。</li></ul><figure class="highlight scilab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs scilab">&#123;<br>    <span class="hljs-string">&quot;level&quot;</span>:<span class="hljs-string">&quot;info&quot;</span>，<br>    <span class="hljs-string">&quot;ts&quot;</span>:<span class="hljs-string">&quot;2020-12-16T08:11:43.720+0800&quot;</span>，<br>    <span class="hljs-string">&quot;caller&quot;</span>:<span class="hljs-string">&quot;traceutil/trace.go:145&quot;</span>，<br>    <span class="hljs-string">&quot;msg&quot;</span>:<span class="hljs-string">&quot;trace[789864563] range&quot;</span>，<br>    <span class="hljs-string">&quot;detail&quot;</span>:<span class="hljs-string">&quot;&#123;range_begin:a; range_end:; response_count:1; response_revision:32011; &#125;&quot;</span>，<br>    <span class="hljs-string">&quot;duration&quot;</span>:<span class="hljs-string">&quot;318.774µs&quot;</span>，<br>    <span class="hljs-string">&quot;start&quot;</span>:<span class="hljs-string">&quot;2020-12-16T08:11:43.719+0800&quot;</span>，<br>    <span class="hljs-string">&quot;end&quot;</span>:<span class="hljs-string">&quot;2020-12-16T08:11:43.720+0800&quot;</span>，<br>    <span class="hljs-string">&quot;steps&quot;</span>:[<br>        <span class="hljs-string">&quot;trace[789864563] &#x27;</span>agreement among raft nodes before linearized reading<span class="hljs-string">&#x27;  (duration: 255.227µs)&quot;</span>，<br>        <span class="hljs-string">&quot;trace[789864563] &#x27;</span>get authentication metadata<span class="hljs-string">&#x27;  (duration: 2.97µs)&quot;</span>，<br>        <span class="hljs-string">&quot;trace[789864563] &#x27;</span>range keys from in-memory index tree<span class="hljs-string">&#x27;  (duration: 44.578µs)&quot;</span>，<br>        <span class="hljs-string">&quot;trace[789864563] &#x27;</span>range keys from bolt db<span class="hljs-string">&#x27;  (duration: 8.688µs)&quot;</span>，<br>        <span class="hljs-string">&quot;trace[789864563] &#x27;</span>filter and sort the key-value pairs<span class="hljs-string">&#x27;  (duration: 578ns)&quot;</span>，<br>        <span class="hljs-string">&quot;trace[789864563] &#x27;</span>assemble the response<span class="hljs-string">&#x27;  (duration: 643ns)&quot;</span><br>    ]<br>&#125;<br><br></code></pre></td></tr></table></figure><p>那么写请求流程会记录哪些阶段耗时呢？</p><p>下面是put写请求的执行trace日志，记录了以下阶段耗时：</p><ul><li>process raft request，写请求提交到Raft模块处理完成耗时；</li><li>get key’s previous created_revision and leaseID，获取key上一个创建版本号及leaseID的耗时；</li><li>marshal mvccpb.KeyValue，序列化KeyValue结构体耗时；</li><li>store kv pair into bolt db，存储kv数据到boltdb的耗时；</li><li>attach lease to kv pair，将lease id关联到kv上所用时间。</li></ul><figure class="highlight scilab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs scilab">&#123;<br>    <span class="hljs-string">&quot;level&quot;</span>:<span class="hljs-string">&quot;info&quot;</span>，<br>    <span class="hljs-string">&quot;ts&quot;</span>:<span class="hljs-string">&quot;2020-12-16T08:25:12.707+0800&quot;</span>，<br>    <span class="hljs-string">&quot;caller&quot;</span>:<span class="hljs-string">&quot;traceutil/trace.go:145&quot;</span>，<br>    <span class="hljs-string">&quot;msg&quot;</span>:<span class="hljs-string">&quot;trace[1402827146] put&quot;</span>，<br>    <span class="hljs-string">&quot;detail&quot;</span>:<span class="hljs-string">&quot;&#123;key:16; req_size:8; response_revision:32030; &#125;&quot;</span>，<br>    <span class="hljs-string">&quot;duration&quot;</span>:<span class="hljs-string">&quot;6.826438ms&quot;</span>，<br>    <span class="hljs-string">&quot;start&quot;</span>:<span class="hljs-string">&quot;2020-12-16T08:25:12.700+0800&quot;</span>，<br>    <span class="hljs-string">&quot;end&quot;</span>:<span class="hljs-string">&quot;2020-12-16T08:25:12.707+0800&quot;</span>，<br>    <span class="hljs-string">&quot;steps&quot;</span>:[<br>        <span class="hljs-string">&quot;trace[1402827146] &#x27;</span>process raft request<span class="hljs-string">&#x27;  (duration: 6.659094ms)&quot;</span>，<br>        <span class="hljs-string">&quot;trace[1402827146] &#x27;</span>get key<span class="hljs-string">&#x27;s previous created_revision and leaseID&#x27;</span>  (duration: <span class="hljs-number">23.498</span>µs)<span class="hljs-string">&quot;，</span><br><span class="hljs-string">        &quot;</span>trace[<span class="hljs-number">1402827146</span>] <span class="hljs-string">&#x27;marshal mvccpb.KeyValue&#x27;</span>  (duration: <span class="hljs-number">1.857</span>µs)<span class="hljs-string">&quot;，</span><br><span class="hljs-string">        &quot;</span>trace[<span class="hljs-number">1402827146</span>] <span class="hljs-string">&#x27;store kv pair into bolt db&#x27;</span>  (duration: <span class="hljs-number">30.121</span>µs)<span class="hljs-string">&quot;，</span><br><span class="hljs-string">        &quot;</span>trace[<span class="hljs-number">1402827146</span>] <span class="hljs-string">&#x27;attach lease to kv pair&#x27;</span>  (duration: <span class="hljs-number">661</span>ns)<span class="hljs-string">&quot;</span><br><span class="hljs-string">    ]</span><br><span class="hljs-string">&#125;</span><br><span class="hljs-string"></span><br></code></pre></td></tr></table></figure><p>通过以上介绍的trace特性，就可以快速定位到高延时读写请求的原因。比如向etcd发起了一个涉及到大量key或value较大的expensive request请求的时候，它会产生如下的warn和trace日志。</p><p>从以下日志中可以看到，此请求查询的vip前缀下所有的kv数据总共是250条，但是涉及的数据包大小有250MB，总耗时约1.85秒，其中从boltdb遍历key消耗了1.63秒。</p><figure class="highlight d"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs d">&#123;<br>    <span class="hljs-string">&quot;level&quot;</span>:<span class="hljs-string">&quot;warn&quot;</span>，<br>    <span class="hljs-string">&quot;ts&quot;</span>:<span class="hljs-string">&quot;2020-12-16T23:02:53.324+0800&quot;</span>，<br>    <span class="hljs-string">&quot;caller&quot;</span>:<span class="hljs-string">&quot;etcdserver/util.go:163&quot;</span>，<br>    <span class="hljs-string">&quot;msg&quot;</span>:<span class="hljs-string">&quot;apply request took too long&quot;</span>，<br>    <span class="hljs-string">&quot;took&quot;</span>:<span class="hljs-string">&quot;1.84796759s&quot;</span>，<br>    <span class="hljs-string">&quot;expected-duration&quot;</span>:<span class="hljs-string">&quot;100ms&quot;</span>，<br>    <span class="hljs-string">&quot;prefix&quot;</span>:<span class="hljs-string">&quot;read-only range &quot;</span>，<br>    <span class="hljs-string">&quot;request&quot;</span>:<span class="hljs-string">&quot;key:&quot;</span>vip<span class="hljs-string">&quot; range_end:&quot;</span>vi<span class="hljs-string">q&quot; &quot;</span>，<br>    <span class="hljs-string">&quot;response&quot;</span>:<span class="hljs-string">&quot;range_response_count:250 size:262150651&quot;</span><br>&#125;<br>&#123;<br>    <span class="hljs-string">&quot;level&quot;</span>:<span class="hljs-string">&quot;info&quot;</span>，<br>    <span class="hljs-string">&quot;ts&quot;</span>:<span class="hljs-string">&quot;2020-12-16T23:02:53.324+0800&quot;</span>，<br>    <span class="hljs-string">&quot;caller&quot;</span>:<span class="hljs-string">&quot;traceutil/trace.go:145&quot;</span>，<br>    <span class="hljs-string">&quot;msg&quot;</span>:<span class="hljs-string">&quot;trace[370341530] range&quot;</span>，<br>    <span class="hljs-string">&quot;detail&quot;</span>:<span class="hljs-string">&quot;&#123;range_begin:vip; range_end:viq; response_count:250; response_revision:32666; &#125;&quot;</span>，<br>    <span class="hljs-string">&quot;duration&quot;</span>:<span class="hljs-string">&quot;1.850335038s&quot;</span>，<br>    <span class="hljs-string">&quot;start&quot;</span>:<span class="hljs-string">&quot;2020-12-16T23:02:51.473+0800&quot;</span>，<br>    <span class="hljs-string">&quot;end&quot;</span>:<span class="hljs-string">&quot;2020-12-16T23:02:53.324+0800&quot;</span>，<br>    <span class="hljs-string">&quot;steps&quot;</span>:[<br>        <span class="hljs-string">&quot;trace[370341530] &#x27;range keys from bolt db&#x27;  (duration: 1.632336981s)&quot;</span><br>    ]<br>&#125;<br><br></code></pre></td></tr></table></figure><p>最后，有两个注意事项。</p><p>第一，在etcd 3.4中，logger默认为capnslog，trace特性只有在当logger为zap时才开启，因此你需要设置–logger&#x3D;zap。</p><p>第二，trace特性并不能记录所有类型的请求，它目前只覆盖了MVCC模块中的range&#x2F;put&#x2F;txn等常用接口。像Authenticate鉴权请求，涉及到大量CPU计算，延时是非常高的，在trace日志中目前没有相关记录。</p><p>如果开启了密码鉴权，在连接数增多、QPS增大后，若突然出现请求超时，如何确定是鉴权还是查询、更新等接口导致的呢？</p><p>etcd默认参数并不会采集各个接口的延时数据，可以通过设置etcd的启动参数–metrics为extensive来开启，获得每个gRPC接口的延时数据。同时可结合各个gRPC接口的请求数，获得QPS。</p><p>如下是某节点的metrics数据，251个Put请求，返回码OK，其中有240个请求在100毫秒内完成。</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs abnf">grpc_server_handled_total&#123;grpc_code<span class="hljs-operator">=</span><span class="hljs-string">&quot;OK&quot;</span>，<br>grpc_method<span class="hljs-operator">=</span><span class="hljs-string">&quot;Put&quot;</span>，grpc_service<span class="hljs-operator">=</span><span class="hljs-string">&quot;etcdserverpb.KV&quot;</span>，<br>grpc_type<span class="hljs-operator">=</span><span class="hljs-string">&quot;unary&quot;</span>&#125; <span class="hljs-number">251</span><br><br>grpc_server_handling_seconds_bucket&#123;grpc_method<span class="hljs-operator">=</span><span class="hljs-string">&quot;Put&quot;</span>，grpc_service<span class="hljs-operator">=</span><span class="hljs-string">&quot;etcdserverpb.KV&quot;</span>，grpc_type<span class="hljs-operator">=</span><span class="hljs-string">&quot;unary&quot;</span>，le<span class="hljs-operator">=</span><span class="hljs-string">&quot;0.005&quot;</span>&#125; <span class="hljs-number">0</span><br>grpc_server_handling_seconds_bucket&#123;grpc_method<span class="hljs-operator">=</span><span class="hljs-string">&quot;Put&quot;</span>，grpc_service<span class="hljs-operator">=</span><span class="hljs-string">&quot;etcdserverpb.KV&quot;</span>，grpc_type<span class="hljs-operator">=</span><span class="hljs-string">&quot;unary&quot;</span>，le<span class="hljs-operator">=</span><span class="hljs-string">&quot;0.01&quot;</span>&#125; <span class="hljs-number">1</span><br>grpc_server_handling_seconds_bucket&#123;grpc_method<span class="hljs-operator">=</span><span class="hljs-string">&quot;Put&quot;</span>，grpc_service<span class="hljs-operator">=</span><span class="hljs-string">&quot;etcdserverpb.KV&quot;</span>，grpc_type<span class="hljs-operator">=</span><span class="hljs-string">&quot;unary&quot;</span>，le<span class="hljs-operator">=</span><span class="hljs-string">&quot;0.025&quot;</span>&#125; <span class="hljs-number">51</span><br>grpc_server_handling_seconds_bucket&#123;grpc_method<span class="hljs-operator">=</span><span class="hljs-string">&quot;Put&quot;</span>，grpc_service<span class="hljs-operator">=</span><span class="hljs-string">&quot;etcdserverpb.KV&quot;</span>，grpc_type<span class="hljs-operator">=</span><span class="hljs-string">&quot;unary&quot;</span>，le<span class="hljs-operator">=</span><span class="hljs-string">&quot;0.05&quot;</span>&#125; <span class="hljs-number">204</span><br>grpc_server_handling_seconds_bucket&#123;grpc_method<span class="hljs-operator">=</span><span class="hljs-string">&quot;Put&quot;</span>，grpc_service<span class="hljs-operator">=</span><span class="hljs-string">&quot;etcdserverpb.KV&quot;</span>，grpc_type<span class="hljs-operator">=</span><span class="hljs-string">&quot;unary&quot;</span>，le<span class="hljs-operator">=</span><span class="hljs-string">&quot;0.1&quot;</span>&#125; <span class="hljs-number">240</span><br><br></code></pre></td></tr></table></figure><h2 id="集群容量、节点CPU-x2F-Memory瓶颈"><a href="#集群容量、节点CPU-x2F-Memory瓶颈" class="headerlink" title="集群容量、节点CPU&#x2F;Memory瓶颈"></a>集群容量、节点CPU&#x2F;Memory瓶颈</h2><p>介绍完网络、磁盘I&#x2F;O、expensive request导致etcd请求延时较高的原因和分析方法后，我们再看看容量和节点资源瓶颈是如何导致高延时请求产生的。</p><p>若网络、磁盘I&#x2F;O正常，也无expensive request，那此时高延时请求是怎么产生的呢？它的trace日志会输出怎样的耗时结果？</p><p>下面是一个社区用户反馈的一个读接口高延时案例的两条trace日志。从第一条日志中我们可以知道瓶颈在于线性读的准备步骤，readIndex和wait applied index。</p><p>那么是其中具体哪个步骤导致的高延时呢？通过在etcd 3.5版本中细化此流程，我们获得了第二条日志，发现瓶颈在于等待applied index &gt;&#x3D; Leader的committed index。</p><figure class="highlight smalltalk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs smalltalk">&#123;<br><span class="hljs-comment">&quot;level&quot;</span>: <span class="hljs-comment">&quot;info&quot;</span>，<br><span class="hljs-comment">&quot;ts&quot;</span>: <span class="hljs-comment">&quot;2020-08-12T08:24:56.181Z&quot;</span>，<br><span class="hljs-comment">&quot;caller&quot;</span>: <span class="hljs-comment">&quot;traceutil/trace.go:145&quot;</span>，<br><span class="hljs-comment">&quot;msg&quot;</span>: <span class="hljs-comment">&quot;trace[677217921] range&quot;</span>，<br><span class="hljs-comment">&quot;detail&quot;</span>: <span class="hljs-comment">&quot;&#123;range_begin:/...redacted...; range_end:; response_count:1; response_revision:2725080604; &#125;&quot;</span>，<br><span class="hljs-comment">&quot;duration&quot;</span>: <span class="hljs-comment">&quot;1.553047811s&quot;</span>，<br><span class="hljs-comment">&quot;start&quot;</span>: <span class="hljs-comment">&quot;2020-08-12T08:24:54.628Z&quot;</span>，<br><span class="hljs-comment">&quot;end&quot;</span>: <span class="hljs-comment">&quot;2020-08-12T08:24:56.181Z&quot;</span>，<br><span class="hljs-comment">&quot;steps&quot;</span>: [<br><span class="hljs-comment">&quot;trace[677217921] &#x27;agreement among raft nodes before linearized reading&#x27;  (duration: 1.534322015s)&quot;</span><br>]<br>&#125;<br><br>&#123;<br>  <span class="hljs-comment">&quot;level&quot;</span>: <span class="hljs-comment">&quot;info&quot;</span>，<br>  <span class="hljs-comment">&quot;ts&quot;</span>: <span class="hljs-comment">&quot;2020-09-22T12:54:01.021Z&quot;</span>，<br>  <span class="hljs-comment">&quot;caller&quot;</span>: <span class="hljs-comment">&quot;traceutil/trace.go:152&quot;</span>，<br>  <span class="hljs-comment">&quot;msg&quot;</span>: <span class="hljs-comment">&quot;trace[2138445431] linearizableReadLoop&quot;</span>，<br>  <span class="hljs-comment">&quot;detail&quot;</span>: <span class="hljs-comment">&quot;&quot;</span>，<br>  <span class="hljs-comment">&quot;duration&quot;</span>: <span class="hljs-comment">&quot;855.447896ms&quot;</span>，<br>  <span class="hljs-comment">&quot;start&quot;</span>: <span class="hljs-comment">&quot;2020-09-22T12:54:00.166Z&quot;</span>，<br>  <span class="hljs-comment">&quot;end&quot;</span>: <span class="hljs-comment">&quot;2020-09-22T12:54:01.021Z&quot;</span>，<br>  <span class="hljs-comment">&quot;steps&quot;</span>: [<br>    <span class="hljs-comment">&quot;trace[2138445431] read index received  (duration: 824.408µs)&quot;</span>，<br>    <span class="hljs-comment">&quot;trace[2138445431] applied index is now lower than readState.Index  (duration: 854.622058ms)&quot;</span><br>  ]<br>&#125;<br><br></code></pre></td></tr></table></figure><p>为什么会发生这样的现象呢?</p><p>首先可以通过etcd_server_slow_apply_total指标，观查其值快速增长的时间点与高延时请求产生的日志时间点是否吻合。</p><p>其次检查是否存在大量写请求。线性读需确保本节点数据与Leader数据一样新， 若本节点的数据与Leader差异较大，本节点追赶Leader数据过程会花费一定时间，最终导致高延时的线性读请求产生。</p><p><strong>etcd适合读多写少的业务场景，若写请求较大，很容易出现容量瓶颈，导致高延时的读写请求产生。</strong></p><p>最后通过ps&#x2F;top&#x2F;mpstat&#x2F;perf等CPU、Memory性能分析工具，检查etcd节点是否存在CPU、Memory瓶颈。goroutine饥饿、内存不足都会导致高延时请求产生，若确定CPU和Memory存在异常，你可以通过开启debug模式，通过pprof分析CPU和内存瓶颈点。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>如下图所示，从以下几个方面介绍了会导致请求延时上升的原因：</p><ul><li>网络质量，如节点之间RTT延时、网卡带宽满，出现丢包；</li><li>磁盘I&#x2F;O抖动，会导致WAL日志持久化、boltdb事务提交出现抖动，Leader出现切换等；</li><li>expensive request，比如大包请求、涉及到大量key遍历、Authenticate密码鉴权等操作；</li><li>容量瓶颈，太多写请求导致线性读请求性能下降等；</li><li>节点配置，CPU繁忙导致请求处理延时、内存不够导致swap等。</li></ul><p><img src="https://static001.geekbang.org/resource/image/93/a3/9375f08cebd596b87b92623c10786fa3.png?wh=1920*1474" alt="img"></p><p>并在分析这些案例的过程中，给你介绍了etcd问题核心工具：metrics、etcd log、trace日志、blktrace、pprof等。</p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>13.db大小：为什么etcd社区建议db大小不超过8G？</title>
    <link href="/2022/10/07/13-db%E5%A4%A7%E5%B0%8F%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88etcd%E7%A4%BE%E5%8C%BA%E5%BB%BA%E8%AE%AEdb%E5%A4%A7%E5%B0%8F%E4%B8%8D%E8%B6%85%E8%BF%878G%EF%BC%9F/"/>
    <url>/2022/10/07/13-db%E5%A4%A7%E5%B0%8F%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88etcd%E7%A4%BE%E5%8C%BA%E5%BB%BA%E8%AE%AEdb%E5%A4%A7%E5%B0%8F%E4%B8%8D%E8%B6%85%E8%BF%878G%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="13-db大小：为什么etcd社区建议db大小不超过8G？"><a href="#13-db大小：为什么etcd社区建议db大小不超过8G？" class="headerlink" title="13.db大小：为什么etcd社区建议db大小不超过8G？"></a>13.db大小：为什么etcd社区建议db大小不超过8G？</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><p>在写流程中分享了etcd Quota模块，那么etcd为什么需要对db增加Quota限制，以及不建议etcd集群db大小超过8G呢？ 过大的db文件对集群性能和稳定性有哪些影响？</p><p>今天分享的主题就是关于db大小。将通过一个大数据量的etcd集群为案例，剖析etcd db大小配额限制背后的设计思考和过大的db潜在隐患。</p><p>帮助理解大数据量对集群的各个模块的影响，配置合理的db Quota值。同时，帮助在实际业务场景中，遵循最佳实践，尽量减少value大小和大key-value更新频率，避免db文件大小不断增长。</p><h2 id="分析整体思路"><a href="#分析整体思路" class="headerlink" title="分析整体思路"></a>分析整体思路</h2><p>首先写入大量数据，构造一个db大小为14G的大集群。然后通过此集群分析db大小的各个影响面，db大小影响面如下图所示。</p><p><img src="https://static001.geekbang.org/resource/image/ab/11/ab657951310461c835963c38e43fdc11.png?wh=1920*685" alt="img"></p><p>首先是 <strong>启动耗时</strong>。etcd启动的时候，需打开boltdb db文件，读取db文件所有key-value数据，用于重建内存treeIndex模块。因此在大量key导致db文件过大的场景中，这会导致etcd启动较慢。</p><p>其次是 <strong>节点内存配置</strong>。etcd在启动的时候会通过mmap将db文件映射内存中，若节点可用内存不足，小于db文件大小时，可能会出现缺页文件中断，导致服务稳定性、性能下降。</p><p>接着是 <strong>treeIndex</strong> 索引性能。因etcd不支持数据分片，内存中的treeIndex若保存了几十万到上千万的key，这会增加查询、修改操作的整体延时。</p><p>然后是 <strong>boltdb性能</strong>。大db文件场景会导致事务提交耗时增长、抖动。</p><p>再次是 <strong>集群稳定性</strong>。大db文件场景下，无论你是百万级别小key还是上千个大value场景，一旦出现expensive request后，很容易导致etcd OOM、节点带宽满而丢包。</p><p>最后是 <strong>快照。</strong> 当Follower节点落后Leader较多数据的时候，会触发Leader生成快照重建发送给Follower节点，Follower基于它进行还原重建操作。较大的db文件会导致Leader发送快照需要消耗较多的CPU、网络带宽资源，同时Follower节点重建还原慢。</p><h2 id="构造大集群"><a href="#构造大集群" class="headerlink" title="构造大集群"></a>构造大集群</h2><p>简单介绍完db大小的六个影响面后，我们下面来构造一个大数据量的集群，用于后续各个影响面的分析。</p><p>首先，我通过一系列如下 <a href="https://github.com/etcd-io/etcd/tree/v3.4.9/tools/benchmark">benchmark</a> 命令，向一个8核32G的3节点的集群写入120万左右key。key大小为32，value大小为256到10K，用以分析大db集群案例中的各个影响面。</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-string">./benchmark</span> put <span class="hljs-params">--key-size</span> 32 <span class="hljs-params">--val-size</span> 10240 <span class="hljs-params">--total</span><br>1000000 <span class="hljs-params">--key-space-size</span> 2000000 <span class="hljs-params">--clients</span> 50 <span class="hljs-params">--conns</span> 50<br><br></code></pre></td></tr></table></figure><p>执行完一系列benchmark命令后，db size达到14G，总key数达到120万，其监控如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/67/60/67aa0c0fe078byy681fe4c55a3983f60.png?wh=1338*362" alt="img"></p><p><img src="https://static001.geekbang.org/resource/image/33/88/331ac3c759578b297546f1651385be88.png?wh=1314*412" alt="img"></p><h2 id="启动耗时"><a href="#启动耗时" class="headerlink" title="启动耗时"></a>启动耗时</h2><p>在如上的集群中，我通过benchmark工具将etcd集群db大小压测到14G后，在重新启动etcd进程的时候，如下日志所示，你会发现启动比较慢，为什么大db文件会影响etcd启动耗时呢？</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">2021</span>-<span class="hljs-number">02</span>-<span class="hljs-number">15</span> <span class="hljs-number">02</span>:<span class="hljs-number">25</span>:<span class="hljs-number">55</span>.<span class="hljs-number">273712</span> I | etcdmain: etcd Version: <span class="hljs-number">3</span>.<span class="hljs-number">4</span>.<span class="hljs-number">9</span><br><span class="hljs-attribute">2021</span>-<span class="hljs-number">02</span>-<span class="hljs-number">15</span> <span class="hljs-number">02</span>:<span class="hljs-number">26</span>:<span class="hljs-number">58</span>.<span class="hljs-number">806882</span> I | etcdserver: recovered store from snapshot at index <span class="hljs-number">2100090</span><br><span class="hljs-attribute">2021</span>-<span class="hljs-number">02</span>-<span class="hljs-number">15</span> <span class="hljs-number">02</span>:<span class="hljs-number">26</span>:<span class="hljs-number">58</span>.<span class="hljs-number">808810</span> I | mvcc: restore compact to <span class="hljs-number">1000002</span><br><span class="hljs-attribute">2021</span>-<span class="hljs-number">02</span>-<span class="hljs-number">15</span> <span class="hljs-number">02</span>:<span class="hljs-number">27</span>:<span class="hljs-number">19</span>.<span class="hljs-number">120141</span> W | etcdserver: backend quota <span class="hljs-number">26442450944</span> exceeds maximum recommended quota <span class="hljs-number">8589934592</span><br><span class="hljs-attribute">2021</span>-<span class="hljs-number">02</span>-<span class="hljs-number">15</span> <span class="hljs-number">02</span>:<span class="hljs-number">27</span>:<span class="hljs-number">19</span>.<span class="hljs-number">297363</span> I | embed: ready to serve client requests<br><br></code></pre></td></tr></table></figure><p>通过对etcd启动流程增加耗时统计，我们可以发现核心瓶颈主要在于打开db文件和重建内存treeIndex模块。</p><p>这里介绍下etcd启动后，重建内存treeIndex的原理。</p><p>我们知道treeIndex模块维护了用户key与boltdb key的映射关系，boltdb的key、value又包含了构建treeIndex的所需的数据。因此etcd启动的时候，会启动不同角色的goroutine并发完成treeIndex构建。</p><p><strong>首先是主goroutine。</strong> 它的职责是遍历boltdb，获取所有key-value数据，并将其反序列化成etcd的mvccpb.KeyValue结构。核心原理是基于etcd存储在boltdb中的key数据有序性，按版本号从1开始批量遍历，每次查询10000条key-value记录，直到查询数据为空。</p><p><strong>其次是构建treeIndex索引的goroutine。</strong> 它从主goroutine获取mvccpb.KeyValue数据，基于key、版本号、是否带删除标识等信息，构建keyIndex对象，插入到treeIndex模块的B-tree中。</p><p>因可能存在多个goroutine并发操作treeIndex，treeIndex的Insert函数会加全局锁，如下所示。etcd启动时只有一个 <strong>构建treeIndex索引的goroutine</strong>，因此key多时，会比较慢。之前我尝试优化成多goroutine并发构建，但是效果不佳，大量耗时会消耗在此锁上。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">func (ti *treeIndex) <span class="hljs-constructor">Insert(<span class="hljs-params">ki</span> <span class="hljs-operator">*</span><span class="hljs-params">keyIndex</span>)</span> &#123;<br>   ti.<span class="hljs-constructor">Lock()</span><br>   defer ti.<span class="hljs-constructor">Unlock()</span><br>   ti.tree.<span class="hljs-constructor">ReplaceOrInsert(<span class="hljs-params">ki</span>)</span><br>&#125;<br><br></code></pre></td></tr></table></figure><h2 id="节点内存配置"><a href="#节点内存配置" class="headerlink" title="节点内存配置"></a>节点内存配置</h2><p>etcd进程重启完成后，在没任何读写QPS情况下，如下所示，会发现etcd所消耗的内存比db大小还大一点。这又是为什么呢？如果etcd db文件大小超过节点内存规格，会导致什么问题吗？</p><p><img src="https://static001.geekbang.org/resource/image/02/a1/027ef8e1759a2800f1a2c1c105d7d7a1.png?wh=1312*394" alt="img"></p><p>etcd在启动的时候，会通过boltdb的Open API获取数据库对象，而Open API它会通过mmap机制将db文件映射到内存中。</p><p>由于etcd调用boltdb Open API的时候，设置了mmap的MAP_POPULATE flag，它会告诉Linux内核预读文件，将db文件内容全部从磁盘加载到物理内存中。</p><p>因此在你节点内存充足的情况下，启动后你看到的etcd占用内存，一般是db文件大小与内存treeIndex之和。</p><p>在节点内存充足的情况下，启动后，client后续发起对etcd的读操作，可直接通过内存获取boltdb的key-value数据，不会产生任何磁盘IO，具备良好的读性能、稳定性。</p><p>而当你的db文件大小超过节点内存配置时，若你查询的key所相关的branch page、leaf page不在内存中，那就会触发主缺页中断，导致读延时抖动、QPS下降。</p><p>因此为了保证etcd集群性能的稳定性，建议etcd节点内存规格要大于你的etcd db文件大小。</p><h2 id="treeIndex"><a href="#treeIndex" class="headerlink" title="treeIndex"></a>treeIndex</h2><p>当我们往集群中写入了一百多万key时，此时你再读取一个key范围操作的延时会出现一定程度上升，这是为什么呢？我们该如何分析耗时是在哪一步导致的？</p><p>在etcd 3.4中提供了trace特性，它可帮助我们定位、分析请求耗时过长问题。不过你需要特别注意的是，此特性在etcd 3.4中，因为依赖zap logger，默认为关闭。你可以通过设置etcd启动参数中的–logger&#x3D;zap来开启。</p><p>开启之后，我们可以在etcd日志中找到类似如下的耗时记录。</p><figure class="highlight scilab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs scilab">&#123;<br><span class="hljs-string">&quot;msg&quot;</span>:<span class="hljs-string">&quot;trace[331581563] range&quot;</span>，<br><span class="hljs-string">&quot;detail&quot;</span>:<span class="hljs-string">&quot;&#123;range_begin:/vip/a; range_end:/vip/b; response_count:19304; response_revision:1005564; &#125;&quot;</span>，<br><span class="hljs-string">&quot;duration&quot;</span>:<span class="hljs-string">&quot;146.432768ms&quot;</span>，<br><span class="hljs-string">&quot;steps&quot;</span>:[<br><span class="hljs-string">&quot;trace[331581563] &#x27;</span>range keys from in-memory treeIndex<span class="hljs-string">&#x27;  (duration: 95.925033ms)&quot;</span>，<br><span class="hljs-string">&quot;trace[331581563] &#x27;</span>range keys from bolt db<span class="hljs-string">&#x27;  (duration: 47.932118ms)&quot;</span><br>]<br><br></code></pre></td></tr></table></figure><p>此日志记录了查询请求”etcdctl get –prefix &#x2F;vip&#x2F;a”。它在treeIndex中查询相关key耗时95ms，从boltdb遍历key时47ms。主要原因还是此查询涉及的key数较多，高达一万九。</p><p>也就是说若treeIndex中存储了百万级的key时，它可能也会产生几十毫秒到数百毫秒的延时，对于期望业务延时稳定在较小阈值内的业务，就无法满足其诉求。</p><h2 id="boltdb性能"><a href="#boltdb性能" class="headerlink" title="boltdb性能"></a>boltdb性能</h2><p>当db文件大小持续增长到16G乃至更大后，从etcd事务提交监控metrics你可能会观察到，boltdb在提交事务时偶尔出现了较高延时，那么延时是怎么产生的呢？</p><p>在 介绍boltdb的原理时，关于db文件的磁盘布局，它是由meta page、branch page、leaf page、free list、free页组成的。同时我给你介绍了boltdb事务提交的四个核心流程，分别是B+ tree的重平衡、分裂，持久化dirty page，持久化freelist以及持久化meta data。</p><p>事务提交延时抖动的原因主要是在B+ tree树的重平衡和分裂过程中，它需要从freelist中申请若干连续的page存储数据，或释放空闲的page到freelist。</p><p>freelist后端实现在boltdb中是array。当申请一个连续的n个page存储数据时，它会遍历boltdb中所有的空闲页，直到找到连续的n个page。因此它的时间复杂度是O(N)。若db文件较大，又存在大量的碎片空闲页，很可能导致超时。</p><p>同时事务提交过程中，也可能会释放若干个page给freelist，因此需要合并到freelist的数组中，此操作时间复杂度是O(NLog N)。</p><p>假设我们db大小16G，page size 4KB，则有400万个page。经过各种修改、压缩后，若存在一半零散分布的碎片空闲页，在最坏的场景下，etcd每次事务提交需要遍历200万个page才能找到连续的n个page，同时还需要持久化freelist到磁盘。</p><p>为了优化boltdb事务提交的性能，etcd社区在bbolt项目中，实现了基于hashmap来管理freelist。通过引入了如下的三个map数据结构（freemaps的key是连续的页数，value是以空闲页的起始页pgid集合，forwardmap和backmap用于释放的时候快速合并页），将申请和释放时间复杂度降低到了O(1)。</p><p>freelist后端实现可以通过bbolt的FreeListType参数来控制，支持array和hashmap。在etcd 3.4版本中目前还是array，未来的3.5版本将默认是hashmap。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs go">freemaps       <span class="hljs-keyword">map</span>[<span class="hljs-type">uint64</span>]pidSet           <span class="hljs-comment">// key is the size of continuous pages(span)，value is a set which contains the starting pgids of same size</span><br>forwardMap     <span class="hljs-keyword">map</span>[pgid]<span class="hljs-type">uint64</span>             <span class="hljs-comment">// key is start pgid，value is its span size</span><br>backwardMap    <span class="hljs-keyword">map</span>[pgid]<span class="hljs-type">uint64</span>             <span class="hljs-comment">// key is end pgid，value is its span size</span><br><br></code></pre></td></tr></table></figure><p>另外在db中若存在大量空闲页，持久化freelist需要消耗较多的db大小，并会导致额外的事务提交延时。</p><p>若未持久化freelist，bbolt支持通过重启时扫描全部page来构造freelist，降低了db大小和提升写事务提交的性能（但是它会带来etcd启动延时的上升）。此行为可以通过bbolt的NoFreelistSync参数来控制，默认是true启用此特性。</p><h2 id="集群稳定性"><a href="#集群稳定性" class="headerlink" title="集群稳定性"></a>集群稳定性</h2><p>db文件增大后，另外一个非常大的隐患是用户client发起的expensive request，容易导致集群出现各种稳定性问题。</p><p>本质原因是etcd不支持数据分片，各个节点保存了所有key-value数据，同时它们又存储在boltdb的一个bucket里面。当你的集群含有百万级以上key的时候，任意一种expensive read请求都可能导致etcd出现OOM、丢包等情况发生。</p><p>那么有哪些expensive read请求会导致etcd不稳定性呢？</p><p><strong>首先是简单的count only查询。</strong> 如下图所示，当你想通过API统计一个集群有多少key时，如果你的key较多，则有可能导致内存突增和较大的延时。</p><p><img src="https://static001.geekbang.org/resource/image/44/a1/44ee247e9a31a455aca28459e5bb45a1.png?wh=1322*418" alt="img"></p><p>在etcd 3.5版本之前，统计key数会遍历treeIndex，把key追加到数组中。然而当数据规模较大时，追加key到数组中的操作会消耗大量内存，同时数组扩容时涉及到大量数据拷贝，会导致延时上升。</p><p><strong>其次是limit查询。</strong> 当你只想查询若干条数据的时候，若你的key较多，也会导致类似count only查询的性能、稳定性问题。</p><p>原因是etcd 3.5版本之前遍历index B-tree时，并未将limit参数下推到索引层，导致了无用的资源和时间消耗。优化方案是将limit参数下推到了索引层，实现查询性能百倍提升。</p><p><strong>最后是大包查询。</strong> 当你未分页批量遍历key-value数据或单key-value数据较大的时候，随着请求QPS增大，etcd OOM、节点出现带宽瓶颈导致丢包的风险会越来越大。</p><p>问题主要由以下两点原因导致：</p><p>第一，etcd需要遍历treeIndex获取key列表。若你未分页，一次查询万级key，显然会消耗大量内存并且高延时。</p><p>第二，获取到key列表、版本号后，etcd需要遍历boltdb，将key-value保存到查询结果数据结构中。如下trace日志所示，一个请求可能在遍历boltdb时花费很长时间，同时可能会消耗几百M甚至数G的内存。随着请求QPS增大，极易出现OOM、丢包等。etcd这块未来的优化点是实现流式传输。</p><figure class="highlight scilab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs scilab">&#123;<br><span class="hljs-string">&quot;level&quot;</span>:<span class="hljs-string">&quot;info&quot;</span>,<br><span class="hljs-string">&quot;ts&quot;</span>:<span class="hljs-string">&quot;2021-02-15T03:44:52.209Z&quot;</span>,<br><span class="hljs-string">&quot;caller&quot;</span>:<span class="hljs-string">&quot;traceutil/trace.go:145&quot;</span>,<br><span class="hljs-string">&quot;msg&quot;</span>:<span class="hljs-string">&quot;trace[1908866301] range&quot;</span>,<br><span class="hljs-string">&quot;detail&quot;</span>:<span class="hljs-string">&quot;&#123;range_begin:; range_end:; response_count:1232274; response_revision:3128500; &#125;&quot;</span>,<br><span class="hljs-string">&quot;duration&quot;</span>:<span class="hljs-string">&quot;9.063748801s&quot;</span>,<br><span class="hljs-string">&quot;start&quot;</span>:<span class="hljs-string">&quot;2021-02-15T03:44:43.145Z&quot;</span>,<br><span class="hljs-string">&quot;end&quot;</span>:<span class="hljs-string">&quot;2021-02-15T03:44:52.209Z&quot;</span>,<br><span class="hljs-string">&quot;steps&quot;</span>:[<br><span class="hljs-string">&quot;trace[1908866301] &#x27;</span>range keys from in-memory index tree<span class="hljs-string">&#x27; (duration: 693.262565ms)&quot;</span>,<br><span class="hljs-string">&quot;trace[1908866301] &#x27;</span>range keys from bolt db<span class="hljs-string">&#x27; (duration: 8.22558566s)&quot;</span>,<br><span class="hljs-string">&quot;trace[1908866301] &#x27;</span>assemble the response<span class="hljs-string">&#x27; (duration: 18.810315ms)&quot;</span><br>]<br>&#125;<br><br></code></pre></td></tr></table></figure><h2 id="快照"><a href="#快照" class="headerlink" title="快照"></a>快照</h2><p>大db文件最后一个影响面是快照。它会影响db备份文件生成速度、Leader发送快照给Follower节点的资源开销、Follower节点通过快照重建恢复的速度。</p><p>etcd提供了快照功能，帮助我们通过API即可备份etcd数据。当etcd收到snapshot请求的时候，它会通过boltdb接口创建一个只读事务Tx，随后通过事务的WriteTo接口，将meta page和data page拷贝到buffer即可。</p><p>但是随着db文件增大，快照事务执行的时间也会越来越长，而长事务则会导致db文件大小发生显著增加。</p><p>也就是说当db大时，生成快照不仅慢，生成快照时可能还会触发db文件大小持续增长，最终达到配额限制。</p><p>为什么长事务可能会导致db大小增长呢？</p><p>快照的另一大作用是当Follower节点异常的时候，Leader生成快照发送给Follower节点，Follower使用快照重建并追赶上Leader。此过程涉及到一定的CPU、内存、网络带宽等资源开销。</p><p>同时，若快照和集群写QPS较大，Leader发送快照给Follower和Follower应用快照到状态机的流程会耗费较长的时间，这可能会导致基于快照重建后的Follower依然无法通过正常的日志复制模式来追赶Leader，只能继续触发Leader生成快照，进而进入死循环，Follower一直处于异常中。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>首先大db文件首先会影响etcd启动耗时，因为etcd需要打开db文件，初始化db对象，并遍历boltdb中的所有key-value以重建内存treeIndex。</p><p>其次，较大db文件会导致etcd依赖更高配置的节点内存规格，etcd通过mmap将db文件映射到内存中。etcd启动后，正常情况下读etcd过程不涉及磁盘IO，若节点内存不够，可能会导致缺页中断，引起延时抖动、服务性能下降。</p><p>接着treeIndex维护了所有key的版本号信息，当treeIndex中含有百万级key时，在treeIndex中搜索指定范围的key的开销是不能忽略的，此开销可能高达上百毫秒。</p><p>然后当db文件过大后，boltdb本身连续空闲页的申请、释放、存储都会存在一定的开销。etcd社区已通过新的freelist管理数据结构hashmap对其进行优化，将时间复杂度降低到了O(1)，同时支持事务提交时不持久化freelist，而是通过重启时扫描page重建，以提升etcd写性能、降低db大小。</p><p>随后介绍了db文件过大后，count only、limit、大包查询等expensive request对集群稳定性的影响。建议你的业务尽量避免任何expensive request请求。</p><p>最后介绍了大db文件对快照功能的影响。大db文件意味着更长的备份时间，而更长的只读事务则可能会导致db文件增长。同时Leader发送快照与Follower基于快照重建都需要较长时间，在集群写请求较大的情况下，可能会陷入死循环，导致落后的Follower节点一直无法追赶上Leader。</p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>12.一致性：为什么基于Raft实现的etcd还会出现数据不一致？</title>
    <link href="/2022/10/07/12-%E4%B8%80%E8%87%B4%E6%80%A7%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E5%9F%BA%E4%BA%8ERaft%E5%AE%9E%E7%8E%B0%E7%9A%84etcd%E8%BF%98%E4%BC%9A%E5%87%BA%E7%8E%B0%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%80%E8%87%B4%EF%BC%9F/"/>
    <url>/2022/10/07/12-%E4%B8%80%E8%87%B4%E6%80%A7%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E5%9F%BA%E4%BA%8ERaft%E5%AE%9E%E7%8E%B0%E7%9A%84etcd%E8%BF%98%E4%BC%9A%E5%87%BA%E7%8E%B0%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%80%E8%87%B4%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="12-一致性：为什么基于Raft实现的etcd还会出现数据不一致？"><a href="#12-一致性：为什么基于Raft实现的etcd还会出现数据不一致？" class="headerlink" title="12.一致性：为什么基于Raft实现的etcd还会出现数据不一致？"></a>12.一致性：为什么基于Raft实现的etcd还会出现数据不一致？</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><p>etcd是基于Raft实现的高可用、强一致分布式存储。</p><p>问题现象：用户在更新Kubernetes集群中的Deployment资源镜像后，无法创建出新Pod，Deployment控制器莫名其妙不工作了。更令人细思极恐的是，部分Node莫名其妙消失了。</p><p>随便找了一个etcd节点查看存储数据，发现Node节点却在。这究竟是怎么一回事呢？ </p><h2 id="从消失的Node说起"><a href="#从消失的Node说起" class="headerlink" title="从消失的Node说起"></a>从消失的Node说起</h2><p>有人反馈Kubernetes集群出现了Deployment滚动更新异常、节点莫名其妙消失了等诡异现象。基于这个现象开始定位之旅。</p><p>首先查看了下Kubernetes集群APIServer、Controller Manager、Scheduler等组件状态，发现都是正常。</p><p>然后查看etcd集群各节点状态，也都是健康的，看了一个etcd节点数据也是正常，于是开始怀疑是不是APIServer出现了什么诡异的Bug了。</p><p>尝试重启APIServer，可Node依旧消失。百思不得其解的同时，只能去确认各个etcd节点上数据是否存在，结果却有了颠覆你固定思维的发现，那就是基于Raft实现的强一致存储竟然出现不一致、数据丢失。除了第一个节点含有数据，另外两个节点竟然找不到。那么问题就来了，另外两个节点数据是如何丢失的呢？</p><h2 id="一步步解密真相"><a href="#一步步解密真相" class="headerlink" title="一步步解密真相"></a>一步步解密真相</h2><p>在进一步深入分析前，结合etcd写流程原理的介绍（如下图），先大胆猜测下可能的原因。</p><p><img src="https://static001.geekbang.org/resource/image/8b/72/8b6dfa84bf8291369ea1803387906c72.png?wh=1920*1265" alt="img"></p><p>猜测1：etcd集群出现分裂，三个节点分裂成两个集群。APIServer配置的后端etcd server地址是三个节点，APIServer并不会检查各节点集群ID是否一致，因此如果分裂，有可能会出现数据“消失”现象。这种故障之前在Kubernetes社区的确也见到过相关issue，一般是变更异常导致的，显著特点是集群ID会不一致。</p><p>猜测2：Raft日志同步异常，其他两个节点会不会因为Raft模块存在特殊Bug导致未收取到相关日志条目呢？这种怀疑我们可以通过etcd自带的WAL工具来判断，它可以显示WAL日志中收到的命令（流程四、五、六）。</p><p>猜测3：如果日志同步没问题，那有没有可能是Apply模块出现了问题，导致日志条目未被应用到MVCC模块呢（流程七）？</p><p>猜测4：若Apply模块执行了相关日志条目到MVCC模块，MVCC模块的treeIndex子模块会不会出现了特殊Bug， 导致更新失败（流程八）？</p><p>猜测5：若MVCC模块的treeIndex模块无异常，写请求到了boltdb存储模块，有没有可能boltdb出现了极端异常导致丢数据呢（流程九）？</p><p>带着以上怀疑和推测，不断抽丝剥茧、去一步步探寻真相。</p><p>首先还是从故障定位第一工具“日志”开始。查看etcd节点日志没发现任何异常日志，但是当查看APIServer日志的时候，发现持续报”required revision has been compacted”，这个错误根据之前的压缩介绍，原因一般是APIServer请求etcd版本号被压缩了。</p><p>于是通过如下命令查看etcd节点详细的状态信息：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs lua">etcdctl endpoint <span class="hljs-built_in">status</span> <span class="hljs-comment">--cluster -w json | python -m</span><br>json.tool<br><br></code></pre></td></tr></table></figure><p>获得以下结果：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs prolog">[<br>    &#123;<br>        <span class="hljs-string">&quot;Endpoint&quot;</span>:<span class="hljs-string">&quot;A&quot;</span>，<br>        <span class="hljs-string">&quot;Status&quot;</span>:&#123;<br>            <span class="hljs-string">&quot;header&quot;</span>:&#123;<br>                <span class="hljs-string">&quot;cluster_id&quot;</span>:<span class="hljs-number">17237436991929493444</span>，<br>                <span class="hljs-string">&quot;member_id&quot;</span>:<span class="hljs-number">9372538179322589801</span>，<br>                <span class="hljs-string">&quot;raft_term&quot;</span>:<span class="hljs-number">10</span>，<br>                <span class="hljs-string">&quot;revision&quot;</span>:<span class="hljs-number">1052950</span><br>            &#125;，<br>            <span class="hljs-string">&quot;leader&quot;</span>:<span class="hljs-number">9372538179322589801</span>，<br>            <span class="hljs-string">&quot;raftAppliedIndex&quot;</span>:<span class="hljs-number">1098420</span>，<br>            <span class="hljs-string">&quot;raftIndex&quot;</span>:<span class="hljs-number">1098430</span>，<br>            <span class="hljs-string">&quot;raftTerm&quot;</span>:<span class="hljs-number">10</span>，<br>            <span class="hljs-string">&quot;version&quot;</span>:<span class="hljs-string">&quot;3.3.17&quot;</span><br>        &#125;<br>    &#125;，<br>    &#123;<br>        <span class="hljs-string">&quot;Endpoint&quot;</span>:<span class="hljs-string">&quot;B&quot;</span>，<br>        <span class="hljs-string">&quot;Status&quot;</span>:&#123;<br>            <span class="hljs-string">&quot;header&quot;</span>:&#123;<br>                <span class="hljs-string">&quot;cluster_id&quot;</span>:<span class="hljs-number">17237436991929493444</span>，<br>                <span class="hljs-string">&quot;member_id&quot;</span>:<span class="hljs-number">10501334649042878790</span>，<br>                <span class="hljs-string">&quot;raft_term&quot;</span>:<span class="hljs-number">10</span>，<br>                <span class="hljs-string">&quot;revision&quot;</span>:<span class="hljs-number">1025860</span><br>            &#125;，<br>            <span class="hljs-string">&quot;leader&quot;</span>:<span class="hljs-number">9372538179322589801</span>，<br>            <span class="hljs-string">&quot;raftAppliedIndex&quot;</span>:<span class="hljs-number">1098418</span>，<br>            <span class="hljs-string">&quot;raftIndex&quot;</span>:<span class="hljs-number">1098428</span>，<br>            <span class="hljs-string">&quot;raftTerm&quot;</span>:<span class="hljs-number">10</span>，<br>            <span class="hljs-string">&quot;version&quot;</span>:<span class="hljs-string">&quot;3.3.17&quot;</span><br>        &#125;<br>    &#125;，<br>    &#123;<br>        <span class="hljs-string">&quot;Endpoint&quot;</span>:<span class="hljs-string">&quot;C&quot;</span>，<br>        <span class="hljs-string">&quot;Status&quot;</span>:&#123;<br>            <span class="hljs-string">&quot;header&quot;</span>:&#123;<br>                <span class="hljs-string">&quot;cluster_id&quot;</span>:<span class="hljs-number">17237436991929493444</span>，<br>                <span class="hljs-string">&quot;member_id&quot;</span>:<span class="hljs-number">18249187646912138824</span>，<br>                <span class="hljs-string">&quot;raft_term&quot;</span>:<span class="hljs-number">10</span>，<br>                <span class="hljs-string">&quot;revision&quot;</span>:<span class="hljs-number">1028860</span><br>            &#125;，<br>            <span class="hljs-string">&quot;leader&quot;</span>:<span class="hljs-number">9372538179322589801</span>，<br>            <span class="hljs-string">&quot;raftAppliedIndex&quot;</span>:<span class="hljs-number">1098408</span>，<br>            <span class="hljs-string">&quot;raftIndex&quot;</span>:<span class="hljs-number">1098428</span>，<br>            <span class="hljs-string">&quot;raftTerm&quot;</span>:<span class="hljs-number">10</span>，<br>            <span class="hljs-string">&quot;version&quot;</span>:<span class="hljs-string">&quot;3.3.17&quot;</span><br>        &#125;<br>    &#125;<br>]<br><br></code></pre></td></tr></table></figure><p>从结果看，我们获得了如下信息：</p><p>第一，集群未分裂，3个节点A、B、C cluster_id都一致，集群分裂的猜测被排除。</p><p>第二，初步判断集群Raft日志条目同步正常，raftIndex表示Raft日志索引号，raftAppliedIndex表示当前状态机应用的日志索引号。这两个核心字段显示三个节点相差很小，考虑到正在写入，未偏离正常范围，Raft同步Bug导致数据丢失也大概率可以排除（不过最好还是用WAL工具验证下现在日志条目同步和写入WAL是否正常）。</p><p>第三，观察三个节点的revision值，相互之间最大差距接近30000，明显偏离标准值。关于revision的含义，它是etcd逻辑时钟，每次写入，就会全局递增。为什么三个节点之间差异如此之大呢？</p><p>接下来我们就一步步验证猜测、解密真相，猜测1集群分裂说被排除后，猜测2Raft日志同步异常也初步被我们排除了，那如何真正确认Raft日志同步正常呢？</p><p>你可以使用下面这个方法验证Raft日志条目同步是否正常。</p><p>首先我们写入一个值，比如put hello为world，然后马上在各个节点上用WAL工具etcd-dump-logs搜索hello。如下所示，各个节点上都可找到我们刚刚写入的命令。</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs gradle">$ etcdctl put hello world<br>OK<br>$ .<span class="hljs-regexp">/bin/</span>tools<span class="hljs-regexp">/etcd-dump-logs ./</span>Node1.etcd/ | <span class="hljs-keyword">grep</span> hello<br><span class="hljs-number">10</span>         <span class="hljs-number">70</span> norm   header:&lt;ID:<span class="hljs-number">3632562852862290438</span> &gt; put:&lt;key:<span class="hljs-string">&quot;hello&quot;</span> value:<span class="hljs-string">&quot;world&quot;</span> &gt;<br>$ .<span class="hljs-regexp">/bin/</span>tools<span class="hljs-regexp">/etcd-dump-logs ./</span>Node2.etcd/ | <span class="hljs-keyword">grep</span> hello<br><span class="hljs-number">10</span>         <span class="hljs-number">70</span> norm   header:&lt;ID:<span class="hljs-number">3632562852862290438</span> &gt; put:&lt;key:<span class="hljs-string">&quot;hello&quot;</span> value:<span class="hljs-string">&quot;world&quot;</span> &gt;<br>$ .<span class="hljs-regexp">/bin/</span>tools<span class="hljs-regexp">/etcd-dump-logs ./</span>Node3.etcd/ | <span class="hljs-keyword">grep</span> hello<br><span class="hljs-number">10</span>         <span class="hljs-number">70</span> norm   header:&lt;ID:<span class="hljs-number">3632562852862290438</span> &gt; put:&lt;key:<span class="hljs-string">&quot;hello&quot;</span> value:<span class="hljs-string">&quot;world&quot;</span> &gt;<br><br></code></pre></td></tr></table></figure><p>Raft日志同步异常猜测被排除后，我们再看下会不会是Apply模块出现了问题。但是<strong>raftAppliedIndex</strong>却显示三个节点几乎无差异，那我们能不能通过这个指标来判断Apply流程是否正常呢？</p><p>源码面前了无秘密，etcd更新raftAppliedIndex核心代码如下所示，你会发现这个指标其实并不靠谱。Apply流程出现逻辑错误时，并没重试机制。etcd无论Apply流程是成功还是失败，都会更新raftAppliedIndex值。也就是一个请求在Apply或MVCC模块即便执行失败了，都依然会更新raftAppliedIndex。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// ApplyEntryNormal apples an EntryNormal type Raftpb request to the EtcdServer</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> （<span class="hljs-title">s</span> *<span class="hljs-title">EtcdServer</span>） <span class="hljs-title">ApplyEntryNormal</span>（<span class="hljs-title">e</span> *<span class="hljs-title">Raftpb</span>.<span class="hljs-title">Entry</span>）</span> &#123;<br>   shouldApplyV3 := <span class="hljs-literal">false</span><br>   <span class="hljs-keyword">if</span> e.Index &gt; s.consistIndex.ConsistentIndex（） &#123;<br>      <span class="hljs-comment">// set the consistent index of current executing entry</span><br>      s.consistIndex.setConsistentIndex（e.Index）<br>      shouldApplyV3 = <span class="hljs-literal">true</span><br>   &#125;<br>   <span class="hljs-keyword">defer</span> s.setAppliedIndex（e.Index）<br>   ....<br> &#125;<br><br></code></pre></td></tr></table></figure><p>而三个节点revision差异偏离标准值，恰好又说明异常etcd节点可能未成功应用日志条目到MVCC模块。我们也可以通过查看MVCC的相关metrics（比如etcd_mvcc_put_total），来排除请求是否到了MVCC模块，事实是丢数据节点的metrics指标值的确远远落后正常节点。</p><p>于是将真凶锁定在Apply流程上。对Apply流程在未向MVCC模块提交请求前可能提前返回的地方，都加了日志。</p><p>同时查看Apply流程还发现，Apply失败的时候并不会打印任何日志。这也解释了为什么出现了数据不一致严重错误，但三个etcd节点却并没有任何异常日志。为了方便定位问题，我们因此增加了Apply错误日志。</p><p>同时测试发现，写入是否成功还跟client连接的节点有关，连接不同节点会出现不同的写入结果，用debug版本替换后，马上就输出了一条错误日志auth: revision in header is old。</p><p>原来数据不一致是因为鉴权版本号不一致导致的，节点在Apply流程的时候，会判断Raft日志条目中的请求鉴权版本号是否小于当前鉴权版本号，如果小于就拒绝写入。</p><p>那为什么各个节点的鉴权版本号会出现不一致呢？那就需要从可能修改鉴权版本号的源头分析。我们发现只有鉴权相关接口才会修改它，同时各个节点鉴权版本号之间差异已经固定不再增加，要成功解决就得再次复现。</p><p>然后还了解到，当时etcd进程有过重启，我们怀疑会不会重启触发了什么Bug，手动尝试复现一直失败。随后基于混沌工程，不断模拟真实业务场景、访问鉴权接口、注入故障（停止etcd进程等），最终功夫不负有心人，实现复现成功。</p><p>真相终于浮出水面，原来当你无意间重启etcd的时候，如果最后一条命令是鉴权相关的，它并不会持久化consistent index（KV接口会持久化）。consistent index、它具有幂等作用，可防止命令重复执行。consistent index的未持久化最终导致鉴权命令重复执行。</p><p>恰好鉴权模块的RoleGrantPermission接口未实现幂等，重复执行会修改鉴权版本号。一连串的Bug最终导致鉴权号出现不一致，随后又放大成MVCC模块的key-value数据不一致，导致严重的数据毁坏。</p><p>这个Bug影响etcd v3所有版本长达3年之久。查清楚问题后，我们也给社区提交了解决方案，合并到master后，同时cherry-pick到etcd 3.3和3.4稳定版本中。etcd v3.3.21和v3.4.8后的版本已经修复此Bug。</p><h2 id="为什么会不一致"><a href="#为什么会不一致" class="headerlink" title="为什么会不一致"></a>为什么会不一致</h2><p>详细了解完这个案例的不一致后，再从本质上深入分析下为什么会出现不一致，以及还有哪些场景会导致类似问题呢？</p><p>首先我们知道，etcd各个节点数据一致性基于Raft算法的日志复制实现的，etcd是个基于复制状态机实现的分布式系统。下图是分布式复制状态机原理架构，核心由3个组件组成，一致性模块、日志、状态机，其工作流程如下：</p><ul><li>client发起一个写请求（set x &#x3D; 3）；</li><li>server向一致性模块（假设是Raft）提交请求，一致性模块生成一个写提案日志条目。若server是Leader，把日志条目广播给其他节点，并持久化日志条目到WAL中；</li><li>当一半以上节点持久化日志条目后，Leader的一致性模块将此日志条目标记为已提交（committed），并通知其他节点提交；</li><li>server从一致性模块获取已经提交的日志条目，异步应用到状态机持久化存储中（boltdb等），然后返回给client。</li></ul><p><img src="https://static001.geekbang.org/resource/image/5c/4f/5c7a3079032f90120a6b309ee401fc4f.png?wh=605*319" alt="img"></p><p>从图中我们可以了解到，在基于复制状态机实现的分布式存储系统中，Raft等一致性算法它只能确保各个节点的日志一致性，也就是图中的流程二。</p><p>而对于流程三来说，server从日志里面获取已提交的日志条目，将其应用到状态机的过程，跟Raft算法本身无关，属于server本身的数据存储逻辑。</p><p><strong>也就是说有可能存在server应用日志条目到状态机失败，进而导致各个节点出现数据不一致。但是这个不一致并非Raft模块导致的，它已超过Raft模块的功能界限。</strong></p><p>比如在上面Node莫名其妙消失的案例中，就是应用日志条目到状态机流程中，出现逻辑错误，导致key-value数据未能持久化存储到boltdb。</p><p>这种逻辑错误即便重试也无法解决，目前社区也没有彻底的根治方案，只能根据具体案例进行针对性的修复。同时社区增加了Apply日志条目失败的警告日志。</p><h2 id="其他典型不一致Bug"><a href="#其他典型不一致Bug" class="headerlink" title="其他典型不一致Bug"></a>其他典型不一致Bug</h2><p>还有哪些场景可能还会导致Apply流程失败呢？之前升级etcd 3.2集群到3.3集群时，遇到的数据不一致的故障事件为例。</p><p>这个故障对外的表现也是令人摸不着头脑，有服务不调度的、有service下的endpoint不更新的。最终经过一番排查发现，原来数据不一致是由于etcd 3.2和3.3版本Lease模块的Revoke Lease行为不一致造成。</p><p>etcd 3.2版本的RevokeLease接口不需要鉴权，而etcd 3.3 RevokeLease接口增加了鉴权，因此当你升级etcd集群的时候，如果etcd 3.3版本收到了来自3.2版本的RevokeLease接口，就会导致因为没权限出现Apply失败，进而导致数据不一致，引发各种诡异现象。</p><p>除了重启etcd、升级etcd可能会导致数据不一致，defrag操作也可能会导致不一致。</p><p>对一个defrag碎片整理来说，它是如何触发数据不一致的呢？ 触发的条件是defrag未正常结束时会生成db.tmp临时文件。这个文件可能包含部分上一次defrag写入的部分key&#x2F;value数据，。而etcd下次defrag时并不会清理它，复用后就可能会出现各种异常场景，如重启后key增多、删除的用户数据key再次出现、删除user&#x2F;role再次出现等。</p><p>etcd 3.2.29、etcd 3.3.19、etcd 3.4.4后的版本都已经修复这个Bug。我建议你根据自己实际情况进行升级，否则踩坑后，数据不一致的修复工作是非常棘手的，风险度极高。</p><p>从以上三个案例里，我们可以看到， <strong>算法一致性不代表一个庞大的分布式系统工程实现中一定能保障一致性，工程实现上充满着各种挑战，从不可靠的网络环境到时钟、再到人为错误、各模块间的复杂交互等，几乎没有一个存储系统能保证任意分支逻辑能被测试用例100%覆盖。</strong></p><p>复制状态机在给我们带来数据同步的便利基础上，也给我们上层逻辑开发提出了高要求。也就是说任何接口逻辑变更etcd需要保证兼容性，否则就很容易出现Apply流程失败，导致数据不一致。</p><p>同时除了Apply流程可能导致数据不一致外，我们从defrag案例中也看到了一些维护变更操作，直接针对底层存储模块boltdb的，也可能会触发Bug，导致数据不一致。</p><h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><p>在了解了etcd数据不一致的风险和原因后，在实践中有哪些方法可以提前发现和规避不一致问题呢？</p><p>分别是：</p><ul><li>开启etcd的数据毁坏检测功能；</li><li>应用层的数据一致性检测；</li><li>定时数据备份；</li><li>良好的运维规范（比如使用较新稳定版本、确保版本一致性、灰度变更）。</li></ul><h3 id="开启etcd的数据毁坏检测功能"><a href="#开启etcd的数据毁坏检测功能" class="headerlink" title="开启etcd的数据毁坏检测功能"></a>开启etcd的数据毁坏检测功能</h3><p>首先介绍下etcd的数据毁坏检测功能。etcd不仅支持在启动的时候，通过–experimental-initial-corrupt-check参数检查各个节点数据是否一致，也支持在运行过程通过指定–experimental-corrupt-check-time参数每隔一定时间检查数据一致性。</p><p>那么它的一致性检测原理是怎样的？如果出现不一致性，etcd会采取什么样动作去降低数据不一致影响面呢？</p><p>其实就是想确定boltdb文件里面的内容跟其他节点内容是否一致。因此可以枚举所有key value，然后比较即可。</p><p>etcd的实现也就是通过遍历treeIndex模块中的所有key获取到版本号，然后再根据版本号从boltdb里面获取key的value，使用crc32 hash算法，将bucket name、key、value组合起来计算它的hash值。</p><p>如果开启了–experimental-initial-corrupt-check，启动的时候每个节点都会去获取peer节点的boltdb hash值，然后相互对比，如果不相等就会无法启动。</p><p>而定时检测是指Leader节点获取它当前最新的版本号，并通过Raft模块的ReadIndex机制确认Leader身份。当确认完成后，获取各个节点的revision和boltdb hash值，若出现Follower节点的revision大于Leader等异常情况时，就可以认为不一致，发送corrupt告警，触发集群corruption保护，拒绝读写。</p><p>从etcd上面的一致性检测方案我们可以了解到，目前采用的方案是比较简单、暴力的。因此可能随着数据规模增大，出现检测耗时增大等扩展性问题。而DynamoDB等使用了merkle tree来实现增量hash检测，这也是etcd未来可能优化的一个方向。</p><p>最后你需要特别注意的是，etcd数据毁坏检测的功能目前还是一个试验(experimental)特性，在比较新的版本才趋于稳定、成熟（推荐v3.4.9以上），预计在未来的etcd 3.5版本中才会变成稳定特性，因此etcd 3.2&#x2F;3.3系列版本就不能使用此方案。</p><h3 id="应用层的数据一致性检测"><a href="#应用层的数据一致性检测" class="headerlink" title="应用层的数据一致性检测"></a>应用层的数据一致性检测</h3><p>那要如何给etcd 3.2&#x2F;3.3版本增加一致性检测呢? 其实除了etcd自带数据毁坏检测，还可以通过在应用层通过一系列方法来检测数据一致性，它们适用于etcd所有版本。</p><p>接下来讲讲应用层检测的原理。</p><p>从上面对数据不一致性案例的分析中，我们知道数据不一致在MVCC、boltdb会出现很多种情况，比如说key数量不一致、etcd逻辑时钟版本号不一致、MVCC模块收到的put操作metrics指标值不一致等等。因此我们的应用层检测方法就是基于它们的差异进行巡检。</p><p>首先针对key数量不一致的情况，我们可以实现巡检功能，定时去统计各个节点的key数，这样可以快速地发现数据不一致，从而及时介入，控制数据不一致影响，降低风险。</p><p>在统计节点key数时，记得查询的时候带上WithCountOnly参数。etcd从treeIndex模块获取到key数后就及时返回了，无需访问boltdb模块。如果你的数据量非常大（涉及到百万级别），那即便是从treeIndex模块返回也会有一定的内存开销，因为它会把key追加到一个数组里面返回。</p><p>而在WithCountOnly场景中，我们只需要统计key数即可。因此我给社区提了优化方案，目前已经合并到master分支。对百万级别的key来说，WithCountOnly时内存开销从数G到几乎零开销，性能也提升数十倍。</p><p>其次可以基于endpoint各个节点的revision信息做一致性监控。一般情况下，各个节点的差异是极小的。</p><p>最后还可以基于etcd MVCC的metrics指标来监控。比如上面提到的mvcc_put_total，理论上每个节点这些MVCC指标是一致的，不会出现偏离太多。</p><h3 id="定时数据备份"><a href="#定时数据备份" class="headerlink" title="定时数据备份"></a>定时数据备份</h3><p>etcd数据不一致的修复工作极其棘手。发生数据不一致后，各个节点可能都包含部分最新数据和脏数据。如果最终我们无法修复，那就只能使用备份数据来恢复了。</p><p>因此备份特别重要，备份可以保障我们在极端场景下，能有保底的机制去恢复业务。 <strong>请记住，在做任何重要变更前一定先备份数据，以及在生产环境中建议增加定期的数据备份机制（比如每隔30分钟备份一次数据）。</strong></p><p>可以使用开源的etcd-operator中的backup-operator去实现定时数据备份，它可以将etcd快照保存在各个公有云的对象存储服务里面。</p><h3 id="良好的运维规范"><a href="#良好的运维规范" class="headerlink" title="良好的运维规范"></a>良好的运维规范</h3><p>最后介绍几个运维规范，这些规范可以帮助我们尽量少踩坑（即便你踩坑后也可以控制故障影响面）。</p><p>首先是确保集群中各节点etcd版本一致。若各个节点的版本不一致，因各版本逻辑存在差异性，这就会增大触发不一致Bug的概率。比如我们前面提到的升级版本触发的不一致Bug就属于此类问题。</p><p>其次是优先使用较新稳定版本的etcd。像上面我们提到的3个不一致Bug，在最新的etcd版本中都得到了修复。你可以根据自己情况进行升级，以避免下次踩坑。同时你可根据实际业务场景以及安全风险，来评估是否有必要开启鉴权，开启鉴权后涉及的逻辑更复杂，有可能增大触发数据不一致Bug的概率。</p><p>最后是你在升级etcd版本的时候，需要多查看change log，评估是否存在可能有不兼容的特性。在你升级集群的时候注意先在测试环境多验证，生产环境务必先灰度、再全量。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>从消失的Node案例为例，介绍了etcd中定位一个复杂不一致问题的思路和方法工具。核心就是根据我们对etcd读写原理的了解，对每个模块可能出现的问题进行大胆猜想。</p><p>同时我们要善于借助日志、metrics、etcd tool等进行验证排除。定位到最终模块问题后，如果很难复现，可以借助混沌工程等技术注入模拟各类故障。 <strong>遇到复杂Bug时，请永远不要轻言放弃，它一定是一个让你快速成长的机会。</strong></p><p>其次我介绍了etcd数据不一致的核心原因：Raft算法只能保证各个节点日志同步的一致性，但Apply流程是异步的，它从一致性模块获取日志命令，应用到状态机的准确性取决于业务逻辑，这块是没有机制保证的。</p><p>同时，defrag等运维管理操作，会直接修改底层存储数据，异常场景处理不严谨也会导致数据不一致。</p><p>数据不一致的风险是非常大的，轻则业务逻辑异常，重则核心数据丢失。我们需要机制去提前发现和规避它，因此最后我详细给你总结了etcd本身和应用层的一致性监控、定时备份数据、良好的运维规范等若干最佳实践，这些都是宝贵的实践总结，希望你能有所收获。</p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>11.压缩：如何回收旧版本数据？</title>
    <link href="/2022/10/06/11-%E5%8E%8B%E7%BC%A9%EF%BC%9A%E5%A6%82%E4%BD%95%E5%9B%9E%E6%94%B6%E6%97%A7%E7%89%88%E6%9C%AC%E6%95%B0%E6%8D%AE%EF%BC%9F/"/>
    <url>/2022/10/06/11-%E5%8E%8B%E7%BC%A9%EF%BC%9A%E5%A6%82%E4%BD%95%E5%9B%9E%E6%94%B6%E6%97%A7%E7%89%88%E6%9C%AC%E6%95%B0%E6%8D%AE%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="11-压缩：如何回收旧版本数据？"><a href="#11-压缩：如何回收旧版本数据？" class="headerlink" title="11.压缩：如何回收旧版本数据？"></a>11.压缩：如何回收旧版本数据？</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><p>在之前的介绍中，我们知道etcd中的每一次更新、删除key操作，treeIndex的keyIndex索引中都会追加一个版本号，在boltdb中会生成一个新版本boltdb key和value。也就是随着不停更新、删除，etcd进程内存占用和db文件就会越来越大。很显然，这会导致etcd OOM和db大小增长到最大db配额，最终不可写。</p><p>那么etcd是通过什么机制来回收历史版本数据，控制索引内存占用和db大小的呢？</p><p>这就是今天要分享的etcd压缩机制。能帮助你理解etcd压缩原理，在使用etcd过程中能根据自己的业务场景，选择适合的压缩策略，避免db大小增长失控而不可写入，帮助你构建稳定的etcd服务。</p><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p><img src="https://static001.geekbang.org/resource/image/7c/21/7c5d5212fa14yy6aaf843ae3dfc5f721.png?wh=1920*918" alt="img"></p><p>在了解etcd压缩模块实现细节前，先了解压缩模块的整体架构图。从图中可知，可以通过client API发起人工的压缩(Compact)操作，也可以配置<strong>自动压缩策略</strong>。在自动压缩策略中，可以根据业务场景选择合适的压缩模式。目前etcd支持两种压缩模式，分别是<strong>时间周期性压缩和版本号压缩</strong>。</p><p>当你通过API发起一个Compact请求后，KV Server收到Compact请求提交到Raft模块处理，在Raft模块中提交后，<strong>Apply模块就会通过MVCC模块的Compact接口执行此压缩任务</strong>。</p><p>Compact接口首先会更新当前server已压缩的版本号，并将耗时昂贵的压缩任务保存到FIFO队列中异步执行。压缩任务执行时，它首先会压缩treeIndex模块中的keyIndex索引，其次会遍历boltdb中的key，删除已废弃的key。</p><p>以上就是压缩模块的一个工作流程。接下来首先介绍如何人工发起一个Compact操作，然后详细介绍周期性压缩模式、版本号压缩模式的工作原理，最后再给你介绍Compact操作核心的原理。</p><h2 id="压缩特性初体验"><a href="#压缩特性初体验" class="headerlink" title="压缩特性初体验"></a>压缩特性初体验</h2><p>在使用etcd过程中，当你遇到”etcdserver: mvcc: database space exceeded”错误时，若是你未开启压缩策略导致db大小达到配额，这时可以使用etcdctl compact命令，主动触发压缩操作，回收历史版本。</p><p>如下所示，可以先通过endpoint status命令获取etcd当前版本号，然后再通过etcdctl compact命令发起压缩操作即可。</p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs subunit"># 获取etcd当前版本号<br>$ rev=$(etcdctl endpoint status --write-out=&quot;json&quot; | egrep -o &#x27;&quot;revision&quot;:[0<span class="hljs-string">-9</span>]*&#x27; | egrep -o &#x27;[0<span class="hljs-string">-9</span>].*&#x27;)<br>$ echo $rev<br>9<br># 执行压缩操作，指定压缩的版本号为当前版本号<br>$ etcdctl compact $rev<br>Compacted revision 9<br># 压缩一个已经压缩的版本号<br>$ etcdctl compact $rev<br><span class="hljs-keyword">Error: </span>etcdserver: mvcc: required revision has been compacted<br># 压缩一个比当前最大版号大的版本号<br>$ etcdctl compact 12<br><span class="hljs-keyword">Error: </span>etcdserver: mvcc: required revision is a future revision<br><br></code></pre></td></tr></table></figure><p>请注意，如果你压缩命令传递的版本号小于等于当前etcd server记录的压缩版本号，etcd server会返回已压缩错误(“mvcc: required revision has been compacted”)给client。如果版本号大于当前etcd server最新的版本号，etcd server则返回一个未来的版本号错误给client(“mvcc: required revision is a future revision”)。</p><p>执行压缩命令的时候，不少初学者有一个常见的误区，就是担心压缩会不会把我最新版本数据给删除？</p><p>压缩的本质是 <strong>回收历史版本</strong>，目标对象仅是 <strong>历史版本</strong>，不包括一个key-value数据的最新版本，因此你可以放心执行压缩命令，不会删除你的最新版本数据。不过介绍Watch机制时提到，Watch特性中的历史版本数据同步，依赖于MVCC中是否还保存了相关数据，因此建议不要每次简单粗暴地回收所有历史版本。</p><p>在生产环境中，建议精细化的控制历史版本数，那如何实现精细化控制呢？</p><p>主要有两种方案，<strong>一种是使用etcd server的自带的自动压缩机制</strong>，根据你的业务场景，配置合适的压缩策略即可。</p><p>另外一种方案是如果你觉得etcd server的自带压缩机制无法满足你的诉求，想更精细化的控制etcd保留的历史版本记录，你就可以基于etcd的Compact API，<strong>在业务逻辑代码中、或定时任务中主动触发压缩操作</strong>。你需要确保发起Compact操作的程序高可用，压缩的频率、保留的历史版本在合理范围内，并最终能使etcd的db 大小保持平稳，否则会导致db大小不断增长，直至db配额满，无法写入。</p><p>在一般情况下，建议使用etcd自带的压缩机制。它支持两种模式，分别是按<strong>时间周期性压缩和保留版本号的压缩</strong>，配置相应策略后，etcd节点会自动化的发起Compact操作。</p><p>接下来详细介绍下etcd的周期性和保留版本号压缩模式。</p><h2 id="周期性压缩"><a href="#周期性压缩" class="headerlink" title="周期性压缩"></a>周期性压缩</h2><p>首先是周期性压缩模式，它适用于什么场景呢？</p><p>当希望etcd只保留最近一段时间写入的历史版本时，就可以选择配置etcd的压缩模式为periodic，保留时间为你自定义的1h等。</p><p>如何给etcd server配置压缩模式和保留时间呢?</p><p>如下所示，etcd server提供了配置压缩模式和保留时间的参数：</p><figure class="highlight nsis"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs nsis">--<span class="hljs-literal">auto</span>-compaction-retention <span class="hljs-string">&#x27;0&#x27;</span><br><span class="hljs-literal">Auto</span> compaction retention length. <span class="hljs-number">0</span> means disable <span class="hljs-literal">auto</span> Compaction.<br>--<span class="hljs-literal">auto</span>-compaction-mode <span class="hljs-string">&#x27;periodic&#x27;</span><br>Interpret <span class="hljs-string">&#x27;auto-Compaction-retention&#x27;</span> one of: periodic|revision.<br><br></code></pre></td></tr></table></figure><p>auto-compaction-mode为periodic时，它表示启用时间周期性压缩，auto-compaction-retention为保留的时间的周期，比如1h。</p><p>auto-compaction-mode为revision时，它表示启用版本号压缩模式，auto-compaction-retention为保留的历史版本号数，比如10000。</p><p>注意，etcd server的auto-compaction-retention为’0’时，将关闭自动压缩策略，</p><p>那么周期性压缩模式的原理是怎样的呢？ etcd是如何知道你配置的1h前的etcd server版本号呢？</p><p>其实非常简单，etcd server启动后，根据你的配置的模式periodic，会创建periodic Compactor，它会异步的获取、记录过去一段时间的版本号。periodic Compactor组件获取你设置的压缩间隔参数1h， 并将其划分成10个区间，也就是每个区间6分钟。每隔6分钟，它会通过etcd MVCC模块的接口获取当前的server版本号，追加到rev数组中。</p><p>因为只需要保留过去1个小时的历史版本，periodic Compactor组件会通过当前时间减去上一次成功执行Compact操作的时间，如果间隔大于一个小时，它会取出rev数组的首元素，通过etcd server的Compact接口，发起压缩操作。</p><p>需要注意的一点是，在etcd v3.3.3版本之前，不同的etcd版本对周期性压缩的行为是有一定差异的，具体的区别你可以参考下 <a href="https://github.com/etcd-io/etcd/blob/v3.4.9/Documentation/op-guide/maintenance.md">官方文档</a>。</p><h2 id="版本号压缩"><a href="#版本号压缩" class="headerlink" title="版本号压缩"></a>版本号压缩</h2><p>了解完周期性压缩模式，我们再看看版本号压缩模式，它又适用于什么场景呢？</p><p>当写请求比较多，可能产生比较多的历史版本导致db增长时，或者不确定配置periodic周期为多少才是最佳的时候，你可以通过设置压缩模式为<strong>revision</strong>，<strong>指定保留的历史版本号数</strong>。比如你希望etcd尽量只保存1万个历史版本，那么你可以指定compaction-mode为revision，auto-compaction-retention为10000。</p><p>它的实现原理又是怎样的呢?</p><p>也很简单，etcd启动后会根据你的压缩模式revision，创建revision Compactor。revision Compactor会根据你设置的保留版本号数，每隔5分钟定时获取当前server的最大版本号，减去你想保留的历史版本数，然后通过etcd server的Compact接口发起如下的压缩操作即可。</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs gauss"><span class="hljs-meta"># 获取当前版本号，减去保留的版本号数</span><br><span class="hljs-built_in">rev</span> := rc.rg.<span class="hljs-built_in">Rev</span>() - rc.retention<br><span class="hljs-meta"># 调用server的Compact接口压缩</span><br>_，err := rc.c.<span class="hljs-built_in">Compact</span>(rc.ctx，&amp;pb.CompactionRequest&#123;Revision: <span class="hljs-built_in">rev</span>&#125;)<br><br></code></pre></td></tr></table></figure><h2 id="压缩原理"><a href="#压缩原理" class="headerlink" title="压缩原理"></a>压缩原理</h2><p>介绍完两种自动化的压缩模式原理后，接下来就深入分析下压缩的本质。当etcd server收到Compact请求后，它是如何执行的呢？ 核心原理是什么？</p><p>如前面的整体架构图所述，Compact请求经过Raft日志同步给多数节点后，etcd会从Raft日志取出Compact请求，应用此请求到状态机执行。</p><p>执行流程如下图所示，MVCC模块的Compact接口首先会检查Compact请求的版本号rev是否已被压缩过，若是则返回ErrCompacted错误给client。其次会检查rev是否大于当前etcd server的最大版本号，若是则返回ErrFutureRev给client，这就是我们上面执行etcdctl compact命令所看到的那两个错误原理。</p><p>通过检查后，Compact接口会通过boltdb的API在meta bucket中更新当前已调度的压缩版本号(scheduledCompactedRev)号，然后将压缩任务追加到FIFO Scheduled中，异步调度执行。</p><p><img src="https://static001.geekbang.org/resource/image/9a/ff/9ac55d639f564b56324b96dc02f0c0ff.png?wh=1920*1332" alt="img"></p><p>为什么Compact接口需要持久化存储当前已调度的压缩版本号到boltdb中呢？</p><p>试想下如果不保存这个版本号，etcd在异步执行的Compact任务过程中crash了，那么异常节点重启后，各个节点数据就会不一致。</p><p>因此etcd通过持久化存储scheduledCompactedRev，节点crash重启后，会重新向FIFO Scheduled中添加压缩任务，已保证各个节点间的数据一致性。</p><p>异步的执行压缩任务会做哪些工作呢？</p><p>首先之前介绍的treeIndex索引模块，它是etcd支持保存历史版本的核心模块，每个key在treeIndex模块中都有一个keyIndex数据结构，记录其历史版本号信息。</p><p><img src="https://static001.geekbang.org/resource/image/4f/dc/4f9cb015a842da0d5bd556d6b45970dc.png?wh=1920*1124" alt="img"></p><p>如上图所示，因此异步压缩任务的第一项工作，就是 <strong>压缩treeIndex模块中的各key的历史版本</strong>、已删除的版本。为了避免压缩工作影响读写性能，首先会克隆一个B-tree，然后通过克隆后的B-tree遍历每一个keyIndex对象，压缩历史版本号、清理已删除的版本。</p><p>假设当前压缩的版本号是CompactedRev， 它会保留keyIndex中最大的版本号，移除小于等于CompactedRev的版本号，并通过一个map记录treeIndex中有效的版本号返回给boltdb模块使用。</p><p>为什么要保留最大版本号呢?</p><p>因为最大版本号是这个key的最新版本，移除了会导致key丢失。而Compact的目的是回收旧版本。当然如果keyIndex中的最大版本号被打了删除标记(tombstone)， 就会从treeIndex中删除这个keyIndex，否则会出现内存泄露。</p><p>Compact任务执行完索引压缩后，它通过遍历B-tree、keyIndex中的所有generation获得当前内存索引模块中有效的版本号，这些信息将帮助etcd清理boltdb中的废弃历史版本。</p><p><img src="https://static001.geekbang.org/resource/image/d6/70/d625753e5a7f0f7f37987764b9204270.png?wh=1920*1129" alt="img"></p><p>压缩任务的第二项工作就是 <strong>删除boltdb中废弃的历史版本数据</strong>。如上图所示，它通过etcd一个名为scheduleCompaction任务来完成。</p><p>scheduleCompaction任务会根据key区间，从0到CompactedRev遍历boltdb中的所有key，通过treeIndex模块返回的有效索引信息，判断这个key是否有效，无效则调用boltdb的delete接口将key-value数据删除。</p><p>在这过程中，scheduleCompaction任务还会更新当前etcd已经完成的压缩版本号(finishedCompactRev)，将其保存到boltdb的meta bucket中。</p><p>scheduleCompaction任务遍历、删除key的过程可能会对boltdb造成压力，为了不影响正常读写请求，它在执行过程中会通过参数控制每次遍历、删除的key数（默认为100，每批间隔10ms），分批完成boltdb key的删除操作。</p><h2 id="为什么压缩后db大小不减少呢"><a href="#为什么压缩后db大小不减少呢" class="headerlink" title="为什么压缩后db大小不减少呢?"></a>为什么压缩后db大小不减少呢?</h2><p>当你执行完压缩任务后，db大小减少了吗？ 事实是并没有减少。那为什么我们都通过boltdb API删除了key，db大小还不减少呢？</p><p>之前介绍boltdb实现时，提到过boltdb将db文件划分成若干个page页，page页又有四种类型，分别是meta page、branch page、leaf page以及freelist page。branch page保存B+ tree的非叶子节点key数据，leaf page保存bucket和key-value数据，freelist会记录哪些页是空闲的。</p><p><strong>当我们通过boltdb删除大量的key，在事务提交后B+ tree经过分裂、平衡，会释放出若干branch&#x2F;leaf page页面，然而boltdb并不会将其释放给磁盘，调整db大小操作是昂贵的，会对性能有较大的损害。</strong></p><p>boltdb是通过freelist page记录这些空闲页的分布位置，当收到新的写请求时，优先从空闲页数组中申请若干连续页使用，实现高性能的读写（而不是直接扩大db大小）。当连续空闲页申请无法得到满足的时候， boltdb才会通过增大db大小来补充空闲页。</p><p>一般情况下，压缩操作释放的空闲页就能满足后续新增写请求的空闲页需求，db大小会趋于整体稳定。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>etcd压缩操作可通过API人工触发，也可以配置压缩模式由etcd server自动触发。压缩模式支持按周期和版本两种。在周期模式中你可以实现保留最近一段时间的历史版本数，在版本模式中你可以实现保留期望的历史版本数。</p><p>压缩的核心工作原理分为两大任务，第一个任务是压缩treeIndex中的各key历史索引，清理已删除key，并将有效的版本号保存到map数据结构中。</p><p>第二个任务是删除boltdb中的无效key。基本原理是根据版本号遍历boltdb已压缩区间范围的key，通过treeIndex返回的有效索引map数据结构判断key是否有效，无效则通过boltdb API删除它。</p><p>最后在执行压缩的操作中，虽然我们删除了boltdb db的key-value数据，但是db大小并不会减少。db大小不变的原因是存放key-value数据的branch和leaf页，它们释放后变成了空闲页，并不会将空间释放给磁盘。</p><p>boltdb通过freelist page来管理一系列空闲页，后续新增的写请求优先从freelist中申请空闲页使用，以提高性能。在写请求速率稳定、新增key-value较少的情况下，压缩操作释放的空闲页就可以基本满足后续写请求对空闲页的需求，db大小就会处于一个基本稳定、健康的状态。</p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>10.boltdb：如何持久化存储你的key-value数据？</title>
    <link href="/2022/10/05/10-boltdb%EF%BC%9A%E5%A6%82%E4%BD%95%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8%E4%BD%A0%E7%9A%84key-value%E6%95%B0%E6%8D%AE%EF%BC%9F/"/>
    <url>/2022/10/05/10-boltdb%EF%BC%9A%E5%A6%82%E4%BD%95%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8%E4%BD%A0%E7%9A%84key-value%E6%95%B0%E6%8D%AE%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="10-boltdb：如何持久化存储你的key-value数据？"><a href="#10-boltdb：如何持久化存储你的key-value数据？" class="headerlink" title="10.boltdb：如何持久化存储你的key-value数据？"></a>10.boltdb：如何持久化存储你的key-value数据？</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><p>boltdb是如何组织你的key-value数据的呢？当你读写一个key时，boltdb是如何工作的？</p><p>通过一个写请求在boltdb中执行的简要流程，分析其背后的boltdb的磁盘文件布局，帮助了解page、node、bucket等核心数据结构的原理与作用，搞懂boltdb基于B+ tree、各类page实现查找、更新、事务提交的原理，让你明白etcd为什么适合读多写少的场景。</p><h2 id="boltdb磁盘布局"><a href="#boltdb磁盘布局" class="headerlink" title="boltdb磁盘布局"></a>boltdb磁盘布局</h2><p>在介绍一个put写请求在boltdb中执行原理前，先从整体上介绍下平时你所看到的etcd db文件的磁盘布局，让你了解下db文件的物理存储结构。</p><p>boltdb文件指的是etcd数据目录下的member&#x2F;snap&#x2F;db的文件， etcd的key-value、lease、meta、member、cluster、auth等所有数据存储在其中。etcd启动的时候，会通过mmap机制将db文件映射到内存，后续可从内存中快速读取文件中的数据。写请求通过fwrite和fdatasync来写入、持久化数据到磁盘。</p><p><img src="https://static001.geekbang.org/resource/image/a6/41/a6086a069a2cf52b38d60716780f2e41.png?wh=1920*1131" alt="img"></p><p>上图是我给你画的db文件磁盘布局，从图中的左边部分你可以看到，文件的内容由若干个page组成，一般情况下page size为4KB。</p><p>page按照功能可分为元数据页(meta page)、B+ tree索引节点页(branch page)、B+ tree 叶子节点页(leaf page)、空闲页管理页(freelist page)、空闲页(free page)。</p><p>文件最开头的两个page是固定的db元数据meta page，空闲页管理页记录了db中哪些页是空闲、可使用的。索引节点页保存了B+ tree的内部节点，如图中的右边部分所示，它们记录了key值，叶子节点页记录了B+ tree中的key-value和bucket数据。</p><p>boltdb逻辑上通过B+ tree来管理branch&#x2F;leaf page， 实现快速查找、写入key-value数据。</p><h2 id="boltdb-API"><a href="#boltdb-API" class="headerlink" title="boltdb API"></a>boltdb API</h2><p>了解完boltdb的磁盘布局后，那么如果要在etcd中执行一个put请求，boltdb中是如何执行的呢？ boltdb作为一个库，提供了什么API给client访问写入数据？</p><p>boltdb提供了非常简单的API给上层业务使用，当我们执行一个put hello为world命令时，boltdb实际写入的key是版本号，value为mvccpb.KeyValue结构体。</p><p>这里我们简化下，假设往key bucket写入一个key为r94，value为world的字符串，其核心代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// 打开boltdb文件，获取db对象</span><br>db,err := bolt.Open(<span class="hljs-string">&quot;db&quot;</span>， <span class="hljs-number">0600</span>， <span class="hljs-literal">nil</span>)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>   log.Fatal(err)<br>&#125;<br><span class="hljs-keyword">defer</span> db.Close()<br><span class="hljs-comment">// 参数true表示创建一个写事务，false读事务</span><br>tx,err := db.Begin(<span class="hljs-literal">true</span>)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>   <span class="hljs-keyword">return</span> err<br>&#125;<br><span class="hljs-keyword">defer</span> tx.Rollback()<br><span class="hljs-comment">// 使用事务对象创建key bucket</span><br>b,err := tx.CreatebucketIfNotExists([]<span class="hljs-type">byte</span>(<span class="hljs-string">&quot;key&quot;</span>))<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>   <span class="hljs-keyword">return</span> err<br>&#125;<br><span class="hljs-comment">// 使用bucket对象更新一个key</span><br><span class="hljs-keyword">if</span> err := b.Put([]<span class="hljs-type">byte</span>(<span class="hljs-string">&quot;r94&quot;</span>),[]<span class="hljs-type">byte</span>(<span class="hljs-string">&quot;world&quot;</span>)); err != <span class="hljs-literal">nil</span> &#123;<br>   <span class="hljs-keyword">return</span> err<br>&#125;<br><span class="hljs-comment">// 提交事务</span><br><span class="hljs-keyword">if</span> err := tx.Commit(); err != <span class="hljs-literal">nil</span> &#123;<br>   <span class="hljs-keyword">return</span> err<br>&#125;<br><br></code></pre></td></tr></table></figure><p>如上所示，通过boltdb的Open API，我们获取到boltdb的核心对象db实例后，然后通过db的Begin API开启写事务，获得写事务对象tx。</p><p>通过写事务对象tx， 你可以创建bucket。这里我们创建了一个名为key的bucket（如果不存在），并使用bucket API往其中更新了一个key为r94，value为world的数据。最后我们使用写事务的Commit接口提交整个事务，完成bucket创建和key-value数据写入。</p><p>看起来是不是非常简单，神秘的boltdb，并未有我们想象的那么难。然而其API简单的背后却是boltdb的一系列巧妙的设计和实现。</p><p>一个key-value数据如何知道该存储在db在哪个page？如何快速找到你的key-value数据？事务提交的原理又是怎样的呢？</p><p>接下来我就和你浅析boltdb背后的奥秘。</p><h2 id="核心数据结构介绍"><a href="#核心数据结构介绍" class="headerlink" title="核心数据结构介绍"></a>核心数据结构介绍</h2><p>上面我们介绍boltdb的磁盘布局时提到，boltdb整个文件由一个个page组成。最开头的两个page描述db元数据信息，而它正是在client调用boltdb Open API时被填充的。那么描述磁盘页面的page数据结构是怎样的呢？元数据页又含有哪些核心数据结构？</p><p>boltdb本身自带了一个工具bbolt，它可以按页打印出db文件的十六进制的内容，下面我们就使用此工具来揭开db文件的神秘面纱。</p><p>下图左边的十六进制是执行如下 <a href="https://github.com/etcd-io/bbolt/blob/master/cmd/bbolt/main.go">bbolt dump</a> 命令，所打印的boltdb第0页的数据，图的右边是对应的page磁盘页结构和meta page的数据结构。</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">$ .<span class="hljs-regexp">/bbolt dump ./i</span>nfra1.etcd<span class="hljs-regexp">/member/</span>snap/db <span class="hljs-number">0</span><br><br></code></pre></td></tr></table></figure><p><img src="https://static001.geekbang.org/resource/image/94/16/94a4b5yydab7yy9a3f340632274f9616.png?wh=1920*1242" alt="img"></p><p>一看上图中的十六进制数据，你可能很懵，没关系，在你了解page磁盘页结构、meta page数据结构后，你就能读懂其含义了。</p><h3 id="page磁盘页结构"><a href="#page磁盘页结构" class="headerlink" title="page磁盘页结构"></a>page磁盘页结构</h3><p>我们先了解下page磁盘页结构，如上图所示，它由页ID(id)、页类型(flags)、数量(count)、溢出页数量(overflow)、页面数据起始位置(ptr)字段组成。</p><p>页类型目前有如下四种：0x01表示branch page，0x02表示leaf page，0x04表示meta page，0x10表示freelist page。</p><p>数量字段仅在页类型为leaf和branch时生效，溢出页数量是指当前页面数据存放不下，需要向后再申请overflow个连续页面使用，页面数据起始位置指向page的载体数据，比如meta page、branch&#x2F;leaf等page的内容。</p><h3 id="meta-page数据结构"><a href="#meta-page数据结构" class="headerlink" title="meta page数据结构"></a>meta page数据结构</h3><p>第0、1页我们知道它是固定存储db元数据的页(meta page)，那么meta page它为了管理整个boltdb含有哪些信息呢？</p><p>如上图中的meta page数据结构所示，你可以看到它由boltdb的文件标识(magic)、版本号(version)、页大小(pagesize)、boltdb的根bucket信息(root bucket)、freelist页面ID(freelist)、总的页面数量(pgid)、上一次写事务ID(txid)、校验码(checksum)组成。</p><h3 id="meta-page十六进制分析"><a href="#meta-page十六进制分析" class="headerlink" title="meta page十六进制分析"></a>meta page十六进制分析</h3><p>了解完page磁盘页结构和meta page数据结构后，我再结合图左边的十六进数据和你简要分析下其含义。</p><p>上图中十六进制输出的是db文件的page 0页结构，左边第一列表示此行十六进制内容对应的文件起始地址，每行16个字节。</p><p>结合page磁盘页和meta page数据结构我们可知，第一行前8个字节描述pgid(忽略第一列)是0。接下来2个字节描述的页类型， 其值为0x04表示meta page， 说明此页的数据存储的是meta page内容，因此ptr开始的数据存储的是meta page内容。</p><p>正如你下图中所看到的，第二行首先含有一个4字节的magic number(0xED0CDAED)，通过它来识别当前文件是否boltdb，接下来是两个字节描述boltdb的版本号0x2， 然后是四个字节的page size大小，0x1000表示4096个字节，四个字节的flags为0。</p><p><img src="https://static001.geekbang.org/resource/image/09/c0/09d8a9174b4539718878fcfb9da84cc0.png?wh=411*161" alt="img"></p><p>第三行对应的就是meta page的root bucket结构（16个字节），它描述了boltdb的root bucket信息，比如一个db中有哪些bucket， bucket里面的数据存储在哪里。</p><p>第四行中前面的8个字节，0x3表示freelist页面ID，此页面记录了db当前哪些页面是空闲的。后面8个字节，0x6表示当前db总的页面数。</p><p>第五行前面的8个字节，0x1a表示上一次的写事务ID，后面的8个字节表示校验码，用于检测文件是否损坏。</p><p>了解完db元数据页面原理后，那么boltdb是如何根据元数据页面信息快速找到你的bucket和key-value数据呢？</p><p>这就涉及到了元数据页面中的root bucket，它是个至关重要的数据结构。下面我们看看它是如何管理一系列bucket、帮助我们查找、写入key-value数据到boltdb中。</p><h3 id="bucket数据结构"><a href="#bucket数据结构" class="headerlink" title="bucket数据结构"></a>bucket数据结构</h3><p>如下命令所示，你可以使用bbolt buckets命令，输出一个db文件的bucket列表。执行完此命令后，我们可以看到之前介绍过的auth&#x2F;lease&#x2F;meta等熟悉的bucket，它们都是etcd默认创建的。那么boltdb是如何存储、管理bucket的呢？</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs awk">$ .<span class="hljs-regexp">/bbolt buckets  ./i</span>nfra1.etcd<span class="hljs-regexp">/member/</span>snap/db<br>alarm<br>auth<br>authRoles<br>authUsers<br>cluster<br>key<br>lease<br>members<br>members_removed<br>meta<br><br></code></pre></td></tr></table></figure><p>在上面我们提到过meta page中的，有一个名为root、类型bucket的重要数据结构，如下所示，bucket由root和sequence两个字段组成，root表示该bucket根节点的page id。注意meta page中的bucket.root字段，存储的是db的root bucket页面信息，你所看到的key&#x2F;lease&#x2F;auth等bucket都是root bucket的子bucket。</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs gauss"><span class="hljs-built_in">type</span> bucket <span class="hljs-keyword">struct</span> &#123;<br>   root     pgid   <span class="hljs-comment">// page id of the bucket&#x27;s root-level page</span><br>   sequence uint64 <span class="hljs-comment">// monotonically incrementing, used by NextSequence()</span><br>&#125;<br><br></code></pre></td></tr></table></figure><p><img src="https://static001.geekbang.org/resource/image/14/9b/14f9c6f5061f44ea3c1d8de4f47a5b9b.png?wh=1920*719" alt="img"></p><p>上面meta page十六进制图中，第三行的16个字节就是描述的root bucket信息。root bucket指向的page id为4，page id为4的页面是什么类型呢？ 我们可以通过如下bbolt pages命令看看各个page类型和元素数量，从下图结果可知，4号页面为leaf page。</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs awk">$ .<span class="hljs-regexp">/bbolt pages  ./i</span>nfra1.etcd<span class="hljs-regexp">/member/</span>snap/db<br>ID       TYPE       ITEMS  OVRFLW<br>======== ========== ====== ======<br><span class="hljs-number">0</span>        meta       <span class="hljs-number">0</span><br><span class="hljs-number">1</span>        meta       <span class="hljs-number">0</span><br><span class="hljs-number">2</span>        free<br><span class="hljs-number">3</span>        freelist   <span class="hljs-number">2</span><br><span class="hljs-number">4</span>        leaf       <span class="hljs-number">10</span><br><span class="hljs-number">5</span>        free<br><br></code></pre></td></tr></table></figure><p>通过上面的分析可知，当bucket比较少时，我们子bucket数据可直接从meta page里指向的leaf page中找到。</p><h3 id="leaf-page"><a href="#leaf-page" class="headerlink" title="leaf page"></a>leaf page</h3><p>meta page的root bucket直接指向的是page id为4的leaf page， page flag为0x02， leaf page它的磁盘布局如下图所示，前半部分是leafPageElement数组，后半部分是key-value数组。</p><p><img src="https://static001.geekbang.org/resource/image/0e/e8/0e70f52dc9752e2yy19f74a044530ee8.png?wh=1920*1013" alt="img"></p><p>leafPageElement包含leaf page的类型flags， 通过它可以区分存储的是bucket名称还是key-value数据。</p><p>当flag为bucketLeafFlag(0x01)时，表示存储的是bucket数据，否则存储的是key-value数据，leafPageElement它还含有key-value的读取偏移量，key-value大小，根据偏移量和key-value大小，我们就可以方便地从leaf page中解析出所有key-value对。</p><p>当存储的是bucket数据的时候，key是bucket名称，value则是bucket结构信息。bucket结构信息含有root page信息，通过root page（基于B+ tree查找算法），你可以快速找到你存储在这个bucket下面的key-value数据所在页面。</p><p>从上面分析你可以看到，每个子bucket至少需要一个page来存储其下面的key-value数据，如果子bucket数据量很少，就会造成磁盘空间的浪费。实际上boltdb实现了inline bucket，在满足一些条件限制的情况下，可以将小的子bucket内嵌在它的父亲叶子节点上，友好的支持了大量小bucket。</p><p>为了方便大家快速理解核心原理，本节我们讨论的bucket是假设都是非inline bucket。</p><p>那么boltdb是如何管理大量bucket、key-value的呢？</p><h3 id="branch-page"><a href="#branch-page" class="headerlink" title="branch page"></a>branch page</h3><p>boltdb使用了B+ tree来高效管理所有子bucket和key-value数据，因此它可以支持大量的bucket和key-value，只不过B+ tree的根节点不再直接指向leaf page，而是branch page索引节点页。branch page flags为0x01。它的磁盘布局如下图所示，前半部分是branchPageElement数组，后半部分是key数组。</p><p><img src="https://static001.geekbang.org/resource/image/61/9d/61af0c7e7e5beb05be6130bda29da49d.png?wh=1920*951" alt="img"></p><p>branchPageElement包含key的读取偏移量、key大小、子节点的page id。根据偏移量和key大小，我们就可以方便地从branch page中解析出所有key，然后二分搜索匹配key，获取其子节点page id，递归搜索，直至从bucketLeafFlag类型的leaf page中找到目的bucket name。</p><p>注意，boltdb在内存中使用了一个名为node的数据结构，来保存page反序列化的结果。下面我给出了一个boltdb读取page到node的代码片段，你可以直观感受下。</p><figure class="highlight roboconf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs roboconf">func (n *node) read(p *page) &#123;<br>   <span class="hljs-attribute">n.pgid = p.id</span><br><span class="hljs-attribute">   n.isLeaf = ((p.flags &amp; leafPageFlag) != 0)</span><br><span class="hljs-attribute">   n.inodes = make(inodes, int(p.count))</span><br><span class="hljs-attribute"></span><br><span class="hljs-attribute">   for i</span> := 0; <span class="hljs-attribute">i &lt; int(p.count); i++ &#123;</span><br><span class="hljs-attribute">      inode</span> := &amp;n<span class="hljs-variable">.inodes</span>[i]<br>      if n<span class="hljs-variable">.isLeaf</span> &#123;<br>         elem := p<span class="hljs-variable">.leafPageElement</span>(uint16(i))<br>         inode<span class="hljs-variable">.flags</span> = elem<span class="hljs-variable">.flags</span><br>         inode<span class="hljs-variable">.key</span> = elem<span class="hljs-variable">.key</span>()<br>         inode<span class="hljs-variable">.value</span> = elem<span class="hljs-variable">.value</span>()<br>      &#125; else &#123;<br>         elem := p<span class="hljs-variable">.branchPageElement</span>(uint16(i))<br>         inode<span class="hljs-variable">.pgid</span> = elem<span class="hljs-variable">.pgid</span><br>         inode<span class="hljs-variable">.key</span> = elem<span class="hljs-variable">.key</span>()<br>      &#125;<br>   &#125;<br><br></code></pre></td></tr></table></figure><p>从上面分析过程中你会发现，boltdb存储bucket和key-value原理是类似的，将page划分成branch page、leaf page，通过B+ tree来管理实现。boltdb为了区分leaf page存储的数据类型是bucket还是key-value，增加了标识字段（leafPageElement.flags），因此key-value的数据存储过程我就不再重复分析了。</p><h3 id="freelist"><a href="#freelist" class="headerlink" title="freelist"></a>freelist</h3><p>介绍完bucket、key-value存储原理后，再看meta page中的另外一个核心字段freelist，它的作用是什么呢？</p><p>我们知道boltdb将db划分成若干个page，那么它是如何知道哪些page在使用中，哪些page未使用呢？</p><p>答案是boltdb通过meta page中的freelist来管理页面的分配，freelist page中记录了哪些页是空闲的。当你在boltdb中删除大量数据的时候，其对应的page就会被释放，页ID存储到freelist所指向的空闲页中。当你写入数据的时候，就可直接从空闲页中申请页面使用。</p><p>下面meta page十六进制图中，第四行的前8个字节就是描述的freelist信息，page id为3。我们可以通过bbolt page命令查看3号page内容，如下所示，它记录了2和5为空闲页，与上面通过bbolt pages命令所看到的信息一致。</p><p><img src="https://static001.geekbang.org/resource/image/4a/9a/4a4d05678cfb785618537d2f930e859a.png?wh=1910*708" alt="img"></p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs gradle">$ .<span class="hljs-regexp">/bbolt page  ./i</span>nfra1.etcd<span class="hljs-regexp">/member/</span>snap/db <span class="hljs-number">3</span><br>page ID:    <span class="hljs-number">3</span><br>page Type:  freelist<br>Total <span class="hljs-keyword">Size</span>: <span class="hljs-number">4096</span> bytes<br>Item <span class="hljs-keyword">Count</span>: <span class="hljs-number">2</span><br>Overflow: <span class="hljs-number">0</span><br><br><span class="hljs-number">2</span><br><span class="hljs-number">5</span><br><br></code></pre></td></tr></table></figure><p>下图是freelist page存储结构，pageflags为0x10，表示freelist类型的页，ptr指向空闲页id数组。注意在boltdb中支持通过多种数据结构（数组和hashmap）来管理free page，这里介绍的是数组。</p><p><img src="https://static001.geekbang.org/resource/image/57/bb/57c6dd899c4cb56198a6092855161ebb.png?wh=1920*1070" alt="img"></p><h2 id="Open原理"><a href="#Open原理" class="headerlink" title="Open原理"></a>Open原理</h2><p>了解完核心数据结构后，就很容易搞懂boltdb Open API的原理了。</p><p>首先它会打开db文件并对其增加文件锁，目的是防止其他进程也以读写模式打开它后，操作meta和free page，导致db文件损坏。</p><p>其次boltdb通过mmap机制将db文件映射到内存中，并读取两个meta page到db对象实例中，然后校验meta page的magic、version、checksum是否有效，若两个meta page都无效，那么db文件就出现了严重损坏，导致异常退出。</p><h2 id="Put原理"><a href="#Put原理" class="headerlink" title="Put原理"></a>Put原理</h2><p>那么成功获取db对象实例后，通过bucket API创建一个bucket、发起一个Put请求更新数据时，boltdb是如何工作的呢？</p><p>根据上面介绍的bucket的核心原理，它首先是根据meta page中记录root bucket的root page，按照B+ tree的查找算法，从root page递归搜索到对应的叶子节点page面，返回key名称、leaf类型。</p><p>如果leaf类型为bucketLeafFlag，且key相等，那么说明已经创建过，不允许bucket重复创建，结束请求。否则往B+ tree中添加一个flag为bucketLeafFlag的key，key名称为bucket name，value为bucket的结构。</p><p>创建完bucket后，你就可以通过bucket的Put API发起一个Put请求更新数据。它的核心原理跟bucket类似，根据子bucket的root page，从root page递归搜索此key到leaf page，如果没有找到，则在返回的位置处插入新key和value。</p><p>为了方便你理解B+ tree查找、插入一个key原理，我给你构造了的一个max degree为5的B+ tree，下图是key r94的查找流程图。</p><p>那么如何确定这个key的插入位置呢？</p><p>首先从boltdb的key bucket的root page里，二分查找大于等于r94的key所在page，最终找到key r9指向的page（流程1）。r9指向的page是个leaf page，B+ tree需要确保叶子节点key的有序性，因此同样二分查找其插入位置，将key r94插入到相关位置（流程二）。</p><p><img src="https://static001.geekbang.org/resource/image/e6/6e/e6d2c12de362b55c7c36c45e5b65706e.png?wh=1920*711" alt="img"></p><p>在核心数据结构介绍中，我和你提到boltdb在内存中通过node数据结构来存储page磁盘页内容，它记录了key-value数据、page id、parent及children的node、B+ tree是否需要进行重平衡和分裂操作等信息。</p><p>因此，当我们执行完一个put请求时，它只是将值更新到boltdb的内存node数据结构里，并未持久化到磁盘中。</p><h2 id="事务提交原理"><a href="#事务提交原理" class="headerlink" title="事务提交原理"></a>事务提交原理</h2><p>那么boltdb何时将数据持久化到db文件中呢？</p><p>当你的代码执行tx.Commit API时，它才会将我们上面保存到node内存数据结构中的数据，持久化到boltdb中。下图我给出了一个事务提交的流程图，接下来我就分别和你简要分析下各个核心步骤。</p><p><img src="https://static001.geekbang.org/resource/image/e9/6f/e93935835e792363ae2edc5290f2266f.png?wh=1536*1532" alt="img"></p><p>首先从上面put案例中我们可以看到，插入了一个新的元素在B+ tree的叶子节点，它可能已不满足B+ tree的特性，因此事务提交时，第一步首先要调整B+ tree，进行重平衡、分裂操作，使其满足B+ tree树的特性。上面案例里插入一个key r94后，经过重平衡、分裂操作后的B+ tree如下图所示。</p><p><img src="https://static001.geekbang.org/resource/image/d3/8c/d31f483a10abeff34a8fef37941ef28c.png?wh=1920*838" alt="img"></p><p>在重平衡、分裂过程中可能会申请、释放free page，freelist所管理的free page也发生了变化。因此事务提交的第二步，就是持久化freelist。</p><p>注意，在etcd v3.4.9中，为了优化写性能等，freelist持久化功能是关闭的。etcd启动获取boltdb db对象的时候，boltdb会遍历所有page，构建空闲页列表。</p><p>事务提交的第三步就是将client更新操作产生的dirty page通过fdatasync系统调用，持久化存储到磁盘中。</p><p>最后，在执行写事务过程中，meta page的txid、freelist等字段会发生变化，因此事务的最后一步就是持久化meta page。</p><p>通过以上四大步骤，就完成了事务提交的工作，成功将数据持久化到了磁盘文件中，安全地完成了一个put操作。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>首先通过一幅boltdb磁盘布局图和bbolt工具，解密了db文件的本质。db文件由meta page、freelist page、branch page、leaf page、free page组成。随后我结合bbolt工具，和你深入介绍了meta page、branch page、leaf page、freelist page的数据结构，帮助你了解key、value数据是如何存储到文件中的。</p><p>然后通过分析一个put请求在boltdb中如何执行的。从Open API获取db对象说起，介绍了其通过mmap将db文件映射到内存，构建meta page，校验meta page的有效性，再到创建bucket，通过bucket API往boltdb添加key-value数据。</p><p>添加bucket和key-value操作本质，是从B+ tree管理的page中找到插入的页和位置，并将数据更新到page的内存node数据结构中。</p><p>真正持久化数据到磁盘是通过事务提交执行的。它首先需要通过一系列重平衡、分裂操作，确保boltdb维护的B+ tree满足相关特性，其次需要持久化freelist page，并将用户更新操作产生的dirty page数据持久化到磁盘中，最后则是持久化meta page。</p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>09.事务：如何安全地实现多key操作？</title>
    <link href="/2022/10/05/09-%E4%BA%8B%E5%8A%A1%EF%BC%9A%E5%A6%82%E4%BD%95%E5%AE%89%E5%85%A8%E5%9C%B0%E5%AE%9E%E7%8E%B0%E5%A4%9Akey%E6%93%8D%E4%BD%9C%EF%BC%9F/"/>
    <url>/2022/10/05/09-%E4%BA%8B%E5%8A%A1%EF%BC%9A%E5%A6%82%E4%BD%95%E5%AE%89%E5%85%A8%E5%9C%B0%E5%AE%9E%E7%8E%B0%E5%A4%9Akey%E6%93%8D%E4%BD%9C%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="09-事务：如何安全地实现多key操作？"><a href="#09-事务：如何安全地实现多key操作？" class="headerlink" title="09.事务：如何安全地实现多key操作？"></a>09.事务：如何安全地实现多key操作？</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><p>在软件开发过程中，我们经常会遇到需要批量执行多个key操作的业务场景，比如转账案例中，Alice给Bob转账100元，Alice账号减少100，Bob账号增加100，这涉及到多个key的原子更新。</p><p>无论发生任何故障，我们应用层期望的结果是，要么两个操作一起成功，要么两个一起失败。我们无法容忍出现一个成功，一个失败的情况。那么etcd是如何解决多key原子更新问题呢？</p><p>这正是分享的主题——事务，它就是为了 <strong>简化应用层的编程模型</strong> 而诞生的，通过转账案例为你剖析etcd事务实现，让你了解etcd如何实现事务ACID特性的，以及MVCC版本号在事务中的重要作用。希望通过本节课，帮助你在业务开发中正确使用事务，保证软件代码的正确性。</p><h2 id="事务特性初体验及API"><a href="#事务特性初体验及API" class="headerlink" title="事务特性初体验及API"></a>事务特性初体验及API</h2><p>如何使用etcd实现Alice向Bob转账功能呢？</p><p>在etcd v2的时候， etcd提供了CAS（Compare and swap），然而其只支持单key，不支持多key，因此无法满足类似转账场景的需求。严格意义上说CAS称不上事务，无法实现事务的各个隔离级别。</p><p>etcd v3为了解决多key的原子操作问题，提供了全新迷你事务API，同时基于MVCC版本号，它可以实现各种隔离级别的事务。它的基本结构如下：</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">client.<span class="hljs-constructor">Txn(<span class="hljs-params">ctx</span>)</span>.<span class="hljs-constructor">If(<span class="hljs-params">cmp1</span>, <span class="hljs-params">cmp2</span>, <span class="hljs-operator">...</span>)</span>.<span class="hljs-constructor">Then(<span class="hljs-params">op1</span>, <span class="hljs-params">op2</span>, <span class="hljs-operator">...</span>,)</span>.<span class="hljs-constructor">Else(<span class="hljs-params">op1</span>, <span class="hljs-params">op2</span>, …)</span><br><br></code></pre></td></tr></table></figure><p>从上面结构中你可以看到， <strong>事务API由If语句、Then语句、Else语句组成</strong>，这与我们平时常见的MySQL事务完全不一样。</p><p>它的基本原理是，在If语句中，你可以添加一系列的条件表达式，若条件表达式全部通过检查，则执行Then语句的get&#x2F;put&#x2F;delete等操作，否则执行Else的get&#x2F;put&#x2F;delete等操作。</p><p>那么If语句支持哪些检查项呢？</p><p>首先是 <strong>key的最近一次修改版本号mod_revision</strong>，简称mod。可以通过它检查key最近一次被修改时的版本号是否符合你的预期。比如当你查询到Alice账号资金为100元时，它的mod_revision是v1，当你发起转账操作时，你得确保Alice账号上的100元未被挪用，这就可以通过mod(“Alice”) &#x3D; “v1” 条件表达式来保障转账安全性。</p><p>其次是 <strong>key的创建版本号create_revision</strong>，简称create。你可以通过它检查key是否已存在。比如在分布式锁场景里，只有分布式锁key(lock)不存在的时候，你才能发起put操作创建锁，这时你可以通过create(“lock”) &#x3D; “0”来判断，因为一个key不存在的话它的create_revision版本号就是0。</p><p>接着是 <strong>key的修改次数version</strong>。你可以通过它检查key的修改次数是否符合预期。比如你期望key在修改次数小于3时，才能发起某些操作时，可以通过version(“key”) &lt; “3”来判断。</p><p>最后是 <strong>key的value值</strong>。你可以通过检查key的value值是否符合预期，然后发起某些操作。比如期望Alice的账号资金为200, value(“Alice”) &#x3D; “200”。</p><p>If语句通过以上MVCC版本号、value值、各种比较运算符(等于、大于、小于、不等于)，实现了灵活的比较的功能，满足你各类业务场景诉求。</p><p>下面给出了一个使用etcdctl的txn事务命令，基于以上介绍的特性，初步实现的一个Alice向Bob转账100元的事务。</p><p>Alice和Bob初始账上资金分别都为200元，事务首先判断Alice账号资金是否为200，若是则执行转账操作，不是则返回最新资金。etcd是如何执行这个事务的呢？ <strong>这个事务实现上有哪些问题呢？</strong></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs subunit">$ etcdctl txn -i<br>compares: //对应If语句<br>value(&quot;Alice&quot;) = &quot;200&quot; //判断Alice账号资金是否为200<br><br><span class="hljs-keyword">success </span>requests (get, put, del): //对应Then语句<br>put Alice 100 //Alice账号初始资金200减100<br>put Bob 300 //Bob账号初始资金200加100<br><br><span class="hljs-keyword">failure </span>requests (get, put, del): //对应Else语句<br>get Alice<br>get Bob<br><br><span class="hljs-keyword">SUCCESS</span><br><span class="hljs-keyword"></span><br><span class="hljs-keyword"></span>OK<br><br>OK<br><br></code></pre></td></tr></table></figure><h2 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h2><p><img src="https://static001.geekbang.org/resource/image/e4/d3/e41a4f83bda29599efcf06f6012b0bd3.png?wh=1920*852" alt="img"></p><p>先介绍下事务的整体流程，为我们后面介绍etcd事务ACID特性的实现做准备。</p><p>上图是etcd事务的执行流程，当你通过client发起一个txn转账事务操作时，通过gRPC KV Server、Raft模块处理后，在Apply模块执行此事务的时候，它首先对你的事务的If语句进行检查，也就是ApplyCompares操作，如果通过此操作，则执行ApplyTxn&#x2F;Then语句，否则执行ApplyTxn&#x2F;Else语句。</p><p>在执行以上操作过程中，它会根据事务是否只读、可写，通过MVCC层的读写事务对象，执行事务中的get&#x2F;put&#x2F;delete各操作，也就是我们上一节课介绍的MVCC对key的读写原理。</p><h2 id="事务ACID特性"><a href="#事务ACID特性" class="headerlink" title="事务ACID特性"></a>事务ACID特性</h2><p>了解完事务的整体执行流程后，那么etcd应该如何正确实现上面案例中Alice向Bob转账的事务呢？别着急，我们先来了解一下事务的ACID特性。在你了解了etcd事务ACID特性实现后，这个转账事务案例的正确解决方案也就简单了。</p><p>ACID是衡量事务的四个特性，由原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）组成。接下来我就为你分析ACID特性在etcd中的实现。</p><h3 id="原子性与持久性"><a href="#原子性与持久性" class="headerlink" title="原子性与持久性"></a>原子性与持久性</h3><p>事务的原子性（Atomicity）是指在一个事务中，所有请求要么同时成功，要么同时失败。比如在我们的转账案例中，是绝对无法容忍Alice账号扣款成功，但是Bob账号资金到账失败的场景。</p><p>持久性（Durability）是指事务一旦提交，其所做的修改会永久保存在数据库。</p><p>软件系统在运行过程中会遇到各种各样的软硬件故障，如果etcd在执行上面事务过程中，刚执行完扣款命令（put Alice 100）就突然crash了，它是如何保证转账事务的原子性与持久性的呢？</p><p><img src="https://static001.geekbang.org/resource/image/cf/9e/cf94ce8fc0649fe5cce45f8b7468019e.png?wh=1920*949" alt="img"></p><p>如上图转账事务流程图所示，etcd在执行一个事务过程中，任何时间点都可能会出现节点crash等异常问题。我在图中给你标注了两个关键的异常时间点，它们分别是T1和T2。接下来我分别为你分析一下etcd在这两个关键时间点异常后，是如何保证事务的原子性和持久性的。</p><h4 id="T1时间点"><a href="#T1时间点" class="headerlink" title="T1时间点"></a>T1时间点</h4><p>T1时间点是在Alice账号扣款100元完成时，Bob账号资金还未成功增加时突然发生了crash。</p><p>从前面介绍的etcd写原理和上面流程图我们可知，此时MVCC写事务持有boltdb写锁，仅是将修改提交到了内存中，保证幂等性、防止日志条目重复执行的一致性索引consistent index也并未更新。同时，负责boltdb事务提交的goroutine因无法持有写锁，也并未将事务提交到持久化存储中。</p><p>因此，T1时间点发生crash异常后，事务并未成功执行和持久化任意数据到磁盘上。在节点重启时，etcd server会重放WAL中的已提交日志条目，再次执行以上转账事务。因此不会出现Alice扣款成功、Bob到帐失败等严重Bug，极大简化了业务的编程复杂度。</p><h4 id="T2时间点"><a href="#T2时间点" class="headerlink" title="T2时间点"></a>T2时间点</h4><p>T2时间点是在MVCC写事务完成转账，server返回给client转账成功后，boltdb的事务提交goroutine，批量将事务持久化到磁盘中时发生了crash。这时etcd又是如何保证原子性和持久性的呢?</p><p>我们知道一致性索引consistent index字段值是和key-value数据在一个boltdb事务里同时持久化到磁盘中的。若在boltdb事务提交过程中发生crash了，简单情况是consistent index和key-value数据都更新失败。那么当节点重启，etcd server重放WAL中已提交日志条目时，同样会再次应用转账事务到状态机中，因此事务的原子性和持久化依然能得到保证。</p><p>更复杂的情况是，当boltdb提交事务的时候，会不会部分数据提交成功，部分数据提交失败呢？这个问题，我将在下一节课通过深入介绍boltdb为你解答。</p><p>了解完etcd事务的原子性和持久性后，那一致性又是怎么一回事呢？事务的一致性难道是指各个节点数据一致性吗？</p><h3 id="一致性"><a href="#一致性" class="headerlink" title="一致性"></a>一致性</h3><p>在软件系统中，到处可见一致性（Consistency）的表述，其实在不同场景下，它的含义是不一样的。</p><p>首先分布式系统中多副本数据一致性，它是指各个副本之间的数据是否一致，比如Redis的主备是异步复制的，那么它的一致性是最终一致性的。</p><p>其次是CAP原理中的一致性是指可线性化。核心原理是虽然整个系统是由多副本组成，但是通过线性化能力支持，对client而言就如一个副本，应用程序无需关心系统有多少个副本。</p><p>然后是一致性哈希，它是一种分布式系统中的数据分片算法，具备良好的分散性、平衡性。</p><p>最后是事务中的一致性，它是指事务变更前后，数据库必须满足若干恒等条件的状态约束， <strong>一致性往往是由数据库和业务程序两方面来保障的</strong>。</p><p><strong>在Alice向Bob转账的案例中有哪些恒等状态呢？</strong></p><p>很明显，转账系统内的各账号资金总额，在转账前后应该一致，同时各账号资产不能小于0。</p><p>为了帮助你更好地理解前面转账事务实现的问题，下面画了幅两个并发转账事务的流程图。</p><p>图中有两个并发的转账事务，Mike向Bob转账100元，Alice也向Bob转账100元，按照我们上面的事务实现，从下图可知转账前系统总资金是600元，转账后却只有500元了，因此它无法保证转账前后账号系统内的资产一致性，导致了资产凭空消失，破坏了事务的一致性。</p><p><img src="https://static001.geekbang.org/resource/image/1f/ea/1ff951756c0ffc427e5a064e3cf8caea.png?wh=1920*1153" alt="img"></p><p>事务一致性被破坏的根本原因是，事务中缺少对Bob账号资产是否发生变化的判断，这就导致账号资金被覆盖。</p><p>为了确保事务的一致性，一方面，业务程序在转账逻辑里面，需检查转账者资产大于等于转账金额。在事务提交时，通过账号资产的版本号，确保双方账号资产未被其他事务修改。若双方账号资产被其他事务修改，账号资产版本号会检查失败，这时业务可以通过获取最新的资产和版本号，发起新的转账事务流程解决。</p><p>另一方面，etcd会通过WAL日志和consistent index、boltdb事务特性，去确保事务的原子性，因此不会有部分成功部分失败的操作，导致资金凭空消失、新增。</p><p>介绍完事务的原子性和持久化、一致性后，我们再看看etcd又是如何提供各种隔离级别的事务，在转账过程中，其他client能看到转账的中间状态吗(如Alice扣款成功，Bob还未增加时)？</p><h3 id="隔离性"><a href="#隔离性" class="headerlink" title="隔离性"></a>隔离性</h3><p>ACID中的I是指Isolation，也就是事务的隔离性，它是指事务在执行过程中的可见性。常见的事务隔离级别有以下四种。</p><p>首先是 <strong>未提交读</strong>（Read UnCommitted），也就是一个client能读取到未提交的事务。比如转账事务过程中，Alice账号资金扣除后，Bob账号上资金还未增加，这时如果其他client读取到这种中间状态，它会发现系统总金额钱减少了，破坏了事务一致性的约束。</p><p>其次是 <strong>已提交读</strong>（Read Committed），指的是只能读取到已经提交的事务数据，但是存在不可重复读的问题。比如事务开始时，你读取了Alice和Bob资金，这时其他事务修改Alice和Bob账号上的资金，你在事务中再次读取时会读取到最新资金，导致两次读取结果不一样。</p><p>接着是 <strong>可重复读</strong>（Repeated Read），它是指在一个事务中，同一个读操作get Alice&#x2F;Bob在事务的任意时刻都能得到同样的结果，其他修改事务提交后也不会影响你本事务所看到的结果。</p><p>最后是 <strong>串行化</strong>（Serializable），它是最高的事务隔离级别，读写相互阻塞，通过牺牲并发能力、串行化来解决事务并发更新过程中的隔离问题。对于串行化我要和你特别补充一点，很多人认为它都是通过读写锁，来实现事务一个个串行提交的，其实这只是在基于锁的并发控制数据库系统实现而已。 <strong>为了优化性能，在基于MVCC机制实现的各个数据库系统中，提供了一个名为“可串行化的快照隔离”级别，相比悲观锁而言，它是一种乐观并发控制，通过快照技术实现的类似串行化的效果，事务提交时能检查是否冲突。</strong></p><p>下面介绍下未提交读、已提交读、可重复读、串行化快照隔离。</p><h4 id="未提交读"><a href="#未提交读" class="headerlink" title="未提交读"></a>未提交读</h4><p>首先是最低的事务隔离级别，未提交读。我们通过如下一个转账事务时间序列图，来分析下一个client能否读取到未提交事务修改的数据，是否存在脏读。</p><p><img src="https://static001.geekbang.org/resource/image/6a/8d/6a526be4949a383fd5263484c706d68d.png?wh=1920*786" alt="img"></p><p>图中有两个事务，一个是用户查询Alice和Bob资产的事务，一个是执行Alice向Bob转账的事务。</p><p>如图中所示，若在Alice向Bob转账事务执行过程中，etcd server收到了client查询Alice和Bob资产的读请求，显然此时我们无法接受client能读取到一个未提交的事务，因为这对应用程序而言会产生严重的BUG。那么etcd是如何保证不出现这种场景呢？</p><p>我们知道etcd基于boltdb实现读写操作的，读请求由boltdb的读事务处理，你可以理解为快照读。写请求由boltdb写事务处理，etcd定时将一批写操作提交到boltdb并清空buffer。</p><p>由于etcd是批量提交写事务的，而读事务又是快照读，因此当MVCC写事务完成时，它需要更新buffer，这样下一个读请求到达时，才能从buffer中获取到最新数据。</p><p>在我们的场景中，转账事务并未结束，执行put Alice为100的操作不会回写buffer，因此避免了脏读的可能性。用户此刻从boltdb快照读事务中查询到的Alice和Bob资产都为200。</p><p>从以上分析可知，etcd并未使用悲观锁来解决脏读的问题，而是通过MVCC机制来实现读写不阻塞，并解决脏读的问题。</p><h4 id="已提交读、可重复读"><a href="#已提交读、可重复读" class="headerlink" title="已提交读、可重复读"></a>已提交读、可重复读</h4><p>比未提交读隔离级别更高的是已提交读，它是指在事务中能读取到已提交数据，但是存在不可重复读的问题。已提交读，也就是说你每次读操作，若未增加任何版本号限制，默认都是当前读，etcd会返回最新已提交的事务结果给你。</p><p>如何理解不可重复读呢?</p><p>在上面用户查询Alice和Bob事务的案例中，第一次查出来资产都是200，第二次是Alice为100，Bob为300，通过读已提交模式，你能及时获取到etcd最新已提交的事务结果，但是出现了不可重复读，两次读出来的Alice和Bob资产不一致。</p><p>那么如何实现可重复读呢？</p><p>你可以通过MVCC快照读，或者参考etcd的事务框架STM实现，它在事务中维护一个读缓存，优先从读缓存中查找，不存在则从etcd查询并更新到缓存中，这样事务中后续读请求都可从缓存中查找，确保了可重复读。</p><p>最后我们再来重点介绍下什么是串行化快照隔离。</p><h4 id="串行化快照隔离"><a href="#串行化快照隔离" class="headerlink" title="串行化快照隔离"></a>串行化快照隔离</h4><p>串行化快照隔离是最严格的事务隔离级别，它是指在在事务刚开始时，首先获取etcd当前的版本号rev，事务中后续发出的读请求都带上这个版本号rev，告诉etcd你需要获取那个时间点的快照数据，etcd的MVCC机制就能确保事务中能读取到同一时刻的数据。</p><p><strong>同时，它还要确保事务提交时，你读写的数据都是最新的，未被其他人修改，也就是要增加冲突检测机制。</strong> 当事务提交出现冲突的时候依赖client重试解决，安全地实现多key原子更新。</p><p>那么如何为上面一致性案例中，两个并发转账的事务，增加冲突检测机制呢？</p><p>核心就是前面介绍MVCC的版本号，通过下面的并发转账事务流程图解释它是如何工作的。</p><p><img src="https://static001.geekbang.org/resource/image/3b/26/3b4c7fb43e03a38aceb2a8c2d5c92226.png?wh=1920*1011" alt="img"></p><p>如上图所示，事务A，Alice向Bob转账100元，事务B，Mike向Bob转账100元，两个事务同时发起转账操作。</p><p>一开始时，Mike的版本号(指mod_revision)是4，Bob版本号是3，Alice版本号是2，资产各自200。为了防止并发写事务冲突，etcd在一个写事务开始时，会独占一个MVCC读写锁。</p><p>事务A会先去etcd查询当前Alice和Bob的资产版本号，用于在事务提交时做冲突检测。在事务A查询后，事务B获得MVCC写锁并完成转账事务，Mike和Bob账号资产分别为100，300，版本号都为5。</p><p>事务B完成后，事务A获得写锁，开始执行事务。</p><p>为了解决并发事务冲突问题，事务A中增加了冲突检测，期望的Alice版本号应为2，Bob为3。结果事务B的修改导致Bob版本号变成了5，因此此事务会执行失败分支，再次查询Alice和Bob版本号和资产，发起新的转账事务，成功通过MVCC冲突检测规则mod(“Alice”) &#x3D; 2 和 mod(“Bob”) &#x3D; 5 后，更新Alice账号资产为100，Bob资产为400，完成转账操作。</p><p>通过上面介绍的快照读和MVCC冲突检测检测机制，etcd就可实现串行化快照隔离能力。</p><h3 id="转账案例应用"><a href="#转账案例应用" class="headerlink" title="转账案例应用"></a>转账案例应用</h3><p>介绍完etcd事务ACID特性实现后，你很容易发现事务特性初体验中的案例问题了，它缺少了完整事务的冲突检测机制。</p><p>首先你可通过一个事务获取Alice和Bob账号的上资金和版本号，用以判断Alice是否有足够的金额转账给Bob和事务提交时做冲突检测。 你可通过如下etcdctl txn命令，获取Alice和Bob账号的资产和最后一次修改时的版本号(mod_revision):</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs maxima">$ etcdctl txn -i -w=json<br>compares:<br><br>success requests (<span class="hljs-built_in">get</span>, <span class="hljs-built_in">put</span>, <span class="hljs-built_in">del</span>):<br><span class="hljs-built_in">get</span> Alice<br><span class="hljs-built_in">get</span> Bob<br><br>failure requests (<span class="hljs-built_in">get</span>, <span class="hljs-built_in">put</span>, <span class="hljs-built_in">del</span>):<br><br>&#123;<br> <span class="hljs-string">&quot;kvs&quot;</span>:[<br>      &#123;<br>          <span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;QWxpY2U=&quot;</span>,<br>          <span class="hljs-string">&quot;create_revision&quot;</span>:<span class="hljs-number">2</span>,<br>          <span class="hljs-string">&quot;mod_revision&quot;</span>:<span class="hljs-number">2</span>,<br>          <span class="hljs-string">&quot;version&quot;</span>:<span class="hljs-number">1</span>,<br>          <span class="hljs-string">&quot;value&quot;</span>:<span class="hljs-string">&quot;MjAw&quot;</span><br>      &#125;<br>  ],<br>    ......<br>  <span class="hljs-string">&quot;kvs&quot;</span>:[<br>      &#123;<br>          <span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;Qm9i&quot;</span>,<br>          <span class="hljs-string">&quot;create_revision&quot;</span>:<span class="hljs-number">3</span>,<br>          <span class="hljs-string">&quot;mod_revision&quot;</span>:<span class="hljs-number">3</span>,<br>          <span class="hljs-string">&quot;version&quot;</span>:<span class="hljs-number">1</span>,<br>          <span class="hljs-string">&quot;value&quot;</span>:<span class="hljs-string">&quot;MzAw&quot;</span><br>      &#125;<br>  ],<br>&#125;<br><br></code></pre></td></tr></table></figure><p>其次发起资金转账操作，Alice账号减去100，Bob账号增加100。为了保证转账事务的准确性、一致性，提交事务的时候需检查Alice和Bob账号最新修改版本号与读取资金时的一致(compares操作中增加版本号检测)，以保证其他事务未修改两个账号的资金。</p><p>若compares操作通过检查，则执行转账操作，否则执行查询Alice和Bob账号资金操作，命令如下:</p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs subunit">$ etcdctl txn -i<br>compares:<br>mod(&quot;Alice&quot;) = &quot;2&quot;<br>mod(&quot;Bob&quot;) = &quot;3&quot;<br><br><span class="hljs-keyword">success </span>requests (get, put, del):<br>put Alice 100<br>put Bob 300<br><br><span class="hljs-keyword">failure </span>requests (get, put, del):<br>get Alice<br>get Bob<br><br><span class="hljs-keyword">SUCCESS</span><br><span class="hljs-keyword"></span><br><span class="hljs-keyword"></span>OK<br><br>OK<br><br></code></pre></td></tr></table></figure><p>到这里我们就完成了一个安全的转账事务操作，从以上流程中可以发现，自己从0到1实现一个完整的事务还是比较繁琐的，幸运的是，etcd社区基于以上介绍的事务特性，提供了一个简单的事务框架 <a href="https://github.com/etcd-io/etcd/blob/v3.4.9/clientv3/concurrency/stm.go">STM</a>，构建了各个事务隔离级别类，帮助你进一步简化应用编程复杂度。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>首先介绍了事务API的基本结构，它由If、Then、Else语句组成。</p><p>其中If支持多个比较规则，它是用于事务提交时的冲突检测，比较的对象支持key的 <strong>mod_revision</strong>、 <strong>create_revision、version、value值</strong>。随后我给你介绍了整个事务执行的基本流程，Apply模块首先执行If的比较规则，为真则执行Then语句，否则执行Else语句。</p><p>接着通过转账案例，四幅转账事务时间序列图，我为你分析了事务的ACID特性，剖析了在etcd中事务的ACID特性的实现。</p><ul><li><p>原子性是指一个事务要么全部成功要么全部失败，etcd基于WAL日志、consistent index、boltdb的事务能力提供支持。</p></li><li><p>一致性是指事务转账前后的，数据库和应用程序期望的恒等状态应该保持不变，这通过数据库和业务应用程序相互协作完成。</p></li><li><p>持久性是指事务提交后，数据不丢失，</p></li><li><p>隔离性是指事务提交过程中的可见性，etcd不存在脏读，基于MVCC机制、boltdb事务你可以实现可重复读、串行化快照隔离级别的事务，保障并发事务场景中你的数据安全性。</p></li></ul><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><p>在数据库事务中，有各种各样的概念，比如脏读、脏写、不可重复读与读倾斜、幻读与写倾斜、更新丢失、快照隔离、可串行化快照隔离? 你知道它们的含义吗？</p><ul><li>脏读、脏写： 在读未提交的隔离级别的情况下，事务A执行过程中，事务A对数据资源进行了修改，事务B读取了事务A修改后且未提交的数据。A因为某些原因回滚了操作，B却使用了A对资源修改后的数据，进行了读写等操作。</li><li>不可重复读： 在读未提交、读提交的隔离级别情况下，事务B读取了两次数据资源，在这两次读取的过程中事务A修改了数据，导致事务B在这两次读取出来的数据不一致。这种在同一个事务中，前后两次读取的数据不一致的现象就是不可重复读。 </li><li>幻读： 在可重复读得隔离级别情况下，事务B前后两次读取同一个范围的数据，在事务B两次读取的过程中事务A新增了数据，导致事务B后一次读取到前一次查询没有看到的行。 </li><li>读倾斜、写倾斜： 读写倾斜是在数据分表不合理的情况下，对某个表的数据存在大量的读取写入的需求，分表不均衡不合理导致的。 </li><li>更新丢失： 第一类丢失，在读未提交的隔离级别情况下，事务A和事务B都对数据进行更新，但是事务A由于某种原因事务回滚了，把已经提交的事务B的更新数据给覆盖了。这种现象就是第一类更新丢失。 第二类丢失，在可重复读的隔离级别情况下，跟第一类更新丢失有点类似，也是两个事务同时对数据进行更新，但是事务A的更新把已提交的事务B的更新数据给覆盖了。这种现象就是第二类更新丢失。</li><li>快照隔离： 每个事务都从一个数据库的快照中读数据，如果有些数据在当前事务开始之后，被其他事务改变了值，快照隔离能够保证当前事务无法看到这个新值。 可串行化快照隔离： 可串行化快照隔离是在快照隔离级别之上，支持串行化。</li></ul><h3 align=center>快速理解脏读、不可重复读、幻读？**【1】脏读（读取未提交数据）**      A事务读取B事务尚未提交的数据，此时如果B事务发生错误并执行回滚操作，那么A事务读取到的数据就是脏数据。就好像原本的数据比较干净、纯粹，此时由于B事务更改了它，这个数据变得不再纯粹。这个时候A事务立即读取了这个脏数据，但事务B良心发现，又用回滚把数据恢复成原来干净、纯粹的样子，而事务A却什么都不知道，最终结果就是事务A读取了此次的脏数据，称为脏读。<p><strong>这种情况常发生于转账与取款操作中:</strong></p><table><thead><tr><th>时间顺序</th><th>转账事务</th><th>取款事务</th></tr></thead><tbody><tr><td>1</td><td></td><td>开始事务</td></tr><tr><td>2</td><td>开始事务</td><td></td></tr><tr><td>3</td><td></td><td>查询账户余额为2000元</td></tr><tr><td>4</td><td></td><td>取款1000元，余额被更改为1000元</td></tr><tr><td>5</td><td>查询账户余额为1000元产生脏读）</td><td></td></tr><tr><td>6</td><td></td><td>取款操作发生未知错误，事务回滚，余额变更为2000元</td></tr><tr><td>7</td><td>转入2000元，余额被更改为3000元（脏读的1000+2000）</td><td></td></tr><tr><td>8</td><td>提交事务</td><td></td></tr><tr><td>备注</td><td><span style="color:#000000;">按照正确逻辑，此时账户余额应该为4000元</span></td><td></td></tr></tbody></table><p style="text-indent:33px;">事务A在执行读取操作，由整个事务A比较大，前后读取同一条数据需要经历很长的时间 。而在事务A第一次读取数据，比如此时读取了小明的年龄为20岁，事务B执行更改操作，将小明的年龄更改为30岁，此时事务A第二次读取到小明的年龄时，发现其年龄是30岁，和之前的数据不一样了，也就是数据不重复了，系统不可以读取到重复的数据，成为不可重复读。</p><div class="table-box"><table cellspacing="0" style="width:421.55pt;"><tbody><tr><td style="width:75.95pt;">            <p style="margin-left:0pt;">时间顺序</p>            </td>            <td style="width:175.8pt;">            <p style="margin-left:0pt;">事务A</p>            </td>            <td style="width:169.8pt;">            <p style="margin-left:0pt;">事务B</p>            </td>        </tr><tr><td style="width:75.95pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">1</span></p>            </td>            <td style="width:175.8pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">开始事务</span></p>            </td>            <td style="width:169.8pt;">            <p style="margin-left:0pt;">&nbsp;</p>            </td>        </tr><tr><td style="width:75.95pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">2</span></p>            </td>            <td style="width:175.8pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">第一次查询，小明的年龄为20岁</span></p>            </td>            <td style="width:169.8pt;">            <p style="margin-left:0pt;">&nbsp;</p>            </td>        </tr><tr><td style="width:75.95pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">3</span></p>            </td>            <td style="width:175.8pt;">            <p style="margin-left:0pt;">&nbsp;</p>            </td>            <td style="width:169.8pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">开始事务</span></p>            </td>        </tr><tr><td style="width:75.95pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">4</span></p>            </td>            <td style="width:175.8pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">其他操作</span></p>            </td>            <td style="width:169.8pt;">            <p style="margin-left:0pt;">&nbsp;</p>            </td>        </tr><tr><td style="width:75.95pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">5</span></p>            </td>            <td style="width:175.8pt;">            <p style="margin-left:0pt;">&nbsp;</p>            </td>            <td style="width:169.8pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">更改小明的年龄为30岁</span></p>            </td>        </tr><tr><td style="width:75.95pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">6</span></p>            </td>            <td style="width:175.8pt;">            <p style="margin-left:0pt;">&nbsp;</p>            </td>            <td style="width:169.8pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">提交事务</span></p>            </td>        </tr><tr><td style="width:75.95pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">7</span></p>            </td>            <td style="width:175.8pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">第二次查询，小明的年龄为30岁</span></p>            </td>            <td style="width:169.8pt;">            <p style="margin-left:0pt;">&nbsp;</p>            </td>        </tr><tr><td style="width:75.95pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">备注</span></p>            </td>            <td colspan="2" style="width:345.6pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">按照正确逻辑，事务A前后两次读取到的数据应该一致</span></p>            </td>        </tr></tbody></table></div><p><strong>【3】幻读（前后多次读取，数据总量不一致）</strong><br>事务A在执行读取操作，需要两次统计数据的总量，前一次查询数据总量后，此时事务B执行了新增数据的操作并提交后，这个时候事务A读取的数据总量和之前统计的不一样，就像产生了幻觉一样，平白无故的多了几条数据，成为幻读。</p><div class="table-box"><table cellspacing="0" style="width:421.55pt;"><tbody><tr><td style="width:75.95pt;">            <p style="margin-left:0pt;">时间顺序</p>            </td>            <td style="width:175.8pt;">            <p style="margin-left:0pt;">事务A</p>            </td>            <td style="width:169.8pt;">            <p style="margin-left:0pt;">事务B</p>            </td>        </tr><tr><td style="width:75.95pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">1</span></p>            </td>            <td style="width:175.8pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">开始事务</span></p>            </td>            <td style="width:169.8pt;">            <p style="margin-left:0pt;">&nbsp;</p>            </td>        </tr><tr><td style="width:75.95pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">2</span></p>            </td>            <td style="width:175.8pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">第一次查询，数据总量为100条</span></p>            </td>            <td style="width:169.8pt;">            <p style="margin-left:0pt;">&nbsp;</p>            </td>        </tr><tr><td style="width:75.95pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">3</span></p>            </td>            <td style="width:175.8pt;">            <p style="margin-left:0pt;">&nbsp;</p>            </td>            <td style="width:169.8pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">开始事务</span></p>            </td>        </tr><tr><td style="width:75.95pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">4</span></p>            </td>            <td style="width:175.8pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">其他操作</span></p>            </td>            <td style="width:169.8pt;">            <p style="margin-left:0pt;">&nbsp;</p>            </td>        </tr><tr><td style="width:75.95pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">5</span></p>            </td>            <td style="width:175.8pt;">            <p style="margin-left:0pt;">&nbsp;</p>            </td>            <td style="width:169.8pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">新增100条数据</span></p>            </td>        </tr><tr><td style="width:75.95pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">6</span></p>            </td>            <td style="width:175.8pt;">            <p style="margin-left:0pt;">&nbsp;</p>            </td>            <td style="width:169.8pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">提交事务</span></p>            </td>        </tr><tr><td style="width:75.95pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">7</span></p>            </td>            <td style="width:175.8pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">第二次查询，数据总量为200条</span></p>            </td>            <td style="width:169.8pt;">            <p style="margin-left:0pt;">&nbsp;</p>            </td>        </tr><tr><td style="width:75.95pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">备注</span></p>            </td>            <td colspan="2" style="width:345.6pt;">            <p style="margin-left:0pt;"><span style="color:#000000;">按照正确逻辑，事务A前后两次读取到的数据总量应该一致</span></p>            </td>        </tr></tbody></table></div><p>理解了脏读、不可重复读、幻读，那么接下来看看事务隔离级别是怎么个回事<a href="https://blog.csdn.net/qq_33591903/article/details/82079302?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522158632818519726869014448%2522%252C%2522scm%2522%253A%252220140713.130056874..%2522%257D&amp;request_id=158632818519726869014448&amp;biz_id=0&amp;utm_source=distribute.pc_search_result.none-task-blog-blog_SOOPENSEARCH-1">【数据库】事务隔离级别</a></p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>08.Watch：如何高效获取数据变化通知？</title>
    <link href="/2022/10/04/08-Watch%EF%BC%9A%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE%E5%8F%98%E5%8C%96%E9%80%9A%E7%9F%A5%EF%BC%9F/"/>
    <url>/2022/10/04/08-Watch%EF%BC%9A%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE%E5%8F%98%E5%8C%96%E9%80%9A%E7%9F%A5%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="08-Watch：如何高效获取数据变化通知？"><a href="#08-Watch：如何高效获取数据变化通知？" class="headerlink" title="08.Watch：如何高效获取数据变化通知？"></a>08.Watch：如何高效获取数据变化通知？</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><p>在Kubernetes中，各种各样的控制器实现了Deployment、StatefulSet、Job等功能强大的Workload。控制器的核心思想是监听、比较资源实际状态与期望状态是否一致，若不一致则进行协调工作，使其最终一致。</p><p>那么当修改一个Deployment的镜像时，Deployment控制器是如何高效的感知到期望状态发生了变化呢？</p><p>要回答这个问题，得从etcd的Watch特性说起，它是Kubernetes控制器的工作基础。今天分享的主题就是etcd的核心特性Watch机制设计实现，通过分析Watch机制的四大核心问题，了解一个变化数据是如何从0到1推送给client，并介绍Watch特性从etcd v2到etcd v3演进、优化过程。</p><h2 id="Watch特性初体验"><a href="#Watch特性初体验" class="headerlink" title="Watch特性初体验"></a>Watch特性初体验</h2><p>先通过几个简单命令，带你初体验下Watch特性。</p><p>启动一个空集群，更新两次key hello后，使用Watch特性如何获取key hello的历史修改记录呢？</p><p>如下所示，可以通过下面的watch命令，带版本号监听key hello，集群版本号可通过endpoint status命令获取，空集群启动后的版本号为1。</p><p>执行后输出如下代码所示，两个事件记录分别对应上面的两次的修改，事件中含有key、value、各类版本号等信息，还可以通过比较create_revision和mod_revision区分此事件是add还是update事件。</p><p>watch命令执行后，你后续执行的增量put hello修改操作，它同样可持续输出最新的变更事件给你。</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs applescript">$ etcdctl <span class="hljs-keyword">put</span> hello world1<br>$ etcdctl <span class="hljs-keyword">put</span> hello world2<br>$ etcdctl watch hello -w=json <span class="hljs-comment">--rev=1</span><br>&#123;<br>    <span class="hljs-string">&quot;Events&quot;</span>:[<br>        &#123;<br>            <span class="hljs-string">&quot;kv&quot;</span>:&#123;<br>                <span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;aGVsbG8=&quot;</span>,<br>                <span class="hljs-string">&quot;create_revision&quot;</span>:<span class="hljs-number">2</span>,<br>                <span class="hljs-string">&quot;mod_revision&quot;</span>:<span class="hljs-number">2</span>,<br>                <span class="hljs-string">&quot;version&quot;</span>:<span class="hljs-number">1</span>,<br>                <span class="hljs-string">&quot;value&quot;</span>:<span class="hljs-string">&quot;d29ybGQx&quot;</span><br>            &#125;<br>        &#125;,<br>        &#123;<br>            <span class="hljs-string">&quot;kv&quot;</span>:&#123;<br>                <span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;aGVsbG8=&quot;</span>,<br>                <span class="hljs-string">&quot;create_revision&quot;</span>:<span class="hljs-number">2</span>,<br>                <span class="hljs-string">&quot;mod_revision&quot;</span>:<span class="hljs-number">3</span>,<br>                <span class="hljs-string">&quot;version&quot;</span>:<span class="hljs-number">2</span>,<br>                <span class="hljs-string">&quot;value&quot;</span>:<span class="hljs-string">&quot;d29ybGQy&quot;</span><br>            &#125;<br>        &#125;<br>    ],<br>    <span class="hljs-string">&quot;CompactRevision&quot;</span>:<span class="hljs-number">0</span>,<br>    <span class="hljs-string">&quot;Canceled&quot;</span>:<span class="hljs-literal">false</span>,<br>    <span class="hljs-string">&quot;Created&quot;</span>:<span class="hljs-literal">false</span><br>&#125;<br><br></code></pre></td></tr></table></figure><p>从以上初体验中，可以看到，基于Watch特性，可以快速获取到你感兴趣的数据变化事件，这也是Kubernetes控制器工作的核心基础。在这过程中，其实有以下四大核心问题：</p><p><strong>第一，client获取事件的机制，etcd是使用轮询模式还是推送模式呢？两者各有什么优缺点？</strong></p><p><strong>第二，事件是如何存储的？ 会保留多久？watch命令中的版本号具有什么作用？</strong></p><p><strong>第三，当client和server端出现短暂网络波动等异常因素后，导致事件堆积时，server端会丢弃事件吗？若你监听的历史版本号server端不存在了，你的代码该如何处理？</strong></p><p><strong>第四，如果你创建了上万个watcher监听key变化，当server端收到一个写请求后，etcd是如何根据变化的key快速找到监听它的watcher呢？</strong></p><p>接下来分别详细聊聊etcd Watch特性是如何解决这四大问题的。搞懂这四个问题，就明白etcd甚至各类分布式存储Watch特性的核心实现原理了。</p><h2 id="轮询-vs-流式推送"><a href="#轮询-vs-流式推送" class="headerlink" title="轮询 vs 流式推送"></a>轮询 vs 流式推送</h2><p>首先第一个问题是 <strong>client获取事件机制</strong>，etcd是使用轮询模式还是推送模式呢？两者各有什么优缺点？</p><p>答案是两种机制etcd都使用过。</p><p>在etcd v2 Watch机制实现中，使用的是HTTP&#x2F;1.x协议，实现简单、兼容性好，每个watcher对应一个TCP连接。client通过HTTP&#x2F;1.1协议长连接定时轮询server，获取最新的数据变化事件。</p><p>然而当你的watcher成千上万的时，即使集群空负载，大量轮询也会产生一定的QPS，server端会消耗大量的socket、内存等资源，导致etcd的扩展性、稳定性无法满足Kubernetes等业务场景诉求。</p><p>etcd v3的Watch机制的设计实现并非凭空出现，它正是吸取了etcd v2的经验、教训而重构诞生的。</p><p>在etcd v3中，为了解决etcd v2的以上缺陷，使用的是基于HTTP&#x2F;2的gRPC协议，双向流的Watch API设计，实现了连接多路复用。</p><p>HTTP&#x2F;2协议为什么能实现多路复用呢？</p><p><img src="https://static001.geekbang.org/resource/image/be/74/be3a019beaf1310d214e5c9948cc9c74.png?wh=1784*534" alt="img"></p><p>在HTTP&#x2F;2协议中，HTTP消息被分解独立的帧（Frame），交错发送，帧是最小的数据单位。每个帧会标识属于哪个流（Stream），流由多个数据帧组成，每个流拥有一个唯一的ID，一个数据流对应一个请求或响应包。</p><p>如上图所示，client正在向server发送数据流5的帧，同时server也正在向client发送数据流1和数据流3的一系列帧。一个连接上有并行的三个数据流，HTTP&#x2F;2可基于帧的流ID将并行、交错发送的帧重新组装成完整的消息。</p><p>通过以上机制，HTTP&#x2F;2就解决了HTTP&#x2F;1的请求阻塞、连接无法复用的问题，实现了多路复用、乱序发送。</p><p>etcd基于以上介绍的HTTP&#x2F;2协议的多路复用等机制，实现了一个client&#x2F;TCP连接支持多gRPC Stream， 一个gRPC Stream又支持多个watcher，如下图所示。同时事件通知模式也从client轮询优化成server流式推送，极大降低了server端socket、内存等资源。</p><p><img src="https://static001.geekbang.org/resource/image/f0/be/f08d1c50c6bc14f09b5028095ce275be.png?wh=1804*1076" alt="img"></p><p>当然在etcd v3 watch性能优化的背后，也带来了Watch API复杂度上升, 不过你不用担心，etcd的clientv3库已经帮助你搞定这些棘手的工作了。</p><p>在clientv3库中，Watch特性被抽象成Watch、Close、RequestProgress三个简单API提供给开发者使用，屏蔽了client与gRPC WatchServer交互的复杂细节，实现了一个client支持多个gRPC Stream，一个gRPC Stream支持多个watcher，显著降低了你的开发复杂度。</p><p>同时当watch连接的节点故障，clientv3库支持自动重连到健康节点，并使用之前已接收的最大版本号创建新的watcher，避免旧事件回放等。</p><h2 id="滑动窗口-vs-MVCC"><a href="#滑动窗口-vs-MVCC" class="headerlink" title="滑动窗口 vs MVCC"></a>滑动窗口 vs MVCC</h2><p>介绍完etcd v2的轮询机制和etcd v3的流式推送机制后，再看第二个问题，事件是如何存储的？ 会保留多久呢？watch命令中的版本号具有什么作用？</p><p>第二个问题的本质是 <strong>历史版本存储</strong>，etcd经历了从滑动窗口到MVCC机制的演变，滑动窗口是仅保存有限的最近历史版本到内存中，而MVCC机制则将历史版本保存在磁盘中，避免了历史版本的丢失，极大的提升了Watch机制的可靠性。</p><p>etcd v2滑动窗口是如何实现的？它有什么缺点呢？</p><p>它使用的是如下一个简单的环形数组来存储历史事件版本，当key被修改后，相关事件就会被添加到数组中来。若超过eventQueue的容量，则淘汰最旧的事件。在etcd v2中，eventQueue的容量是固定的1000，因此它最多只会保存1000条事件记录，不会占用大量etcd内存导致etcd OOM。</p><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-keyword">type</span> <span class="hljs-type">EventHistory</span> struct &#123;<br>   <span class="hljs-type">Queue</span>      eventQueue<br>   <span class="hljs-type">StartIndex</span> uint64<br>   <span class="hljs-type">LastIndex</span>  uint64<br>   rwl        sync.<span class="hljs-type">RWMutex</span><br>&#125;<br><br></code></pre></td></tr></table></figure><p>但是它的缺陷显而易见的，固定的事件窗口只能保存有限的历史事件版本，是不可靠的。当写请求较多的时候、client与server网络出现波动等异常时，很容易导致事件丢失，client不得不触发大量的expensive查询操作，以获取最新的数据及版本号，才能持续监听数据。</p><p>特别是对于重度依赖Watch机制的Kubernetes来说，显然是无法接受的。因为这会导致控制器等组件频繁的发起expensive List Pod等资源操作，导致APIServer&#x2F;etcd出现高负载、OOM等，对稳定性造成极大的伤害。</p><p>etcd v3的MVCC机制，正如上一节课所介绍的，就是为解决etcd v2 Watch机制不可靠而诞生。相比etcd v2直接保存事件到内存的环形数组中，etcd v3则是将一个key的历史修改版本保存在boltdb里面。boltdb是一个基于磁盘文件的持久化存储，因此它重启后历史事件不像etcd v2一样会丢失，同时你可通过配置压缩策略，来控制保存的历史版本数，在压缩篇我会和你详细讨论它。</p><p>最后watch命令中的版本号具有什么作用呢?</p><p>在上一节课中我们深入介绍了它的含义，版本号是etcd逻辑时钟，当client因网络等异常出现连接闪断后，通过版本号，它就可从server端的boltdb中获取错过的历史事件，而无需全量同步，它是etcd Watch机制数据增量同步的核心。</p><h2 id="可靠的事件推送机制"><a href="#可靠的事件推送机制" class="headerlink" title="可靠的事件推送机制"></a>可靠的事件推送机制</h2><p>再看第三个问题，当client和server端出现短暂网络波动等异常因素后，导致事件堆积时，server端会丢弃事件吗？若你监听的历史版本号server端不存在了，你的代码该如何处理？</p><p>第三个问题的本质是 <strong>可靠事件推送机制</strong>，要搞懂它，我们就得弄懂etcd Watch特性的整体架构、核心流程，下图是Watch特性整体架构图。</p><h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><p><img src="https://static001.geekbang.org/resource/image/42/bf/42575d8d0a034e823b8e48d4ca0a49bf.png?wh=1920*1075" alt="img"></p><p>通过上面的架构图，简要介绍下一个watch请求流程，对全流程有个整体的认识。</p><p>当你通过etcdctl或API发起一个watch key请求的时候，etcd的gRPCWatchServer收到watch请求后，会创建一个serverWatchStream, 它负责接收client的gRPC Stream的create&#x2F;cancel watcher请求(recvLoop goroutine)，并将从MVCC模块接收的Watch事件转发给client(sendLoop goroutine)。</p><p>当serverWatchStream收到create watcher请求后，serverWatchStream会调用MVCC模块的WatchStream子模块分配一个watcher id，并将watcher注册到MVCC的WatchableKV模块。</p><p>在etcd启动的时候，WatchableKV模块会运行syncWatchersLoop和syncVictimsLoop goroutine，分别负责不同场景下的事件推送，它们也是Watch特性可靠性的核心之一。</p><p>从架构图中你可以看到Watch特性的核心实现是WatchableKV模块，下面我就为你抽丝剥茧，看看”etcdctl watch hello -w&#x3D;json –rev&#x3D;1”命令在WatchableKV模块是如何处理的？面对各类异常，它如何实现可靠事件推送？</p><p><strong>etcd核心解决方案是复杂度管理，问题拆分。</strong></p><p>etcd根据不同场景，对问题进行了分解，将watcher按场景分类，实现了轻重分离、低耦合。我首先给你介绍下synced watcher、unsynced watcher它们各自的含义。</p><p><strong>synced watcher</strong>，顾名思义，表示此类watcher监听的数据都已经同步完毕，在等待新的变更。</p><p>如果你创建的watcher未指定版本号(默认0)、或指定的版本号大于etcd sever当前最新的版本号(currentRev)，那么它就会保存到synced watcherGroup中。watcherGroup负责管理多个watcher，能够根据key快速找到监听该key的一个或多个watcher。</p><p><strong>unsynced watcher</strong>，表示此类watcher监听的数据还未同步完成，落后于当前最新数据变更，正在努力追赶。</p><p>如果你创建的watcher指定版本号小于etcd server当前最新版本号，那么它就会保存到unsynced watcherGroup中。比如我们的这个案例中watch带指定版本号1监听时，版本号1和etcd server当前版本之间的数据并未同步给你，因此它就属于此类。</p><p>从以上介绍中，我们可以将可靠的事件推送机制拆分成最新事件推送、异常场景重试、历史事件推送机制三个子问题来进行分析。</p><p>下面是第一个子问题，最新事件推送机制。</p><h3 id="最新事件推送机制"><a href="#最新事件推送机制" class="headerlink" title="最新事件推送机制"></a>最新事件推送机制</h3><p>当etcd收到一个写请求，key-value发生变化的时候，处于syncedGroup中的watcher，是如何获取到最新变化事件并推送给client的呢？</p><p><img src="https://static001.geekbang.org/resource/image/5y/48/5yy0cbf2833c438812086287d2ebf948.png?wh=1920*1060" alt="img"></p><p>当创建完成watcher后，此时你执行put hello修改操作时，如上图所示，请求经过KVServer、Raft模块后Apply到状态机时，在MVCC的put事务中，它会将本次修改的后的mvccpb.KeyValue保存到一个changes数组中。</p><p>在put事务结束时，如下面的精简代码所示，它会将KeyValue转换成Event事件，然后回调watchableStore.notify函数（流程5）。notify会匹配出监听过此key并处于synced watcherGroup中的watcher，同时事件中的版本号要大于等于watcher监听的最小版本号，才能将事件发送到此watcher的事件channel中。</p><p>serverWatchStream的sendLoop goroutine监听到channel消息后，读出消息立即推送给client（流程6和7），至此，完成一个最新修改事件推送。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs stylus">evs := <span class="hljs-built_in">make</span>(<span class="hljs-selector-attr">[]</span>mvccpb<span class="hljs-selector-class">.Event</span>, <span class="hljs-built_in">len</span>(changes))<br><span class="hljs-keyword">for</span> <span class="hljs-selector-tag">i</span>, change := range changes &#123;<br>   evs<span class="hljs-selector-attr">[i]</span><span class="hljs-selector-class">.Kv</span> = &amp;changes<span class="hljs-selector-attr">[i]</span><br>   <span class="hljs-keyword">if</span> change<span class="hljs-selector-class">.CreateRevision</span> == <span class="hljs-number">0</span> &#123;<br>      evs<span class="hljs-selector-attr">[i]</span><span class="hljs-selector-class">.Type</span> = mvccpb<span class="hljs-selector-class">.DELETE</span><br>      evs<span class="hljs-selector-attr">[i]</span><span class="hljs-selector-class">.Kv</span><span class="hljs-selector-class">.ModRevision</span> = rev<br>   &#125; <span class="hljs-keyword">else</span> &#123;<br>      evs<span class="hljs-selector-attr">[i]</span><span class="hljs-selector-class">.Type</span> = mvccpb<span class="hljs-selector-class">.PUT</span><br>   &#125;<br>&#125;<br>tw<span class="hljs-selector-class">.s</span><span class="hljs-selector-class">.notify</span>(rev, evs)<br><br></code></pre></td></tr></table></figure><p>注意接收Watch事件channel的buffer容量默认1024(etcd v3.4.9)。若client与server端因网络波动、高负载等原因导致推送缓慢，buffer满了，事件会丢失吗？</p><p>这就是第二个子问题，异常场景的重试机制。</p><h3 id="异常场景重试机制"><a href="#异常场景重试机制" class="headerlink" title="异常场景重试机制"></a>异常场景重试机制</h3><p>若出现channel buffer满了，etcd为了保证Watch事件的高可靠性，并不会丢弃它，而是将此watcher从synced watcherGroup中删除，然后将此watcher和事件列表保存到一个名为受害者victim的watcherBatch结构中，通过 <strong>异步机制重试</strong> 保证事件的可靠性。</p><p>还有一个点需要注意的是，notify操作它是在修改事务结束时同步调用的，必须是轻量级、高性能、无阻塞的，否则会严重影响集群写性能。</p><p>那么若因网络波动、CPU高负载等异常导致watcher处于victim集合中后，etcd是如何处理这种slow watcher呢？</p><p>在介绍Watch机制整体架构时，我们知道WatchableKV模块会启动两个异步goroutine，其中一个是syncVictimsLoop，正是它负责slower watcher的堆积的事件推送。</p><p>它的基本工作原理是，遍历victim watcherBatch数据结构，尝试将堆积的事件再次推送到watcher的接收channel中。若推送失败，则再次加入到victim watcherBatch数据结构中等待下次重试。</p><p>若推送成功，watcher监听的最小版本号(minRev)小于等于server当前版本号(currentRev)，说明可能还有历史事件未推送，需加入到unsynced watcherGroup中，由下面介绍的历史事件推送机制，推送minRev到currentRev之间的事件。</p><p>若watcher的最小版本号大于server当前版本号，则加入到synced watcher集合中，进入上面介绍的最新事件通知机制。</p><p>下面我给你画了一幅图总结各类watcher状态转换关系，希望能帮助你快速厘清之间关系。</p><p><img src="https://static001.geekbang.org/resource/image/40/8e/40ec1087113edfc9f7yy0f32394b948e.png?wh=1920*1065" alt="img"></p><p>介绍完最新事件推送、异常场景重试机制后，那历史事件推送机制又是怎么工作的呢？</p><h3 id="历史事件推送机制"><a href="#历史事件推送机制" class="headerlink" title="历史事件推送机制"></a>历史事件推送机制</h3><p>WatchableKV模块的另一个goroutine，syncWatchersLoop，正是负责unsynced watcherGroup中的watcher历史事件推送。</p><p>在历史事件推送机制中，如果你监听老的版本号已经被etcd压缩了，client该如何处理？</p><p>要了解这个问题，我们就得搞清楚syncWatchersLoop如何工作，它的核心支撑是boltdb中存储了key-value的历史版本。</p><p>syncWatchersLoop，它会遍历处于unsynced watcherGroup中的每个watcher，为了优化性能，它会选择一批unsynced watcher批量同步，找出这一批unsynced watcher中监听的最小版本号。</p><p>因boltdb的key是按版本号存储的，因此可通过指定查询的key范围的最小版本号作为开始区间，当前server最大版本号作为结束区间，遍历boltdb获得所有历史数据。</p><p>然后将KeyValue结构转换成事件，匹配出监听过事件中key的watcher后，将事件发送给对应的watcher事件接收channel即可。发送完成后，watcher从unsynced watcherGroup中移除、添加到synced watcherGroup中，如下面的watcher状态转换图黑色虚线框所示。</p><p><img src="https://static001.geekbang.org/resource/image/a7/b4/a7a04846de2be66f1162af8845b13ab4.png?wh=1920*1098" alt="img"></p><p>若watcher监听的版本号已经小于当前etcd server压缩的版本号，历史变更数据就可能已丢失，因此etcd server会返回ErrCompacted错误给client。client收到此错误后，需重新获取数据最新版本号后，再次Watch。你在业务开发过程中，使用Watch API最常见的一个错误之一就是未处理此错误。</p><h2 id="高效的事件匹配"><a href="#高效的事件匹配" class="headerlink" title="高效的事件匹配"></a>高效的事件匹配</h2><p>介绍完可靠的事件推送机制后，最后我们再看第四个问题，如果你创建了上万个watcher监听key变化，当server端收到一个写请求后，etcd是如何根据变化的key快速找到监听它的watcher呢？一个个遍历watcher吗？</p><p>显然一个个遍历watcher是最简单的方法，但是它的时间复杂度是O(N)，在watcher数较多的场景下，会导致性能出现瓶颈。更何况etcd是在执行一个写事务结束时，同步触发事件通知流程的，若匹配watcher开销较大，将严重影响etcd性能。</p><p>那使用什么数据结构来快速查找哪些watcher监听了一个事件中的key呢？</p><p>也许你会说使用map记录下哪些watcher监听了什么key不就可以了吗？ etcd的确使用<strong>map</strong>记录了监听单个key的watcher，但是你要注意的是Watch特性不仅仅可以监听单key，它还可以指定监听key范围、key前缀，因此etcd还使用了如下的<strong>区间树</strong>。</p><p><img src="https://static001.geekbang.org/resource/image/5a/88/5ae0a99629021e4a05c08yyd0df92f88.png?wh=1920*1040" alt="img"></p><p>当收到创建watcher请求的时候，它会把watcher监听的key范围插入到上面的区间树中，区间的值保存了监听同样key范围的watcher集合&#x2F;watcherSet。</p><p>当产生一个事件时，etcd首先需要从map查找是否有watcher监听了单key，其次它还需要从区间树找出与此key相交的所有区间，然后从区间的值获取监听的watcher集合。</p><p><strong>区间树支持快速查找一个key是否在某个区间内，时间复杂度O(LogN)，因此etcd基于map和区间树实现了watcher与事件快速匹配，具备良好的扩展性。</strong></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>提出了Watch特性设计实现的四个核心问题，分别是<strong>获取事件机制、事件历史版本存储、如何实现可靠的事件推送机制、如何高效的将事件与watcher进行匹配。</strong></p><p>在获取事件机制、事件历史版本存储两个问题中，我给你介绍了etcd v2在使用HTTP&#x2F;1.x轮询、滑动窗口时，存在大量的连接数、丢事件等问题，导致扩展性、稳定性较差。</p><p>而etcd v3 Watch特性优化思路是基于HTTP&#x2F;2的流式传输、多路复用，实现了一个连接支持多个watcher，减少了大量连接数，事件存储也从滑动窗口优化成稳定可靠的MVCC机制，历史版本保存在磁盘中，具备更好的扩展性、稳定性。</p><p>在实现可靠的事件推送机制问题中，我通过一个整体架构图带你了解整个Watch机制的核心链路，数据推送流程。</p><p>Watch特性的核心实现模块是watchableStore，它通过将watcher划分为synced&#x2F;unsynced&#x2F;victim三类，将问题进行了分解，并通过多个后台异步循环 goroutine负责不同场景下的事件推送，提供了各类异常等场景下的Watch事件重试机制，尽力确保变更事件不丢失、按逻辑时钟版本号顺序推送给client。</p><p>最后一个事件匹配性能问题，etcd基于map和区间树数实现了watcher与事件快速匹配，保障了大规模场景下的Watch机制性能和读写稳定性。</p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>07.MVCC：etcd如何实现多版本并发控制？</title>
    <link href="/2022/10/04/07-MVCC%EF%BC%9Aetcd%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%A4%9A%E7%89%88%E6%9C%AC%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%EF%BC%9F/"/>
    <url>/2022/10/04/07-MVCC%EF%BC%9Aetcd%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%A4%9A%E7%89%88%E6%9C%AC%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="07-MVCC：如何实现多版本并发控制？"><a href="#07-MVCC：如何实现多版本并发控制？" class="headerlink" title="07.MVCC：如何实现多版本并发控制？"></a>07.MVCC：如何实现多版本并发控制？</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><p>etcd v2时，存在若干局限，如仅保留最新版本key-value数据、丢弃历史版本。而<strong>etcd核心特性watch又依赖历史版本</strong>（为什么watch机制依赖历史版本），因此etcd v2为了缓解这个问题，会在内存中维护一个较短的全局事件滑动窗口，保留最近的1000条变更事件。但是在集群写请求较多等场景下，它依然无法提供可靠的Watch机制。</p><p>那么不可靠的etcd v2事件机制，在etcd v3中是如何解决的呢？</p><p>今天分享的MVCC（Multiversion concurrency control）机制，正是为解决这个问题而诞生的。</p><p>MVCC机制的核心思想是保存一个key-value数据的多个历史版本，etcd基于它不仅实现了可靠的Watch机制，避免了client频繁发起List Pod等expensive request操作，保障etcd集群稳定性。而且MVCC还能以较低的并发控制开销，实现各类隔离级别的事务，保障事务的安全性，是事务特性的基础。</p><h2 id="什么是MVCC"><a href="#什么是MVCC" class="headerlink" title="什么是MVCC"></a>什么是MVCC</h2><p>首先什么是MVCC，从名字上理解，它是一个基于多版本技术实现的一种并发控制机制。那常见的并发机制有哪些？MVCC的优点在哪里呢？</p><p>提到并发控制机制你可能就没那么陌生了，比如数据库中的悲观锁，也就是通过锁机制确保同一时刻只能有一个事务对数据进行修改操作，常见的实现方案有读写锁、互斥锁、两阶段锁等。</p><p>悲观锁是一种事先预防机制，它悲观地认为多个并发事务可能会发生冲突，因此它要求事务必须先获得锁，才能进行修改数据操作。但是悲观锁粒度过大、高并发场景下大量事务会阻塞等，会导致服务性能较差。</p><p><strong>MVCC机制正是基于多版本技术实现的一种乐观锁机制</strong>，它乐观地认为数据不会发生冲突，但是当事务提交时，具备检测数据是否冲突的能力。</p><p>在MVCC数据库中，你更新一个key-value数据的时候，它并不会直接覆盖原数据，而是新增一个版本来存储新的数据，每个数据都有一个版本号。版本号它是一个逻辑时间，为了方便你深入理解版本号意义，在下面我给你画了一个etcd MVCC版本号时间序列图。</p><p>从图中你可以看到，随着时间增长，你每次修改操作，版本号都会递增。每修改一次，生成一条新的数据记录。 <strong>当指定版本号读取数据时，它实际上访问的是版本号生成那个时间点的快照数据</strong>。当删除数据的时候，它实际也是新增一条带删除标识的数据记录。</p><p><img src="https://static001.geekbang.org/resource/image/1f/2c/1fbf4aa426c8b78570ed310a8c9e2c2c.png?wh=1920*806" alt="img"></p><h2 id="MVCC特性初体验"><a href="#MVCC特性初体验" class="headerlink" title="MVCC特性初体验"></a>MVCC特性初体验</h2><p>了解完什么是MVCC后，先通过几个简单命令，体验下MVCC特性，看看它是如何查询历史修改记录，以及找回不小心删除的key的。</p><p>启动一个空集群，更新两次key hello后，如何获取key hello的上一个版本值呢？ 删除key hello后，还能读到历史版本吗?</p><p>如下面的命令所示，第一次key hello更新完后，通过get命令获取下它的key-value详细信息。除了key、value信息，还有各类版本号这里关于mod_revision字段，它表示key最后一次修改时的etcd版本号。</p><p>当我们再次更新key hello为world2后，然后通过查询时指定key第一次更新后的版本号，会发现我们查询到了第一次更新的值，甚至我们执行删除key hello后，依然可以获得到这个值。那么etcd是如何实现的呢?</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 更新key hello为world1</span><br>$ etcdctl put hello world1<br>OK<br><span class="hljs-comment"># 通过指定输出模式为json,查看key hello更新后的详细信息</span><br>$ etcdctl <span class="hljs-built_in">get</span> hello <span class="hljs-attribute">-w</span>=json<br>&#123;<br>    <span class="hljs-string">&quot;kvs&quot;</span>:[<br>        &#123;<br>            <span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;aGVsbG8=&quot;</span>,<br>            <span class="hljs-string">&quot;create_revision&quot;</span>:2,<br>            <span class="hljs-string">&quot;mod_revision&quot;</span>:2,<br>            <span class="hljs-string">&quot;version&quot;</span>:1,<br>            <span class="hljs-string">&quot;value&quot;</span>:<span class="hljs-string">&quot;d29ybGQx&quot;</span><br>        &#125;<br>    ],<br>    <span class="hljs-string">&quot;count&quot;</span>:1<br>&#125;<br><span class="hljs-comment"># 再次修改key hello为world2</span><br>$ etcdctl put hello world2<br>OK<br><span class="hljs-comment"># 确认修改成功,最新值为wolrd2</span><br>$ etcdctl <span class="hljs-built_in">get</span> hello<br>hello<br>world2<br><span class="hljs-comment"># 指定查询版本号,获得了hello上一次修改的值</span><br>$ etcdctl <span class="hljs-built_in">get</span> hello <span class="hljs-attribute">--rev</span>=2<br>hello<br>world1<br><span class="hljs-comment"># 删除key hello</span><br>$ etcdctl del  hello<br>1<br><span class="hljs-comment"># 删除后指定查询版本号3,获得了hello删除前的值</span><br>$ etcdctl <span class="hljs-built_in">get</span> hello <span class="hljs-attribute">--rev</span>=3<br>hello<br>world2<br><br></code></pre></td></tr></table></figure><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p>在详细介绍etcd如何实现MVCC特性前，从整体上介绍下MVCC模块。下图是MVCC模块的一个整体架构图，整个MVCC特性由treeIndex、Backend&#x2F;boltdb组成。</p><p>当你执行MVCC特性初体验中的put命令后，请求经过gRPC KV Server、Raft模块流转，对应的日志条目被提交后，Apply模块开始执行此日志内容。</p><p><img src="https://static001.geekbang.org/resource/image/f5/2c/f5799da8d51a381527068a95bb13592c.png?wh=1920*1064" alt="img"></p><p>Apply模块通过MVCC模块来执行put请求，持久化key-value数据。MVCC模块将请求请划分成两个类别，分别是读事务（ReadTxn）和写事务（WriteTxn）。读事务负责处理range请求，写事务负责put&#x2F;delete操作。读写事务基于treeIndex、Backend&#x2F;boltdb提供的能力，实现对key-value的增删改查功能。</p><p>treeIndex模块基于内存版B-tree实现了key索引管理，它保存了用户key与版本号（revision）的映射关系等信息。</p><p>Backend模块负责etcd的key-value持久化存储，主要由ReadTx、BatchTx、Buffer组成，ReadTx定义了抽象的读事务接口，BatchTx在ReadTx之上定义了抽象的写事务接口，Buffer是数据缓存区。</p><p>etcd设计上支持多种Backend实现，目前实现的Backend是boltdb。boltdb是一个基于B+ tree实现的、支持事务的key-value嵌入式数据库。</p><p>treeIndex与boltdb关系你可参考下图。当你发起一个get hello命令时，从treeIndex中获取key的版本号，然后再通过这个版本号，从boltdb获取value信息。boltdb的value是包含用户key-value、各种版本号、lease信息的结构体。</p><p><img src="https://static001.geekbang.org/resource/image/e7/8f/e713636c6cf9c46c7c19f677232d858f.png?wh=1920*903" alt="img"></p><p>接下来重点聊聊treeIndex模块的原理与核心数据结构。</p><h2 id="treeIndex原理"><a href="#treeIndex原理" class="headerlink" title="treeIndex原理"></a>treeIndex原理</h2><p>为什么需要treeIndex模块呢?</p><p>对于etcd v2来说，当你通过etcdctl发起一个put hello操作时，etcd v2直接更新内存树，这就导致历史版本直接被覆盖，无法支持保存key的历史版本。在etcd v3中引入treeIndex模块正是为了解决这个问题，<strong>支持保存key的历史版本，提供稳定的Watch机制和事务隔离等能力。</strong></p><p><strong>那etcd v3又是如何基于treeIndex模块，实现保存key的历史版本的呢?</strong></p><p>etcd在每次修改key时会生成一个全局递增的版本号（revision），然后通过数据结构B-tree保存用户key与版本号之间的关系，再以版本号作为boltdb key，以用户的key-value等信息作为boltdb value，保存到boltdb。</p><p><strong>etcd保存用户key与版本号映射关系的数据结构B-tree，为什么etcd使用它而不使用哈希表、平衡二叉树？</strong></p><p>从etcd的功能特性上分析， 因etcd支持范围查询，因此保存索引的数据结构也必须支持范围查询才行。所以哈希表不适合，而B-tree支持范围查询。</p><p>从性能上分析，平横二叉树每个节点只能容纳一个数据、导致树的高度较高，而B-tree每个节点可以容纳多个数据，树的高度更低，更扁平，涉及的查找次数更少，具有优越的增、删、改、查性能。</p><p>Google的开源项目btree，使用Go语言实现了一个内存版的B-tree，对外提供了简单易用的接口。etcd正是基于btree库实现了一个名为treeIndex的索引模块，通过它来查询、保存用户key与版本号之间的关系。</p><p>下图是个最大度（degree &gt; 1，简称d）为5的B-tree，度是B-tree中的一个核心参数，它决定了你每个节点上的数据量多少、节点的“胖”、“瘦”程度。</p><p>从图中你可以看到，节点越胖，意味着一个节点可以存储更多数据，树的高度越低。在一个度为d的B-tree中，节点保存的最大key数为2d - 1，否则需要进行平衡、分裂操作。这里你要注意的是在etcd treeIndex模块中，创建的是最大度32的B-tree，也就是一个叶子节点最多可以保存63个key。</p><p><img src="https://static001.geekbang.org/resource/image/44/74/448c8a2bb3b5d2d48dfb6ea585172c74.png?wh=1920*934" alt="img"></p><p>从图中可以看到，你通过put&#x2F;txn命令写入的一系列key，treeIndex模块基于B-tree将其组织起来，节点之间基于用户key比较大小。当你查找一个key k95时，通过B-tree的特性，你仅需通过图中流程1和2两次快速比较，就可快速找到k95所在的节点。</p><p>在treeIndex中，每个节点的key是一个keyIndex结构，etcd就是通过它保存了用户的key与版本号的映射关系。</p><p>那么keyIndex结构包含哪些信息呢？下面是字段说明，可以参考一下。</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs awk">type keyIndex struct &#123;<br>   key         []byte <span class="hljs-regexp">//</span>用户的key名称，比如我们案例中的<span class="hljs-string">&quot;hello&quot;</span><br>   modified    revision <span class="hljs-regexp">//</span>最后一次修改key时的etcd版本号,比如我们案例中的刚写入hello为world1时的，版本号为<span class="hljs-number">2</span><br>   generations []generation <span class="hljs-regexp">//g</span>eneration保存了一个key若干代版本号信息，每代中包含对key的多次修改的版本号列表<br>&#125;<br><br></code></pre></td></tr></table></figure><p>keyIndex中包含用户的key、最后一次修改key时的etcd版本号、key的若干代（generation）版本号信息，每代中包含对key的多次修改的版本号列表。那我们要如何理解generations？为什么它是个数组呢?</p><p>generations表示一个key从创建到删除的过程，每代对应key的一个生命周期的开始与结束。当你第一次创建一个key时，会生成第0代，后续的修改操作都是在往第0代中追加修改版本号。当你把key删除后，它就会生成新的第1代，一个key不断经历创建、删除的过程，它就会生成多个代。</p><p>generation结构详细信息如下：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs awk">type generation struct &#123;<br>   ver     int64    <span class="hljs-regexp">//</span>表示此key的修改次数<br>   created revision <span class="hljs-regexp">//</span>表示generation结构创建时的版本号<br>   revs    []revision <span class="hljs-regexp">//</span>每次修改key时的revision追加到此数组<br>&#125;<br><br></code></pre></td></tr></table></figure><p>generation结构中包含此key的修改次数、generation创建时的版本号、对此key的修改版本号记录列表。</p><p>需要注意的是版本号（revision）并不是一个简单的整数，而是一个结构体。revision结构及含义如下：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs awk">type revision struct &#123;<br>   main int64    <span class="hljs-regexp">//</span> 一个全局递增的主版本号，随put<span class="hljs-regexp">/txn/</span><span class="hljs-keyword">delete</span>事务递增，一个事务内的key main版本号是一致的<br>   sub int64    <span class="hljs-regexp">//</span> 一个事务内的子版本号，从<span class="hljs-number">0</span>开始随事务内put/<span class="hljs-keyword">delete</span>操作递增<br>&#125;<br><br></code></pre></td></tr></table></figure><p>revision包含main和sub两个字段，main是全局递增的版本号，它是个etcd逻辑时钟，随着put&#x2F;txn&#x2F;delete等事务递增。sub是一个事务内的子版本号，从0开始随事务内的put&#x2F;delete操作递增。</p><p>比如启动一个空集群，全局版本号默认为1，执行下面的txn事务，它包含两次put、一次get操作，那么按照我们上面介绍的原理，全局版本号随读写事务自增，因此是main为2，sub随事务内的put&#x2F;delete操作递增，因此key hello的revison为{2,0}，key world的revision为{2,1}。</p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs subunit">$ etcdctl txn -i<br>compares:<br><br><span class="hljs-keyword">success </span>requests (get，put，del):<br>put hello 1<br>get hello<br>put world 2<br><br></code></pre></td></tr></table></figure><p>介绍完treeIndex基本原理、核心数据结构后，再看看在MVCC特性初体验中的更新、查询、删除key案例里，treeIndex与boltdb是如何协作，完成以上key-value操作的?</p><h2 id="MVCC更新key原理"><a href="#MVCC更新key原理" class="headerlink" title="MVCC更新key原理"></a>MVCC更新key原理</h2><p>当我们通过etcdctl发起一个put hello操作时，如下面的put事务流程图流程一所示，在put写事务中，首先它需要从treeIndex模块中查询key的keyIndex索引信息，keyIndex中存储了key的创建版本号、修改的次数等信息，这些信息在事务中发挥着重要作用，因此会存储在boltdb的value中。</p><p>在案例中，因为是第一次创建hello key，此时keyIndex索引为空。</p><p><img src="https://static001.geekbang.org/resource/image/84/e1/84377555cb4150ea7286c9ef3c5e17e1.png?wh=1920*1063" alt="img"></p><p>其次etcd会根据当前的全局版本号（空集群启动时默认为1）自增，生成put hello操作对应的版本号revision{2,0}，这就是boltdb的key。</p><p>boltdb的value是mvccpb.KeyValue结构体，它是由用户key、value、create_revision、mod_revision、version、lease组成。它们的含义分别如下：</p><ul><li>create_revision表示此key创建时的版本号。在我们的案例中，key hello是第一次创建，那么值就是2。当你再次修改key hello的时候，写事务会从treeIndex模块查询hello第一次创建的版本号，也就是keyIndex.generations[i].created字段，赋值给create_revision字段；</li><li>mod_revision表示key最后一次修改时的版本号，即put操作发生时的全局版本号加1；</li><li>version表示此key的修改次数。每次修改的时候，写事务会从treeIndex模块查询hello已经历过的修改次数，也就是keyIndex.generations[i].ver字段，将ver字段值加1后，赋值给version字段。</li></ul><p>填充好boltdb的KeyValue结构体后，这时就可以通过Backend的写事务batchTx接口将key{2,0},value为mvccpb.KeyValue保存到boltdb的缓存中，并同步更新buffer，如上图中的流程二所示。</p><p>此时存储到boltdb中的key、value数据如下：</p><p><img src="D:\桌面\学习资料\17689609062\etcd实战课\images\340226\a245b18eabc86ea83a71349f49bdceba.jpg"></p><p>然后put事务需将本次修改的版本号与用户key的映射关系保存到treeIndex模块中，也就是上图中的流程三。</p><p>因为key hello是首次创建，treeIndex模块它会生成key hello对应的keyIndex对象，并填充相关数据结构。</p><p>keyIndex填充后的结果如下所示：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">key</span> hello的keyIndex:<br><span class="hljs-attribute">key</span>:     <span class="hljs-string">&quot;hello&quot;</span><br><span class="hljs-attribute">modified</span>: &lt;<span class="hljs-number">2</span>,<span class="hljs-number">0</span>&gt;<br><span class="hljs-attribute">generations</span>:<span class="hljs-meta"></span><br><span class="hljs-meta">[&#123;ver:1,created:&lt;2,0&gt;,revisions: [&lt;2,0&gt;]&#125; ]</span><br><br></code></pre></td></tr></table></figure><p>我们来简易分析一下上面的结果。</p><ul><li>key为hello，modified为最后一次修改版本号&lt;2,0&gt;，key hello是首次创建的，因此新增一个generation代跟踪它的生命周期、修改记录；</li><li>generation的ver表示修改次数，首次创建为1，后续随着修改操作递增；</li><li>generation.created表示创建generation时的版本号为&lt;2,0&gt;；</li><li>revision数组保存对此key修改的版本号列表，每次修改都会将将相应的版本号追加到revisions数组中。</li></ul><p>通过以上流程，一个put操作终于完成。</p><p>但是此时数据还并未持久化，为了提升etcd的写吞吐量、性能，一般情况下（默认堆积的写事务数大于1万才在写事务结束时同步持久化），数据持久化由Backend的异步goroutine完成，它通过事务批量提交，定时将boltdb页缓存中的脏数据提交到持久化存储磁盘中，也就是下图中的黑色虚线框住的流程四。</p><p><img src="https://static001.geekbang.org/resource/image/5d/a2/5de49651cedf4595648aeba3c131cea2.png?wh=1920*1059" alt="img"></p><h2 id="MVCC查询key原理"><a href="#MVCC查询key原理" class="headerlink" title="MVCC查询key原理"></a>MVCC查询key原理</h2><p>完成put hello为world1操作后，这时你通过etcdctl发起一个get hello操作，MVCC模块首先会创建一个读事务对象（TxnRead），在etcd 3.4中Backend实现了ConcurrentReadTx， 也就是并发读特性。</p><p>并发读特性的核心原理是创建读事务对象时，它会全量拷贝当前写事务未提交的buffer数据，并发的读写事务不再阻塞在一个buffer资源锁上，实现了全并发读。</p><p><img src="https://static001.geekbang.org/resource/image/55/ee/55998d8a1f3091076a9119d85e7175ee.png?wh=1920*1070" alt="img"></p><p>如上图所示，在读事务中，它首先需要根据key从treeIndex模块获取版本号，因我们未带版本号读，默认是读取最新的数据。treeIndex模块从B-tree中，根据key查找到keyIndex对象后，匹配有效的generation，返回generation的revisions数组中最后一个版本号{2,0}给读事务对象。</p><p>读事务对象根据此版本号为key，通过Backend的并发读事务（ConcurrentReadTx）接口，优先从buffer中查询，命中则直接返回，否则从boltdb中查询此key的value信息。</p><p>那指定版本号读取历史记录又是怎么实现的呢？</p><p>当你再次发起一个put hello为world2修改操作时，key hello对应的keyIndex的结果如下面所示，keyIndex.modified字段更新为&lt;3,0&gt;，generation的revision数组追加最新的版本号&lt;3,0&gt;，ver修改为2。</p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">key hello的keyIndex:<br>key:     <span class="hljs-string">&quot;hello&quot;</span><br>modified: <span class="hljs-variable">&lt;3,0&gt;</span><br>generations:<br>[&#123;ver:2,created:<span class="hljs-variable">&lt;2,0&gt;</span>,revisions: [<span class="hljs-variable">&lt;2,0&gt;</span>,<span class="hljs-variable">&lt;3,0&gt;</span>]&#125;]<br><br></code></pre></td></tr></table></figure><p>boltdb插入一个新的key revision{3,0}，此时存储到boltdb中的key-value数据如下：</p><p><img src="https://static001.geekbang.org/resource/image/8b/f7/8bec06d61622f2a99ea9dd2f78e693f7.jpg?wh=1432*477" alt="img"></p><p>这时你再发起一个指定历史版本号为2的读请求时，实际是读版本号为2的时间点的快照数据。treeIndex模块会遍历generation内的历史版本号，返回小于等于2的最大历史版本号，在我们这个案例中，也就是revision{2,0}，以它作为boltdb的key，从boltdb中查询出value即可。</p><h2 id="MVCC删除key原理"><a href="#MVCC删除key原理" class="headerlink" title="MVCC删除key原理"></a>MVCC删除key原理</h2><p>介绍完MVCC更新、查询key的原理后，我们接着往下看。当你执行etcdctl del hello命令时，etcd会立刻从treeIndex和boltdb中删除此数据吗？还是增加一个标记实现延迟删除（lazy delete）呢？</p><p>答案为etcd实现的是<strong>延期删除模式</strong>，原理与key更新类似。</p><p>与更新key不一样之处在于，一方面，生成的boltdb key版本号{4,0,t}追加了删除标识（tombstone,简写t），boltdb value变成只含用户key的KeyValue结构体。另一方面treeIndex模块也会给此key hello对应的keyIndex对象，追加一个空的generation对象，表示此索引对应的key被删除了。</p><p>当你再次查询hello的时候，treeIndex模块根据key hello查找到keyindex对象后，若发现其存在空的generation对象，并且查询的版本号大于等于被删除时的版本号，则会返回空。</p><p>etcdctl hello操作后的keyIndex的结果如下面所示：</p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">key hello的keyIndex:<br>key:     <span class="hljs-string">&quot;hello&quot;</span><br>modified: <span class="hljs-variable">&lt;4,0&gt;</span><br>generations:<br>[<br>&#123;ver:3,created:<span class="hljs-variable">&lt;2,0&gt;</span>,revisions: [<span class="hljs-variable">&lt;2,0&gt;</span>,<span class="hljs-variable">&lt;3,0&gt;</span>,<span class="hljs-variable">&lt;4,0&gt;</span>(t)]&#125;，<br>&#123;empty&#125;<br>]<br><br></code></pre></td></tr></table></figure><p>boltdb此时会插入一个新的key revision{4,0,t}，此时存储到boltdb中的key-value数据如下：</p><p><img src="https://static001.geekbang.org/resource/image/da/17/da4e5bc5033619dda296c022ac6yyc17.jpg?wh=1611*581" alt="img"></p><p><strong>那么key打上删除标记后有哪些用途呢？什么时候会真正删除它呢？</strong></p><p>一方面删除key时会生成events，Watch模块根据key的删除标识，会生成对应的Delete事件。</p><p>另一方面，当你重启etcd，遍历boltdb中的key构建treeIndex内存树时，你需要知道哪些key是已经被删除的，并为对应的key索引生成tombstone标识。而真正删除treeIndex中的索引对象、boltdb中的key是通过压缩(compactor)组件异步完成。</p><p>正因为etcd的删除key操作是基于以上延期删除原理实现的，因此只要压缩组件未回收历史版本，我们就能从etcd中找回误删的数据。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>treeIndex模块基于Google开源的btree库实现，它的核心数据结构keyIndex，保存了用户key与版本号关系。每次修改key都会生成新的版本号，生成新的boltdb key-value。boltdb的key为版本号，value包含用户key-value、各种版本号、lease的mvccpb.KeyValue结构体。</p><p>当未带版本号查询key时，etcd返回的是key最新版本数据。当你指定版本号读取数据时，etcd实际上返回的是版本号生成那个时间点的快照数据。</p><p>删除一个数据时，etcd并未真正删除它，而是基于lazy delete实现的异步删除。删除原理本质上与更新操作类似，只不过boltdb的key会打上删除标记，keyIndex索引中追加空的generation。真正删除key是通过etcd的压缩组件去异步实现的，在后面的课程里我会继续和你深入介绍。</p><p>基于以上原理特性的实现，etcd实现了保存key历史版本的功能，是高可靠Watch机制的基础。基于key-value中的各种版本号信息，etcd可提供各种级别的简易事务隔离能力。基于Backend&#x2F;boltdb提供的MVCC机制，etcd可实现读写不冲突。</p><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><p>你认为etcd为什么删除使用lazy delete方式呢？ 相比同步delete,各有什么优缺点？当你突然删除大量key后，db大小是立刻增加还是减少呢？</p><p>1是为了保证key对应的watcher能够获取到key的所有状态信息，留给watcher时间做相应的处理。2是实时从boltdb删除key，会可能触发树的不平衡，影响其他读写请求的性能。3.增大。</p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>06.租约：如何检测你的客户端存活？</title>
    <link href="/2022/10/04/06-%E7%A7%9F%E7%BA%A6%EF%BC%9A%E5%A6%82%E4%BD%95%E6%A3%80%E6%B5%8B%E4%BD%A0%E7%9A%84%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AD%98%E6%B4%BB%EF%BC%9F/"/>
    <url>/2022/10/04/06-%E7%A7%9F%E7%BA%A6%EF%BC%9A%E5%A6%82%E4%BD%95%E6%A3%80%E6%B5%8B%E4%BD%A0%E7%9A%84%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AD%98%E6%B4%BB%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="06-租约：如何检测你的客户端存活？"><a href="#06-租约：如何检测你的客户端存活？" class="headerlink" title="06.租约：如何检测你的客户端存活？"></a>06.租约：如何检测你的客户端存活？</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><p>etcd的一个典型的应用场景是Leader选举，那么etcd为什么可以用来实现Leader选举？核心特性实现原理又是怎样的？</p><h2 id="什么是Lease"><a href="#什么是Lease" class="headerlink" title="什么是Lease"></a>什么是Lease</h2><p>在实际业务场景中，常常会遇到类似Kubernetes的调度器、控制器组件同一时刻只能存在一个副本对外提供服务的情况。然而单副本部署的组件，是无法保证其高可用性的。</p><p>那为了解决单副本的可用性问题，我们就需要多副本部署。同时，为了保证同一时刻只有一个能对外提供服务，我们需要引入Leader选举机制。那么Leader选举本质是要解决什么问题呢？</p><p>首先当然是要保证Leader的唯一性，确保集群不出现多个Leader，才能保证业务逻辑准确性，也就是安全性（Safety）、互斥性。</p><p>其次是主节点故障后，备节点应可快速感知到其异常，也就是<strong>活性（liveness）检测</strong>。实现活性检测主要有两种方案。</p><p>方案一为<strong>被动型检测</strong>，你可以通过探测节点定时拨测Leader节点，看是否健康，比如Redis Sentinel。</p><p>方案二为<strong>主动型上报</strong>，Leader节点可定期向协调服务发送”特殊心跳”汇报健康状态，若其未正常发送心跳，并超过和协调服务约定的最大存活时间后，就会被协调服务移除Leader身份标识。同时其他节点可通过协调服务，快速感知到Leader故障了，进而发起新的选举。</p><p>我们文章的主题，<strong>Lease，正是基于主动型上报模式</strong>， <strong>提供的一种活性检测机制</strong>。Lease顾名思义，client和etcd server之间存在一个约定，内容是etcd server保证在约定的有效期内（TTL），不会删除你关联到此Lease上的key-value。</p><p>若未在有效期内续租，那么etcd server就会删除Lease和其关联的key-value。</p><p>你可以基于Lease的TTL特性，解决类似Leader选举、Kubernetes Event自动淘汰、服务发现场景中故障节点自动剔除等问题。为了帮助理解Lease的核心特性原理，以一个实际场景中的经常遇到的异常节点自动剔除为案例，围绕这个问题，深入介绍Lease特性的实现。</p><p>在这个案例中，期望的效果是，在节点异常时，表示节点健康的key能被从etcd集群中自动删除。</p><h2 id="Lease整体架构"><a href="#Lease整体架构" class="headerlink" title="Lease整体架构"></a>Lease整体架构</h2><p>Lease模块简要架构图。</p><p><img src="https://static001.geekbang.org/resource/image/ac/7c/ac70641fa3d41c2dac31dbb551394b7c.png?wh=2464*1552" alt="img"></p><p>etcd在启动的时候，创建Lessor模块的时候，它会启动两个常驻goroutine，如上图所示，一个是RevokeExpiredLease任务，定时检查是否有过期Lease，发起撤销过期的Lease操作。一个是CheckpointScheduledLease，定时触发更新Lease的剩余到期时间的操作。</p><p>Lessor模块提供了Grant、Revoke、LeaseTimeToLive、LeaseKeepAlive API给client使用，各接口作用如下:</p><ul><li>Grant表示创建一个TTL为你指定秒数的Lease，Lessor会将Lease信息持久化存储在boltdb中；</li><li>Revoke表示撤销Lease并删除其关联的数据；</li><li>LeaseTimeToLive表示获取一个Lease的有效期、剩余时间；</li><li>LeaseKeepAlive表示为Lease续期。</li></ul><h2 id="key如何关联Lease"><a href="#key如何关联Lease" class="headerlink" title="key如何关联Lease"></a>key如何关联Lease</h2><p>如何基于Lease特性实现检测一个节点存活。</p><p>首先如何为节点健康指标创建一个租约、并与节点健康指标key关联呢?</p><p>如KV模块的一样，client可通过clientv3库的Lease API发起RPC调用，可以使用如下的etcdctl命令为node的健康状态指标，创建一个Lease，有效期为600秒。然后通过timetolive命令，查看Lease的有效期、剩余时间。</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-comment"># 创建一个TTL为600秒的lease，etcd server返回LeaseID</span><br><span class="hljs-variable">$ </span>etcdctl lease grant <span class="hljs-number">600</span><br>lease <span class="hljs-number">326975935</span>f48f814 granted <span class="hljs-keyword">with</span> TTL(<span class="hljs-number">600</span>s)<br><br><span class="hljs-comment"># 查看lease的TTL、剩余时间</span><br><span class="hljs-variable">$ </span>etcdctl lease timetolive <span class="hljs-number">326975935</span>f48f814<br>lease <span class="hljs-number">326975935</span>f48f814 granted <span class="hljs-keyword">with</span> TTL(<span class="hljs-number">600</span>s)， remaining(<span class="hljs-number">590</span>s)<br><br></code></pre></td></tr></table></figure><p>当Lease server收到client的创建一个有效期600秒的Lease请求后，会通过Raft模块完成日志同步，随后Apply模块通过Lessor模块的Grant接口执行日志条目内容。</p><p>首先Lessor的Grant接口会把Lease保存到内存的ItemMap数据结构中，然后它需要持久化Lease，将Lease数据保存到boltdb的Lease bucket中，返回一个唯一的LeaseID给client。</p><p>通过这样一个流程，就基本完成了Lease的创建。那么节点的健康指标数据如何关联到此Lease上呢？</p><p>很简单，KV模块的API接口提供了一个”–lease”参数，你可以通过如下命令，将key node关联到对应的LeaseID上。然后你查询的时候增加-w参数输出格式为json，就可查看到key关联的LeaseID。</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">$ etcdctl put <span class="hljs-keyword">node</span> <span class="hljs-title">healthy</span> --lease <span class="hljs-number">326975935</span>f48f818<br>OK<br>$ etcdctl get <span class="hljs-keyword">node</span> <span class="hljs-title">-w</span>=json | python -m json.tool<br>&#123;<br>    <span class="hljs-string">&quot;kvs&quot;</span>:[<br>        &#123;<br>            <span class="hljs-string">&quot;create_revision&quot;</span>:<span class="hljs-number">24</span>，<br>            <span class="hljs-string">&quot;key&quot;</span>:<span class="hljs-string">&quot;bm9kZQ==&quot;</span>，<br>            <span class="hljs-string">&quot;Lease&quot;</span>:<span class="hljs-number">3632563850270275608</span>，<br>            <span class="hljs-string">&quot;mod_revision&quot;</span>:<span class="hljs-number">24</span>，<br>            <span class="hljs-string">&quot;value&quot;</span>:<span class="hljs-string">&quot;aGVhbHRoeQ==&quot;</span>，<br>            <span class="hljs-string">&quot;version&quot;</span>:<span class="hljs-number">1</span><br>        &#125;<br>    ]<br>&#125;<br><br></code></pre></td></tr></table></figure><p>以上流程原理如下图所示，它描述了用户的key是如何与指定Lease关联的。当你通过put等命令新增一个指定了”–lease”的key时，MVCC模块它会通过Lessor模块的Attach方法，将key关联到Lease的key内存集合ItemSet中。</p><p><img src="https://static001.geekbang.org/resource/image/aa/ee/aaf8bf5c3841a641f8c51fcc34ac67ee.png?wh=2024*1450" alt="img"></p><p>一个Lease关联的key集合是保存在内存中的，那么etcd重启时，是如何知道每个Lease上关联了哪些key呢?</p><p>答案是etcd的MVCC模块在持久化存储key-value的时候，保存到boltdb的value是个结构体（mvccpb.KeyValue）， 它不仅包含你的key-value数据，还包含了关联的LeaseID等信息。因此当etcd重启时，可根据此信息，重建关联各个Lease的key集合列表。</p><h2 id="如何优化Lease续期性能"><a href="#如何优化Lease续期性能" class="headerlink" title="如何优化Lease续期性能"></a>如何优化Lease续期性能</h2><p>通过以上流程，我们完成了Lease创建和数据关联操作。在正常情况下，你的节点存活时，需要定期发送KeepAlive请求给etcd续期健康状态的Lease，否则你的Lease和关联的数据就会被删除。</p><p>那么Lease是如何续期的? 作为一个高频率的请求API，etcd如何优化Lease续期的性能呢？</p><p>Lease续期其实很简单，核心是将Lease的过期时间更新为当前系统时间加其TTL。关键问题在于续期的性能能否满足业务诉求。</p><p>然而影响续期性能因素又是源自多方面的。首先是TTL，TTL过长会导致节点异常后，无法及时从etcd中删除，影响服务可用性，而过短，则要求client频繁发送续期请求。其次是Lease数，如果Lease成千上万个，那么etcd可能无法支撑如此大规模的Lease数，导致高负载。</p><p>如何解决呢？</p><p>首先我们回顾下早期etcd v2版本是如何实现TTL特性的。在早期v2版本中，没有Lease概念，TTL属性是在key上面，为了保证key不删除，即便你的TTL相同，client也需要为每个TTL、key创建一个HTTP&#x2F;1.x 连接，定时发送续期请求给etcd server。</p><p>很显然，v2老版本这种设计，因不支持连接多路复用、相同TTL无法复用导致性能较差，无法支撑较大规模的Lease场景。</p><p>etcd v3版本为了解决以上问题，提出了Lease特性，TTL属性转移到了Lease上， 同时协议从HTTP&#x2F;1.x优化成gRPC协议。</p><p>一方面不同key若TTL相同，可复用同一个Lease， 显著减少了Lease数。另一方面，通过gRPC HTTP&#x2F;2实现了多路复用，流式传输，同一连接可支持为多个Lease续期，大大减少了连接数。</p><p>通过以上两个优化，实现Lease性能大幅提升，满足了各个业务场景诉求。</p><h2 id="如何高效淘汰过期Lease"><a href="#如何高效淘汰过期Lease" class="headerlink" title="如何高效淘汰过期Lease"></a>如何高效淘汰过期Lease</h2><p>在了解完节点正常情况下的Lease续期特性后，再看看节点异常时，未正常续期后，etcd又是如何淘汰过期Lease、删除节点健康指标key的。</p><p>淘汰过期Lease的工作由Lessor模块的一个异步goroutine负责。如下面架构图虚线框所示，它会定时从最小堆中取出已过期的Lease，执行删除Lease和其关联的key列表数据的RevokeExpiredLease任务。</p><p><img src="https://static001.geekbang.org/resource/image/b0/6b/b09e9d30157876b031ed206391698c6b.png?wh=2552*1550" alt="img"></p><p>从图中可以看到，目前etcd是基于最小堆来管理Lease，实现快速淘汰过期的Lease。</p><p>etcd早期的时候，淘汰Lease非常暴力。etcd会直接遍历所有Lease，逐个检查Lease是否过期，过期则从Lease关联的key集合中，取出key列表，删除它们，时间复杂度是O(N)。</p><p>然而这种方案随着Lease数增大，毫无疑问它的性能会变得越来越差。我们能否按过期时间排序呢？这样每次只需轮询、检查排在前面的Lease过期时间，一旦轮询到未过期的Lease， 则可结束本轮检查。</p><p>刚刚说的就是etcd Lease高效淘汰方案最小堆的实现方法。每次新增Lease、续期的时候，它会插入、更新一个对象到最小堆中，对象含有LeaseID和其到期时间unixnano，对象之间按到期时间升序排序。</p><p>etcd Lessor主循环每隔500ms执行一次撤销Lease检查（RevokeExpiredLease），每次轮询堆顶的元素，若已过期则加入到待淘汰列表，直到堆顶的Lease过期时间大于当前，则结束本轮轮询。</p><p>相比早期O(N)的遍历时间复杂度，使用堆后，插入、更新、删除，它的时间复杂度是O(Log N)，查询堆顶对象是否过期时间复杂度仅为O(1)，性能大大提升，可支撑大规模场景下Lease的高效淘汰。</p><p>获取到待过期的LeaseID后，Leader是如何通知其他Follower节点淘汰它们呢？</p><p>Lessor模块会将已确认过期的LeaseID，保存在一个名为expiredC的channel中，而etcd server的主循环会定期从channel中获取LeaseID，发起revoke请求，通过Raft Log传递给Follower节点。</p><p>各个节点收到revoke Lease请求后，获取关联到此Lease上的key列表，从boltdb中删除key，从Lessor的Lease map内存中删除此Lease对象，最后还需要从boltdb的Lease bucket中删除这个Lease。</p><p>以上就是Lease的过期自动淘汰逻辑。Leader节点按过期时间维护了一个最小堆，若你的节点异常未正常续期，那么随着时间消逝，对应的Lease则会过期，Lessor主循环定时轮询过期的Lease。获取到ID后，Leader发起revoke操作，通知整个集群删除Lease和关联的数据。</p><h2 id="为什么需要checkpoint机制"><a href="#为什么需要checkpoint机制" class="headerlink" title="为什么需要checkpoint机制"></a>为什么需要checkpoint机制</h2><p>了解完Lease的创建、续期、自动淘汰机制后，可能已经发现，检查Lease是否过期、维护最小堆、针对过期的Lease发起revoke操作，都是Leader节点负责的，它类似于Lease的仲裁者，通过以上清晰的权责划分，降低了Lease特性的实现复杂度。</p><p>那么当Leader因重启、crash、磁盘IO等异常不可用时，Follower节点就会发起Leader选举，新Leader要完成以上职责，必须重建Lease过期最小堆等管理数据结构，那么以上重建可能会触发什么问题呢？</p><p>当你的集群发生Leader切换后，新的Leader基于Lease map信息，按Lease过期时间构建一个最小堆时，etcd早期版本为了优化性能，并未持久化存储Lease剩余TTL信息，因此重建的时候就会自动给所有Lease自动续期了。</p><p>然而若较频繁出现Leader切换，切换时间小于Lease的TTL，这会导致Lease永远无法删除，大量key堆积，db大小超过配额等异常。</p><p>为了解决这个问题，etcd引入了检查点机制，也就是下面架构图中黑色虚线框所示的CheckPointScheduledLeases的任务。</p><p><img src="https://static001.geekbang.org/resource/image/70/59/70ece2fa3bc400edd8d3b09f752ea759.png?wh=2580*1560" alt="img"></p><p>一方面，etcd启动的时候，Leader节点后台会运行此异步任务，定期批量地将Lease剩余的TTL基于Raft Log同步给Follower节点，Follower节点收到CheckPoint请求后，更新内存数据结构LeaseMap的剩余TTL信息。</p><p>另一方面，当Leader节点收到KeepAlive请求的时候，它也会通过checkpoint机制把此Lease的剩余TTL重置，并同步给Follower节点，尽量确保续期后集群各个节点的Lease 剩余TTL一致性。</p><p>最后要注意的是，此特性对性能有一定影响，目前仍然是试验特性。可以通过<strong>experimental-enable-lease-checkpoint</strong>参数开启。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Lease的核心是TTL，当Lease的TTL过期时，它会自动删除其关联的key-value数据。</p><p>首先是Lease创建及续期。当你创建Lease时，etcd会保存Lease信息到boltdb的Lease bucket中。为了防止Lease被淘汰，你需要定期发送LeaseKeepAlive请求给etcd server续期Lease，本质是更新Lease的到期时间。</p><p>续期的核心挑战是性能，etcd经历了从TTL属性在key上，到独立抽象出Lease，支持多key复用相同TTL，同时协议从HTTP&#x2F;1.x优化成gRPC协议，支持多路连接复用，显著降低了server连接数等资源开销。</p><p>其次是Lease的淘汰机制，etcd的Lease淘汰算法经历了从时间复杂度O(N)到O(Log N)的演进，核心是轮询最小堆的Lease是否过期，若过期生成revoke请求，它会清理Lease和其关联的数据。</p><p>最后介绍了Lease的checkpoint机制，它是为了解决Leader异常情况下TTL自动被续期，可能导致Lease永不淘汰的问题而诞生。</p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>05.鉴权：如何保护你的数据安全？</title>
    <link href="/2022/10/03/05-%E9%89%B4%E6%9D%83%EF%BC%9A%E5%A6%82%E4%BD%95%E4%BF%9D%E6%8A%A4%E4%BD%A0%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AE%89%E5%85%A8%EF%BC%9F/"/>
    <url>/2022/10/03/05-%E9%89%B4%E6%9D%83%EF%BC%9A%E5%A6%82%E4%BD%95%E4%BF%9D%E6%8A%A4%E4%BD%A0%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AE%89%E5%85%A8%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="05-鉴权：如何保护你的数据安全？"><a href="#05-鉴权：如何保护你的数据安全？" class="headerlink" title="05. 鉴权：如何保护你的数据安全？"></a>05. 鉴权：如何保护你的数据安全？</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><p>当使用etcd存储业务敏感数据、多租户共享使用同etcd集群的时候，应该如何防止匿名用户访问你的etcd数据呢？多租户场景又如何最小化用户权限分配，防止越权访问的？</p><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p>etcd鉴权体系架构由控制面和数据面组成。</p><p><img src="https://static001.geekbang.org/resource/image/30/4e/304257ac790aeda91616bfe42800364e.png?wh=1920*420" alt="img"></p><p>上图是是etcd鉴权体系控制面，可以通过客户端工具etcdctl和鉴权API动态调整认证、鉴权规则，AuthServer收到请求后，为了确保各节点间鉴权元数据一致性，会通过Raft模块进行数据同步。</p><p>当对应的Raft日志条目被集群半数以上节点确认后，Apply模块通过鉴权存储(AuthStore)模块，执行日志条目的内容，将规则存储到boltdb的一系列“鉴权表”里面。</p><p>下图是数据面鉴权流程，由认证和授权流程组成。认证的目的是检查client的身份是否合法、防止匿名用户访问等。目前etcd实现了两种认证机制，分别是密码认证和证书认证。</p><p><img src="https://static001.geekbang.org/resource/image/2c/55/2c8f90fd1a30fab9b9a88ba18c24c555.png?wh=1920*1136" alt="img"></p><p>认证通过后，为了提高密码认证性能，会分配一个Token（类似我们生活中的门票、通信证）给client，client后续其他请求携带此Token，server就可快速完成client的身份校验工作。</p><p>实现分配Token的服务也有多种，这是TokenProvider所负责的，目前支持SimpleToken和JWT两种。</p><p>通过认证后，在访问MVCC模块之前，还需要通过授权流程。授权的目的是检查client是否有权限操作你请求的数据路径，etcd实现了RBAC机制，支持为每个用户分配一个角色，为每个角色授予最小化的权限。</p><p><img src="https://static001.geekbang.org/resource/image/8d/8a/8d18f8877ea7c8fbyybebae236a8688a.png?wh=1920*1125" alt="img"></p><p>好了，etcd鉴权体系的整个流程讲完了，下面就以 put hello命令为例，给你深入分析以上鉴权体系是如何进行身份认证来防止匿名访问的，又是如何实现细粒度的权限控制以防止越权访问的。</p><h2 id="认证"><a href="#认证" class="headerlink" title="认证"></a>认证</h2><p>首先我们来看第一个问题，如何防止匿名用户访问你的etcd数据呢？</p><p>解决方案当然是认证用户身份。那etcd提供了哪些机制来验证client身份呢?</p><p>正如我整体架构中给你介绍的，etcd目前实现了两种机制，分别是用户密码认证和证书认证，下面我分别给你介绍这两种机制在etcd中如何实现，以及这两种机制各自的优缺点。</p><h3 id="密码认证"><a href="#密码认证" class="headerlink" title="密码认证"></a>密码认证</h3><p>首先我们来讲讲用户密码认证。etcd支持为每个用户分配一个账号名称、密码。密码认证在我们生活中无处不在，从银行卡取款到微信、微博app登录，再到核武器发射，密码认证应用及其广泛，是最基础的鉴权的方式。</p><p>但密码认证存在两大难点，它们分别是如何保障密码安全性和提升密码认证性能。</p><h4 id="如何保障密码安全性"><a href="#如何保障密码安全性" class="headerlink" title="如何保障密码安全性"></a>如何保障密码安全性</h4><p>首先来看第一个难点：如何保障密码安全性。</p><p>收到用户鉴权请求的时候，检查用户请求中密码与存储中是否一样，不就可以了吗？ 这种方案的确够简单，但若存储密码的文件万一被黑客脱库了，那么所有用户的密码都将被泄露，进而可能会导致重大数据泄露事故。</p><p>也许可以奇思妙想构建一个加密算法?然后将密码翻译下，比如将密码中的每个字符按照字母表序替换成字母后的第XX个字母。然而这种加密算法，它是可逆的，一旦被黑客识别到规律，还原出你的密码后，脱库后也将导致全部账号数据泄密。</p><p>那么是否用一种不可逆的加密算法就行了呢？比如常见的MD5，SHA-1，这方案听起来似乎有点道理，然而还是不严谨，因为它们的计算速度非常快，黑客可以通过暴力枚举、字典、彩虹表等手段，快速将你的密码全部破解。</p><p>LinkedIn在2012年的时候650万用户密码被泄露，黑客3天就暴力破解出90%用户的密码，原因就是LinkedIn仅仅使用了SHA-1加密算法。</p><p><strong>那应该如何进一步增强不可逆hash算法的破解难度？</strong></p><p>一方面可以使用安全性更高的hash算法，比如SHA-256，它输出位数更多、计算更加复杂且耗CPU。</p><p>另一方面可以在每个用户密码hash值的计算过程中，引入一个随机、较长的加盐(salt)参数，它可以让相同的密码输出不同的结果，这让彩虹表破解直接失效。</p><p>彩虹表是黑客破解密码的一种方法之一，它预加载了常用密码使用MD5&#x2F;SHA-1计算的hash值，可通过hash值匹配快速破解你的密码。</p><p>最后还可以增加密码hash值计算过程中的开销，比如循环迭代更多次，增加破解的时间成本。</p><p><strong>etcd的鉴权模块如何安全存储用户密码？</strong></p><p>etcd的用户密码存储正是融合了以上讨论的高安全性hash函数（Blowfish encryption algorithm）、随机的加盐salt、可自定义的hash值计算迭代次数cost。</p><p>下面通过几个简单etcd鉴权API，介绍密码认证的原理。</p><p>首先你可以通过如下的auth enable命令开启鉴权，注意etcd会先要求你创建一个root账号，它拥有集群的最高读写权限。</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">$ etcdctl <span class="hljs-keyword">user</span> <span class="hljs-title">add</span> root:root<br><span class="hljs-keyword">User</span> <span class="hljs-title">root</span> created<br>$ etcdctl auth enable<br>Authentication Enabled<br><br></code></pre></td></tr></table></figure><p>启用鉴权后，这时client发起如下put hello操作时， etcd server会返回”user name is empty”错误给client，就初步达到了防止匿名用户访问你的etcd数据目的。 那么etcd server是在哪里做的鉴权的呢?</p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs subunit">$ etcdctl put hello world<br><span class="hljs-keyword">Error: </span>etcdserver: user name is empty<br><br></code></pre></td></tr></table></figure><p>etcd server收到put hello请求的时候，在提交到Raft模块前，它会从你请求的上下文中获取你的用户身份信息。如果你未通过认证，那么在状态机应用put命令的时候，检查身份权限的时候发现是空，就会返回此错误给client。</p><p>下面我通过鉴权模块的user命令，给etcd增加一个alice账号。我们一起来看看etcd鉴权模块是如何基于我上面介绍的技术方案，来安全存储alice账号信息。</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">$ etcdctl <span class="hljs-keyword">user</span> <span class="hljs-title">add</span> alice:alice --<span class="hljs-keyword">user</span> <span class="hljs-title">root</span>:root<br><span class="hljs-keyword">User</span> <span class="hljs-title">alice</span> created<br><br></code></pre></td></tr></table></figure><p>鉴权模块收到此命令后，它会使用bcrpt库的blowfish算法，基于明文密码、随机分配的salt、自定义的cost、迭代多次计算得到一个hash值，并将加密算法版本、salt值、cost、hash值组成一个字符串，作为加密后的密码。</p><p>最后，鉴权模块将用户名alice作为key，用户名、加密后的密码作为value，存储到boltdb的authUsers bucket里面，完成一个账号创建。</p><p>当你使用alice账号访问etcd的时候，你需要先调用鉴权模块的Authenticate接口，它会验证你的身份合法性。</p><p>那么etcd如何验证你密码正确性的呢？</p><p>鉴权模块首先会根据你请求的用户名alice，从boltdb获取加密后的密码，因此hash值包含了算法版本、salt、cost等信息，因此可以根据你请求中的明文密码，计算出最终的hash值，若计算结果与存储一致，那么身份校验通过。</p><h4 id="如何提升密码认证性能"><a href="#如何提升密码认证性能" class="headerlink" title="如何提升密码认证性能"></a>如何提升密码认证性能</h4><p>通过以上的鉴权安全性的深入分析，我们知道身份验证这个过程开销极其昂贵，那么问题来了，如何避免频繁、昂贵的密码计算匹配，提升密码认证的性能呢？</p><p>这就是密码认证的第二个难点，如何保证性能。</p><p>想想我们办理港澳通行证的时候，流程特别复杂，需要各种身份证明、照片、指纹信息，办理成功后，下发通信证，每次过关你只需要刷下通信证即可，高效而便捷。</p><p>那么，在软件系统领域如果身份验证通过了后，我们是否也可以返回一个类似通信证的凭据给client，后续请求携带通信证，只要通行证合法且在有效期内，就无需再次鉴权了呢？</p><p>是的，etcd也有类似这样的凭据。当etcd server验证用户密码成功后，它就会返回一个Token字符串给client，用于表示用户的身份。后续请求携带此Token，就无需再次进行密码校验，实现了通信证的效果。</p><p>etcd目前支持两种Token，分别为Simple Token和JWT Token。</p><p><strong>Simple Token</strong></p><p>Simple Token实现正如名字所言，简单。</p><p>Simple Token的核心原理是当一个用户身份验证通过后，生成一个随机的字符串值Token返回给client，并在内存中使用map存储用户和Token映射关系。当收到用户的请求时， etcd会从请求中获取Token值，转换成对应的用户名信息，返回给下层模块使用。</p><p>Token是你身份的象征，若此Token泄露了，那你的数据就可能存在泄露的风险。etcd是如何应对这种潜在的安全风险呢？</p><p>etcd生成的每个Token，都有一个过期时间TTL属性，Token过期后client需再次验证身份，因此可显著缩小数据泄露的时间窗口，在性能上、安全性上实现平衡。</p><p>在etcd v3.4.9版本中，Token默认有效期是5分钟，etcd server会定时检查你的Token是否过期，若过期则从map数据结构中删除此Token。</p><p>不过要注意的是，Simple Token字符串本身并未含任何有价值信息，因此client无法及时、准确获取到Token过期时间。所以client不容易提前去规避因Token失效导致的请求报错。</p><p>从以上介绍中，Simple Token有哪些不足之处？为什么etcd社区仅建议在开发、测试环境中使用Simple Token呢？</p><p>首先它是有状态的，etcd server需要使用内存存储Token和用户名的映射关系。</p><p>其次，它的可描述性很弱，client无法通过Token获取到过期时间、用户名、签发者等信息。</p><p>etcd鉴权模块实现的另外一个Token Provider方案JWT，正是为了解决这些不足之处而生。</p><p><strong>JWT Token</strong></p><p>JWT是Json Web Token缩写， 它是一个基于JSON的开放标准（RFC 7519）定义的一种紧凑、独立的格式，可用于在身份提供者和服务提供者间，传递被认证的用户身份信息。它由Header、Payload、Signature三个对象组成， 每个对象都是一个JSON结构体。</p><p>第一个对象是Header，它包含alg和typ两个字段，alg表示签名的算法，etcd支持RSA、ESA、PS系列，typ表示类型就是JWT。</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs 1c">&#123;<br><span class="hljs-string">&quot;alg&quot;</span>: <span class="hljs-string">&quot;RS256&quot;</span>，<br><span class="hljs-string">&quot;typ&quot;</span>: <span class="hljs-string">&quot;JWT&quot;</span><br>&#125;<br><br></code></pre></td></tr></table></figure><p>第二对象是Payload，它表示载荷，包含用户名、过期时间等信息，可以自定义添加字段。</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">&#123;<br><span class="hljs-string">&quot;username&quot;</span>: username，<br><span class="hljs-string">&quot;revision&quot;</span>: revision，<br><span class="hljs-string">&quot;exp&quot;</span>:      time.<span class="hljs-constructor">Now()</span>.<span class="hljs-constructor">Add(<span class="hljs-params">t</span>.<span class="hljs-params">ttl</span>)</span>.<span class="hljs-constructor">Unix()</span>，<br>&#125;<br><br></code></pre></td></tr></table></figure><p>第三个对象是签名，首先它将header、payload使用base64 url编码，然后将编码后的</p><p>字符串用”.”连接在一起，最后用我们选择的签名算法比如RSA系列的私钥对其计算签名，输出结果即是Signature。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">signature=<span class="hljs-built_in">RSA256</span>(<br><span class="hljs-function"><span class="hljs-title">base64UrlEncode</span><span class="hljs-params">(header)</span></span> + <span class="hljs-string">&quot;.&quot;</span> +<br><span class="hljs-function"><span class="hljs-title">base64UrlEncode</span><span class="hljs-params">(payload)</span></span>，<br>key)<br><br></code></pre></td></tr></table></figure><p>JWT就是由base64UrlEncode(header).base64UrlEncode(payload).signature组成。</p><p>为什么说JWT是独立、紧凑的格式呢？</p><p>从以上原理介绍中我们知道，它是无状态的。JWT Token自带用户名、版本号、过期时间等描述信息，etcd server不需要保存它，client可方便、高效的获取到Token的过期时间、用户名等信息。它解决了Simple Token的若干不足之处，安全性更高，etcd社区建议大家在生产环境若使用了密码认证，应使用JWT Token( –auth-token ‘jwt’)，而不是默认的Simple Token。</p><p>介绍完密码认证实现过程中的两个核心挑战，密码存储安全和性能的解决方案之后，是否对密码认证的安全性、性能还有所担忧呢？</p><p>接下来我给你介绍etcd的另外一种高性能、更安全的鉴权方案，x509证书认证。</p><h3 id="证书认证"><a href="#证书认证" class="headerlink" title="证书认证"></a>证书认证</h3><p>密码认证一般使用在client和server基于HTTP协议通信的内网场景中。当对安全有更高要求的时候，你需要使用HTTPS协议加密通信数据，防止中间人攻击和数据被篡改等安全风险。</p><p>HTTPS是利用非对称加密实现身份认证和密钥协商，因此使用HTTPS协议的时候，你需要使用CA证书给client生成证书才能访问。</p><p>那么一个client证书包含哪些信息呢？使用证书认证的时候，etcd server如何知道你发送的请求对应的用户名称？</p><p>我们可以使用下面的openssl命令查看client证书的内容，下图是一个x509 client证书的内容，它含有证书版本、序列号、签名算法、签发者、有效期、主体名等信息，我们重点要关注的是主体名中的CN字段。</p><p>在etcd中，如果你使用了HTTPS协议并启用了client证书认证(–client-cert-auth)，它会取CN字段作为用户名，在我们的案例中，alice就是client发送请求的用户名。</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs applescript">openssl x509 -noout -<span class="hljs-built_in">text</span> -<span class="hljs-keyword">in</span> client.pem<br><br></code></pre></td></tr></table></figure><p><img src="https://static001.geekbang.org/resource/image/55/94/55e03b4353c9a467493a3922cf68b294.png?wh=1144*854" alt="img"></p><p>证书认证在稳定性、性能上都优于密码认证。</p><p>稳定性上，它不存在Token过期、使用更加方便、会让少踩坑，避免了不少Token失效而触发的Bug。性能上，证书认证无需像密码认证一样调用昂贵的密码认证操作(Authenticate请求)，此接口支持的性能极低。</p><h2 id="授权"><a href="#授权" class="headerlink" title="授权"></a>授权</h2><p>当使用如上创建的alice账号执行put hello操作的时候，etcd却会返回如下的”etcdserver: permission denied”无权限错误，这是为什么呢？</p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs subunit">$ etcdctl put hello world --user alice:alice<br><span class="hljs-keyword">Error: </span>etcdserver: permission denied<br><br></code></pre></td></tr></table></figure><p>这是因为开启鉴权后，put请求命令在应用到状态机前，etcd还会对发出此请求的用户进行权限检查， 判断其是否有权限操作请求的数据。常用的权限控制方法有ACL(Access Control List)、ABAC(Attribute-based access control)、RBAC(Role-based access control)，etcd实现的是RBAC机制。</p><h3 id="RBAC"><a href="#RBAC" class="headerlink" title="RBAC"></a>RBAC</h3><p>什么是基于角色权限的控制系统(RBAC)呢？</p><p>它由下图中的三部分组成，User、Role、Permission。User表示用户，如alice。Role表示角色，它是权限的赋予对象。Permission表示具体权限明细，比如赋予Role对key范围在[key，KeyEnd]数据拥有什么权限。目前支持三种权限，分别是READ、WRITE、READWRITE。</p><p><img src="https://static001.geekbang.org/resource/image/ee/60/ee6e0a9a63aeaa2d3505ab1a37360760.png?wh=1786*1134" alt="img"></p><p>下面我们通过etcd的RBAC机制，给alice用户赋予一个可读写[hello,helly]数据范围的读写权限， 如何操作呢?</p><p>按照上面介绍的RBAC原理，首先需要创建一个role，这里我们命名为admin，然后新增了一个可读写[hello,helly]数据范围的权限给admin角色，并将admin的角色的权限授予了用户alice。详细如下：</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">$ <span class="hljs-comment">#创建一个admin role</span><br>etcdctl <span class="hljs-keyword">role</span> <span class="hljs-title">add</span> admin  --<span class="hljs-keyword">user</span> <span class="hljs-title">root</span>:root<br><span class="hljs-keyword">Role</span> <span class="hljs-title">admin</span> created<br><span class="hljs-comment"># #分配一个可读写[hello，helly]范围数据的权限给admin role</span><br>$ etcdctl <span class="hljs-keyword">role</span> <span class="hljs-title">grant-permission</span> admin readwrite hello helly --<span class="hljs-keyword">user</span> <span class="hljs-title">root</span>:root<br><span class="hljs-keyword">Role</span> <span class="hljs-title">admin</span> updated<br><span class="hljs-comment"># 将用户alice和admin role关联起来，赋予admin权限给user</span><br>$ etcdctl <span class="hljs-keyword">user</span> <span class="hljs-title">grant-role</span> alice admin --<span class="hljs-keyword">user</span> <span class="hljs-title">root</span>:root<br><span class="hljs-keyword">Role</span> <span class="hljs-title">admin</span> is granted to <span class="hljs-keyword">user</span> <span class="hljs-title">alice</span><br><br></code></pre></td></tr></table></figure><p>然后再次使用etcdctl执行put hello命令时，鉴权模块会从boltdb查询alice用户对应的权限列表。</p><p>因为有可能一个用户拥有成百上千个权限列表，etcd为了提升权限检查的性能，引入了区间树，检查用户操作的key是否在已授权的区间，时间复杂度仅为O(logN)。</p><p>在这个案例中，很明显hello在admin角色可读写的[hello，helly)数据范围内，因此它有权限更新key hello，执行成功。也可以尝试更新key hey，因为此key未在鉴权的数据区间内，因此etcd server会返回”etcdserver: permission denied”错误给client，如下所示。</p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs subunit">$ etcdctl put hello world --user alice:alice<br>OK<br>$ etcdctl put hey hey --user alice:alice<br><span class="hljs-keyword">Error: </span>etcdserver: permission denied<br><br></code></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>设计实现一个鉴权模块最关键的目标和挑战应该是<strong>安全、性能以及一致性</strong>。</p><p>首先鉴权目的是为了保证安全，必须防止恶意用户绕过鉴权系统、伪造、篡改、越权等行为，同时设计上要有前瞻性，做到即使被拖库也影响可控。etcd的解决方案是通过密码安全加密存储、证书认证、RBAC等机制保证其安全性。</p><p>然后，鉴权作为了一个核心的前置模块，性能上不能拖后腿，不能成为影响业务性能的一个核心瓶颈。etcd的解决方案是通过Token降低频繁、昂贵的密码验证开销，可应用在内网、小规模业务场景，同时支持使用证书认证，不存在Token过期，巧妙的取CN字段作为用户名，可满足较大规模的业务场景鉴权诉求。</p><p>接着，鉴权系统面临的业务场景是复杂的，因此权限控制系统应当具备良好的扩展性，业务可根据自己实际场景选择合适的鉴权方法。etcd的Token Provider和RBAC扩展机制，都具备较好的扩展性、灵活性。尤其是RBAC机制，让你可以精细化的控制每个用户权限，实现权限最小化分配。</p><p>最后鉴权系统元数据的存储应当是可靠的，各个节点鉴权数据应确保一致，确保鉴权行为一致性。早期etcd v2版本时，因鉴权命令未经过Raft模块，存在数据不一致的问题，在etcd v3中通过Raft模块同步鉴权指令日志指令，实现鉴权数据一致性。</p><p><strong>权衡</strong></p><p><strong>密码鉴权简单易用，但是潜在隐患多，证书可能略麻烦，特别是多租户场景，每个用户证书都不一样，需要独立生成，总的而言，还是不能为一时方便偷懒选用密码</strong></p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>04.Raft协议：etcd如何实现高可用、数据强一致的？</title>
    <link href="/2022/10/03/04-Raft%E5%8D%8F%E8%AE%AE%EF%BC%9Aetcd%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8%E3%80%81%E6%95%B0%E6%8D%AE%E5%BC%BA%E4%B8%80%E8%87%B4%E7%9A%84%EF%BC%9F/"/>
    <url>/2022/10/03/04-Raft%E5%8D%8F%E8%AE%AE%EF%BC%9Aetcd%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8%E3%80%81%E6%95%B0%E6%8D%AE%E5%BC%BA%E4%B8%80%E8%87%B4%E7%9A%84%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="04-Raft协议：etcd如何实现高可用、数据强一致的？"><a href="#04-Raft协议：etcd如何实现高可用、数据强一致的？" class="headerlink" title="04.Raft协议：etcd如何实现高可用、数据强一致的？"></a>04.Raft协议：etcd如何实现高可用、数据强一致的？</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><h2 id="如何避免单点故障"><a href="#如何避免单点故障" class="headerlink" title="如何避免单点故障"></a>如何避免单点故障</h2><p>早期我们使用的数据存储服务，它们往往是部署在单节点上的。但是单节点存在单点故障，一宕机就整个服务不可用，对业务影响非常大。</p><p>随后，为了解决单点问题，软件系统工程师引入了数据复制技术，实现多副本。通过数据复制方案，一方面我们可以提高服务可用性，避免单点故障。另一方面，多副本可以提升读吞吐量、甚至就近部署在业务所在的地理位置，降低访问延迟。</p><p><strong>多副本复制是如何实现的呢？</strong></p><p>多副本常用的技术方案主要有主从复制和去中心化复制。主从复制，又分为全同步复制、异步复制、半同步复制，比如MySQL&#x2F;Redis单机主备版就基于主从复制实现的。</p><p><strong>全同步复制</strong> 是指主收到一个写请求后，必须等待全部从节点确认返回后，才能返回给客户端成功。因此如果一个从节点故障，整个系统就会不可用。这种方案为了保证多副本的一致性，而牺牲了可用性，一般使用不多。</p><p><strong>异步复制</strong> 是指主收到一个写请求后，可及时返回给client，异步将请求转发给各个副本，若还未将请求转发到副本前就故障了，则可能导致数据丢失，但是可用性是最高的。</p><p><strong>半同步复制</strong> 介于全同步复制、异步复制之间，它是指主收到一个写请求后，至少有一个副本接收数据后，就可以返回给客户端成功，在数据一致性、可用性上实现了平衡和取舍。</p><p>跟主从复制相反的就是 <strong>去中心化复制</strong>，它是指在一个n副本节点集群中，任意节点都可接受写请求，但一个成功的写入需要w个节点确认，读取也必须查询至少r个节点。</p><p>可以根据实际业务场景对数据一致性的敏感度，设置合适w&#x2F;r参数。比如你希望每次写入后，任意client都能读取到新值，如果n是3个副本，你可以将w和r设置为2，这样当你读两个节点时候，必有一个节点含有最近写入的新值，这种读我们称之为法定票数读（quorum read）。</p><p>AWS的Dynamo系统就是基于去中心化的复制算法实现的。它的优点是节点角色都是平等的，降低运维复杂度，可用性更高。但是缺陷是去中心化复制，势必会导致各种写入冲突，业务需要关注冲突处理。</p><p>从以上分析中，为了解决单点故障，从而引入了多副本。但基于复制算法实现的数据库，为了保证服务可用性，大多数提供的是最终一致性，总而言之，不管是主从复制还是异步复制，都存在一定的缺陷。</p><p><strong>如何解决以上复制算法的困境呢？</strong></p><p>答案就是共识算法，它最早是基于复制状态机背景下提出来的。 下图是复制状态机的结构（引用自Raft paper）， 它由共识模块、日志模块、状态机组成。通过共识模块保证各个节点日志的一致性，然后各个节点基于同样的日志、顺序执行指令，最终各个复制状态机的结果实现一致。</p><p><img src="https://static001.geekbang.org/resource/image/3y/eb/3yy3fbc1ab564e3af9ac9223db1435eb.png?wh=605*319" alt="img"></p><p>共识算法的祖师爷是Paxos， 但是由于它过于复杂，难于理解，工程实践上也较难落地，导致在工程界落地较慢。standford大学的Diego提出的Raft算法正是为了可理解性、易实现而诞生的，它通过问题分解，将复杂的共识问题拆分成三个子问题，分别是：</p><ul><li>Leader选举，Leader故障后集群能快速选出新Leader；</li><li>日志复制， 集群只有Leader能写入日志， Leader负责复制日志到Follower节点，并强制Follower节点与自己保持相同；</li><li>安全性，一个任期内集群只能产生一个Leader、已提交的日志条目在发生Leader选举时，一定会存在更高任期的新Leader日志中、各个节点的状态机应用的任意位置的日志条目内容应一样等。</li></ul><p>下面以实际场景为案例，深入讨论这三个子问题，看看Raft是如何解决这三个问题，以及在etcd中的应用实现。</p><h2 id="Leader选举"><a href="#Leader选举" class="headerlink" title="Leader选举"></a>Leader选举</h2><p>当etcd server收到client发起的put hello写请求后，KV模块会向Raft模块提交一个put提案，<strong>只有集群Leader才能处理写提案，</strong>如果此时集群中无Leader， 整个请求就会超时。</p><p>那么Leader是怎么诞生的呢？Leader crash之后其他节点如何竞选呢？</p><p>首先在Raft协议中它定义了集群中的如下节点状态，任何时刻，每个节点肯定处于其中一个状态：</p><ul><li>Follower，跟随者， 同步从Leader收到的日志，etcd启动的时候默认为此状态；</li><li>Candidate，竞选者，可以发起Leader选举；</li><li>Leader，集群领导者， 唯一性，拥有同步日志的特权，需定时广播心跳给Follower节点，以维持领导者身份。</li></ul><p><img src="https://static001.geekbang.org/resource/image/a5/09/a5a210eec289d8e4e363255906391009.png?wh=1808*978" alt="img"></p><p>上图是节点状态变化关系图，当Follower节点接收Leader节点心跳消息超时后，它会转变成Candidate节点，并可发起竞选Leader投票，若获得集群多数节点的支持后，它就可转变成Leader节点。</p><p>下面我以Leader crash场景为案例，给你详细介绍一下etcd Leader选举原理。</p><p>假设集群总共3个节点，A节点为Leader，B、C节点为Follower。</p><p><img src="https://static001.geekbang.org/resource/image/a2/59/a20ba5b17de79d6ce8c78a712a364359.png?wh=1920*942" alt="img"></p><p>如上Leader选举图左边部分所示， 正常情况下，Leader节点会按照心跳间隔时间，定时广播心跳消息（MsgHeartbeat消息）给Follower节点，以维持Leader身份。 Follower收到后回复心跳应答包消息（MsgHeartbeatResp消息）给Leader。</p><p>细心的你可能注意到上图中的Leader节点下方有一个任期号（term）， 它具有什么样的作用呢？</p><p>这是因为Raft将时间划分成一个个任期，任期用连续的整数表示，每个任期从一次选举开始，赢得选举的节点在该任期内充当Leader的职责，随着时间的消逝，集群可能会发生新的选举，任期号也会单调递增。</p><p>通过任期号，可以比较各个节点的数据新旧、识别过期的Leader等，它在Raft算法中充当逻辑时钟，发挥着重要作用。</p><p>了解完正常情况下Leader维持身份的原理后，我们再看异常情况下，也就Leader crash后，etcd是如何自愈的呢？</p><p>如上Leader选举图右边部分所示，当Leader节点异常后，Follower节点会接收Leader的心跳消息超时，当超时时间大于竞选超时时间后，它们会进入Candidate状态。</p><p>这里要提醒下你，etcd默认心跳间隔时间（heartbeat-interval）是100ms， 默认竞选超时时间（election timeout）是1000ms， 你需要根据实际部署环境、业务场景适当调优，否则就很可能会频繁发生Leader选举切换，导致服务稳定性下降，后面我们实践篇会再详细介绍。</p><p>进入Candidate状态的节点，会立即发起选举流程，自增任期号，投票给自己，并向其他节点发送竞选Leader投票消息（MsgVote）。</p><p>C节点收到Follower B节点竞选Leader消息后，这时候可能会出现如下两种情况：</p><ul><li>第一种情况是C节点判断B节点的数据至少和自己一样新、B节点任期号大于C当前任期号、并且C未投票给其他候选者，就可投票给B。这时B节点获得了集群多数节点支持，于是成为了新的Leader。</li><li>第二种情况是，恰好C也心跳超时超过竞选时间了，它也发起了选举，并投票给了自己，那么它将拒绝投票给B，这时谁也无法获取集群多数派支持，只能等待竞选超时，开启新一轮选举。Raft为了优化选票被瓜分导致选举失败的问题，引入了随机数，每个节点等待发起选举的时间点不一致，优雅的解决了潜在的竞选活锁，同时易于理解。</li></ul><p>Leader选出来后，它什么时候又会变成Follower状态呢？ 从上面的状态转换关系图中你可以看到，如果现有Leader发现了新的Leader任期号，那么它就需要转换到Follower节点。A节点crash后，再次启动成为Follower，假设因为网络问题无法连通B、C节点，这时候根据状态图，我们知道它将不停自增任期号，发起选举。等A节点网络异常恢复后，那么现有Leader收到了新的任期号，就会触发新一轮Leader选举，影响服务的可用性。</p><p>然而A节点的数据是远远落后B、C的，是无法获得集群Leader地位的，发起的选举无效且对集群稳定性有伤害。</p><p>那如何避免以上场景中的无效的选举呢？</p><p>在etcd 3.4中，etcd引入了一个PreVote参数（默认false），可以用来启用PreCandidate状态解决此问题，如下图所示。Follower在转换成Candidate状态前，先进入PreCandidate状态，不自增任期号， 发起预投票。若获得集群多数节点认可，确定有概率成为Leader才能进入Candidate状态，发起选举流程。</p><p><img src="https://static001.geekbang.org/resource/image/16/06/169ae84055byya38b616d2e71cfb9706.png?wh=1920*971" alt="img"></p><p>因A节点数据落后较多，预投票请求无法获得多数节点认可，因此它就不会进入Candidate状态，导致集群重新选举。</p><p>这就是Raft Leader选举核心原理，使用心跳机制维持Leader身份、触发Leader选举，etcd基于它实现了高可用，只要集群一半以上节点存活、可相互通信，Leader宕机后，就能快速选举出新的Leader，继续对外提供服务。</p><h2 id="日志复制"><a href="#日志复制" class="headerlink" title="日志复制"></a>日志复制</h2><p>假设在上面的Leader选举流程中，B成为了新的Leader，它收到put提案后，它是如何将日志同步给Follower节点的呢？ 什么时候它可以确定一个日志条目为已提交，通知etcdserver模块应用日志条目指令到状态机呢？</p><p>这就涉及到Raft日志复制原理，下面画了一幅Leader收到put请求后，向Follower节点复制日志的整体流程图，简称流程图，在图中我用序号给你标识了核心流程。</p><p>我将结合流程图、后面的Raft的日志图和你简要分析Leader B收到put hello为world的请求后，是如何将此请求同步给其他Follower节点的。</p><p><img src="https://static001.geekbang.org/resource/image/a5/83/a57a990cff7ca0254368d6351ae5b983.png?wh=1920*1327" alt="img"></p><p>首先Leader收到client的请求后，etcdserver的KV模块会向Raft模块提交一个put hello为world提案消息（流程图中的序号2流程）， 它的消息类型是MsgProp。</p><p>Leader的Raft模块获取到MsgProp提案消息后，为此提案生成一个日志条目，追加到未持久化、不稳定的Raft日志中，随后会遍历集群Follower列表和进度信息，为每个Follower生成追加（MsgApp）类型的RPC消息，此消息中包含待复制给Follower的日志条目。</p><p>这里就出现两个疑问了。第一，Leader是如何知道从哪个索引位置发送日志条目给Follower，以及Follower已复制的日志最大索引是多少呢？第二，日志条目什么时候才会追加到稳定的Raft日志中呢？Raft模块负责持久化吗？</p><p>首先我来给你介绍下什么是Raft日志。下图是Raft日志复制过程中的日志细节图，简称日志图1。</p><p>在日志图中，最上方的是日志条目序号&#x2F;索引，日志由有序号标识的一个个条目组成，每个日志条目内容保存了Leader任期号和提案内容。最开始的时候，A节点是Leader，任期号为1，A节点crash后，B节点通过选举成为新的Leader， 任期号为2。</p><p>日志图1描述的是hello日志条目未提交前的各节点Raft日志状态。</p><p><img src="https://static001.geekbang.org/resource/image/3d/87/3dd2b6042e6e0cc86f96f24764b7f587.png?wh=1920*1003" alt="img"></p><p>现在就可以来回答第一个疑问了。Leader会维护两个核心字段来追踪各个Follower的进度信息，一个字段是NextIndex， 它表示Leader发送给Follower节点的下一个日志条目索引。一个字段是MatchIndex， 它表示Follower节点已复制的最大日志条目的索引，比如上面的日志图1中C节点的已复制最大日志条目索引为5，A节点为4。</p><p>再看第二个疑问。etcd Raft模块设计实现上抽象了网络、存储、日志等模块，它本身并不会进行网络、存储相关的操作，上层应用需结合自己业务场景选择内置的模块或自定义实现网络、存储、日志等模块。</p><p>上层应用通过Raft模块的输出接口（如Ready结构），获取到待持久化的日志条目和待发送给Peer节点的消息后（如上面的MsgApp日志消息），需持久化日志条目到自定义的WAL模块，通过自定义的网络模块将消息发送给Peer节点。</p><p>日志条目持久化到稳定存储中后，这时候你就可以将日志条目追加到稳定的Raft日志中。即便这个日志是内存存储，节点重启时也不会丢失任何日志条目，因为WAL模块已持久化此日志条目，可通过它重建Raft日志。</p><p>etcd Raft模块提供了一个内置的内存存储（MemoryStorage）模块实现，etcd使用的就是它，Raft日志条目保存在内存中。网络模块并未提供内置的实现，etcd基于HTTP协议实现了peer节点间的网络通信，并根据消息类型，支持选择pipeline、stream等模式发送，显著提高了网络吞吐量、降低了延时。</p><p>解答完以上两个疑问后，我们继续分析etcd是如何与Raft模块交互，获取待持久化的日志条目和发送给peer节点的消息。</p><p>正如刚刚讲到的，Raft模块输入是Msg消息，输出是一个Ready结构，它包含待持久化的日志条目、发送给peer节点的消息、已提交的日志条目内容、线性查询结果等Raft输出核心信息。</p><p>etcdserver模块通过channel从Raft模块获取到Ready结构后（流程图中的序号3流程），因B节点是Leader，它首先会通过基于HTTP协议的网络模块将追加日志条目消息（MsgApp）广播给Follower，并同时将待持久化的日志条目持久化到WAL文件中（流程图中的序号4流程），最后将日志条目追加到稳定的Raft日志存储中（流程图中的序号5流程）。</p><p>各个Follower收到追加日志条目（MsgApp）消息，并通过安全检查后，它会持久化消息到WAL日志中，并将消息追加到Raft日志存储，随后会向Leader回复一个应答追加日志条目（MsgAppResp）的消息，告知Leader当前已复制的日志最大索引（流程图中的序号6流程）。</p><p>Leader收到应答追加日志条目（MsgAppResp）消息后，会将Follower回复的已复制日志最大索引更新到跟踪Follower进展的Match Index字段，如下面的日志图2中的Follower C MatchIndex为6，Follower A为5，日志图2描述的是hello日志条目提交后的各节点Raft日志状态。</p><p><img src="https://static001.geekbang.org/resource/image/eb/63/ebbf739a94f9300a85f21da7e55f1e63.png?wh=1920*994" alt="img"></p><p>最后Leader根据Follower的MatchIndex信息，计算出一个位置，如果这个位置已经被一半以上节点持久化，那么这个位置之前的日志条目都可以被标记为已提交。</p><p>在我们这个案例中日志图2里6号索引位置之前的日志条目已被多数节点复制，那么他们状态都可被设置为已提交。Leader可通过在发送心跳消息（MsgHeartbeat）给Follower节点时，告知它已经提交的日志索引位置。</p><p>最后各个节点的etcdserver模块，可通过channel从Raft模块获取到已提交的日志条目（流程图中的序号7流程），应用日志条目内容到存储状态机（流程图中的序号8流程），返回结果给client。</p><p>通过以上流程，Leader就完成了同步日志条目给Follower的任务，一个日志条目被确定为已提交的前提是，它需要被Leader同步到一半以上节点上。以上就是etcd Raft日志复制的核心原理。</p><h2 id="安全性"><a href="#安全性" class="headerlink" title="安全性"></a>安全性</h2><p>如果在上面的日志图2中，Leader B在应用日志指令put hello为world到状态机，并返回给client成功后，突然crash了，那么Follower A和C是否都有资格选举成为Leader呢？</p><p>从日志图2中我们可以看到，如果A成为了Leader那么就会导致数据丢失，因为它并未含有刚刚client已经写入成功的put hello为world指令。</p><p>Raft算法如何确保面对这类问题时不丢数据和各节点数据一致性呢？</p><p>这就是Raft的第三个子问题需要解决的。Raft通过给选举和日志复制增加一系列规则，来实现Raft算法的安全性。</p><h3 id="选举规则"><a href="#选举规则" class="headerlink" title="选举规则"></a>选举规则</h3><p>当节点收到选举投票的时候，需检查候选者的最后一条日志中的任期号，若小于自己则拒绝投票。如果任期号相同，日志却比自己短，也拒绝为其投票。</p><p>比如在日志图2中，Folllower A和C任期号相同，但是Follower C的数据比Follower A要长，那么在选举的时候，Follower C将拒绝投票给A， 因为它的数据不是最新的。</p><p>同时，对于一个给定的任期号，最多只会有一个leader被选举出来，leader的诞生需获得集群一半以上的节点支持。每个节点在同一个任期内只能为一个节点投票，节点需要将投票信息持久化，防止异常重启后再投票给其他节点。</p><p>通过以上规则就可防止日志图2中的Follower A节点成为Leader。</p><h3 id="日志复制规则"><a href="#日志复制规则" class="headerlink" title="日志复制规则"></a>日志复制规则</h3><p>在日志图2中，Leader B返回给client成功后若突然crash了，此时可能还并未将6号日志条目已提交的消息通知到Follower A和C，那么如何确保6号日志条目不被新Leader删除呢？ 同时在etcd集群运行过程中，Leader节点若频繁发生crash后，可能会导致Follower节点与Leader节点日志条目冲突，如何保证各个节点的同Raft日志位置含有同样的日志条目？</p><p>以上各类异常场景的安全性是通过Raft算法中的Leader完全特性和只附加原则、日志匹配等安全机制来保证的。</p><p><strong>Leader完全特性</strong> 是指如果某个日志条目在某个任期号中已经被提交，那么这个条目必然出现在更大任期号的所有Leader中。</p><p>Leader只能追加日志条目，不能删除已持久化的日志条目（ <strong>只附加原则</strong>），因此Follower C成为新Leader后，会将前任的6号日志条目复制到A节点。</p><p>为了保证各个节点日志一致性，Raft算法在追加日志的时候，引入了一致性检查。Leader在发送追加日志RPC消息时，会把新的日志条目紧接着之前的条目的索引位置和任期号包含在里面。Follower节点会检查相同索引位置的任期号是否与Leader一致，一致才能追加，这就是 <strong>日志匹配特性</strong>。它本质上是一种归纳法，一开始日志空满足匹配特性，随后每增加一个日志条目时，都要求上一个日志条目信息与Leader一致，那么最终整个日志集肯定是一致的。</p><p>通过以上的Leader选举限制、Leader完全特性、只附加原则、日志匹配等安全特性，Raft就实现了一个可严格通过数学反证法、归纳法证明的高可用、一致性算法，为etcd的安全性保驾护航。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Raft虽然诞生晚，但它却是共识算法里面在工程界应用最广泛的。它将一个复杂问题拆分成三个子问题，分别是<strong>Leader选举、日志复制和安全性。</strong></p><p><strong>Raft通过心跳机制、随机化等实现了Leader选举，只要集群半数以上节点存活可相互通信，etcd就可对外提供高可用服务。</strong></p><p>Raft日志复制确保了etcd多节点间的数据一致性，我通过一个etcd日志复制整体流程图为你详细介绍了etcd写请求从提交到Raft模块，到被应用到状态机执行的各个流程，剖析了日志复制的核心原理，即一个日志条目只有被Leader同步到一半以上节点上，此日志条目才能称之为成功复制、已提交。Raft的安全性，通过对Leader选举和日志复制增加一系列规则，保证了整个集群的一致性、完整性。</p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>03.etcd写请求的执行过程</title>
    <link href="/2022/10/02/03-etcd%E5%86%99%E8%AF%B7%E6%B1%82%E7%9A%84%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/"/>
    <url>/2022/10/02/03-etcd%E5%86%99%E8%AF%B7%E6%B1%82%E7%9A%84%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="etcd-写请求的执行过程"><a href="#etcd-写请求的执行过程" class="headerlink" title="etcd 写请求的执行过程"></a>etcd 写请求的执行过程</h2><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><p>etcd 一个写请求执行流程又是怎样的呢？在执行写请求过程中，如果进程 crash 了，如何保证数据不丢、命令不重复执行呢？</p><h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><p><img src="https://static001.geekbang.org/resource/image/8b/72/8b6dfa84bf8291369ea1803387906c72.png?wh=1920*1265" alt="img"></p><p>为了能够更直观地理解 etcd 的写请求流程，在如上的架构图中，用序号标识了下面的一个 put hello 为 world 的写请求的简要执行流程，从整体上快速了解一个写请求的全貌。</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs applescript">etcdctl <span class="hljs-keyword">put</span> hello world <span class="hljs-comment">--endpoints http://127.0.0.1:2379</span><br>OK<br></code></pre></td></tr></table></figure><p>首先 client 端通过负载均衡算法选择一个 etcd 节点，发起 gRPC 调用。然后 etcd 节点收到请求后经过 gRPC 拦截器、Quota 模块后，进入 KVServer 模块，KVServer 模块向 Raft 模块提交一个提案，提案内容为“大家好，请使用 put 方法执行一个 key 为 hello，value 为 world 的命令”。</p><p>随后此提案通过 Raft HTTP 网络模块转发、经过集群多数节点持久化后，状态会变成已提交，etcdserver 从 Raft 模块获取已提交的日志条目，传递给 Apply 模块，Apply 模块通过 MVCC 模块执行提案内容，更新状态机。</p><p>与读流程不一样的是写流程还涉及 Quota、WAL、Apply 三个模块。crash-safe 及幂等性也正是基于 WAL 和 Apply 流程的 consistent index 等实现的</p><p>下面就让我们沿着写请求执行流程图，从 0 到 1 分析一个 key-value 是如何安全、幂等地持久化到磁盘的。</p><h3 id="Quota-模块"><a href="#Quota-模块" class="headerlink" title="Quota 模块"></a>Quota 模块</h3><p>首先是流程一 client 端发起 gRPC 调用到 etcd 节点，和读请求不一样的是，写请求需要经过流程二 db 配额（Quota）模块，它有什么功能呢？</p><p>先从此模块的一个常见错误说起，你在使用 etcd 过程中是否遇到过”etcdserver: mvcc: database space exceeded”错误呢？</p><p>它是指当前 etcd db 文件大小超过了配额，当出现此错误后，整个集群将不可写入，只读，对业务的影响非常大。</p><p><strong>哪些情况会触发这个错误呢？</strong></p><p>一方面默认 db 配额仅为 2G，当业务数据、写入 QPS、Kubernetes 集群规模增大后， etcd db 大小就可能会超过 2G。</p><p>另一方面我们知道 etcd v3 是个 MVCC 数据库，保存了 key 的历史版本，当你未配置压缩策略的时候，随着数据不断写入，db 大小会不断增大，导致超限。</p><p>最后要特别注意的是，如果使用的是 etcd 3.2.10 之前的旧版本，请注意备份可能会触发 boltdb 的一个 Bug，它会导致 db 大小不断上涨，最终达到配额限制。</p><p><strong>了解完触发 Quota 限制的原因后，再详细了解下 Quota 模块它是如何工作的。</strong></p><p>当 etcd server 收到 put&#x2F;txn 等写请求的时候，会首先检查下当前 etcd db 大小加上你请求的 key-value 大小之和是否超过了配额（quota-backend-bytes）。</p><p>如果超过了配额，它会产生一个告警（Alarm）请求，告警类型是 NO SPACE，并通过 Raft 日志同步给其它节点，告知 db 无空间了，并将告警持久化存储到 db 中。</p><p>最终，无论是 API 层 gRPC 模块还是负责将 Raft 侧已提交的日志条目应用到状态机的 Apply 模块，都拒绝写入，集群只读。</p><p><strong>那遇到这个错误时应该如何解决呢？</strong></p><p>首先当然是<strong>调大配额</strong>。具体多大合适呢？etcd 社区建议不超过 8G。遇到过这个错误的你是否还记得，为什么当你把配额（quota-backend-bytes）调大后，集群依然拒绝写入呢?</p><p>原因就是我们前面提到的 NO SPACE 告警。Apply 模块在执行每个命令的时候，都会去检查当前是否存在 NO SPACE 告警，如果有则拒绝写入。所以还需要你额外发送一个取消告警（etcdctl alarm disarm）的命令，以消除所有告警。</p><p>其次你需要<strong>检查 etcd 的压缩（compact）配置是否开启、配置是否合理</strong>。etcd 保存了一个 key 所有变更历史版本，如果没有一个机制去回收旧的版本，那么内存和 db 大小就会一直膨胀，在 etcd 里面，压缩模块负责回收旧版本的工作。</p><p>压缩模块支持按多种方式回收旧版本，比如保留最近一段时间内的历史版本。不过要注意，它仅仅是将旧版本占用的空间打个空闲（Free）标记，后续新的数据写入的时候可复用这块空间，而无需申请新的空间。</p><p>如果你需要回收空间，减少 db 大小，得使用碎片整理（defrag）， 它会遍历旧的 db 文件数据，写入到一个新的 db 文件。但是它对服务性能有较大影响，不建议你在生产集群频繁使用。</p><p>最后你需要<strong>注意配额（quota-backend-bytes）的行为</strong>，默认’0’就是使用 etcd 默认的 2GB 大小，你需要根据你的业务场景适当调优。如果你填的是个小于 0 的数，就会禁用配额功能，这可能会让你的 db 大小处于失控，导致性能下降，不建议你禁用配额。</p><h3 id="KVServer-模块"><a href="#KVServer-模块" class="headerlink" title="KVServer 模块"></a>KVServer 模块</h3><p>通过流程二的配额检查后，请求就从 API 层转发到了流程三的 KVServer 模块的 put 方法，我们知道 etcd 是基于 Raft 算法实现节点间数据复制的，因此它需要将 put 写请求内容打包成一个提案消息，提交给 Raft 模块。不过 KVServer 模块在提交提案前，还有如下的一系列检查和限速。</p><h4 id="Preflight-Check"><a href="#Preflight-Check" class="headerlink" title="Preflight Check"></a>Preflight Check</h4><p>为了保证集群稳定性，避免雪崩，任何提交到 Raft 模块的请求，都会做一些简单的限速判断。如下面的流程图所示，首先，如果 Raft 模块已提交的日志索引（committed index）比已应用到状态机的日志索引（applied index）超过了 5000，那么它就返回一个”etcdserver: too many requests”错误给 client。</p><p><img src="https://static001.geekbang.org/resource/image/dc/54/dc8e373e06f2ab5f63a7948c4a6c8554.png?wh=1164*1004" alt="img"></p><p>然后它会尝试去获取请求中的鉴权信息，若使用了密码鉴权、请求中携带了 token，如果 token 无效，则返回”auth: invalid auth token”错误给 client。</p><p>其次它会检查你写入的包大小是否超过默认的 1.5MB， 如果超过了会返回”etcdserver: request is too large”错误给给 client。</p><h4 id="Propose"><a href="#Propose" class="headerlink" title="Propose"></a>Propose</h4><p>最后通过一系列检查之后，会生成一个唯一的 ID，将此请求关联到一个对应的消息通知 channel，然后向 Raft 模块发起（Propose）一个提案（Proposal），提案内容为“大家好，请使用 put 方法执行一个 key 为 hello，value 为 world 的命令”，也就是整体架构图里的流程四。</p><p>向 Raft 模块发起提案后，KVServer 模块会等待此 put 请求，等待写入结果通过消息通知 channel 返回或者超时。etcd 默认超时时间是 7 秒（5 秒磁盘 IO 延时 +2*1 秒竞选超时时间），如果一个请求超时未返回结果，则可能会出现你熟悉的 etcdserver: request timed out 错误。</p><h3 id="WAL-模块"><a href="#WAL-模块" class="headerlink" title="WAL 模块"></a>WAL 模块</h3><p>Raft 模块收到提案后，如果当前节点是 Follower，它会转发给 Leader，只有 Leader 才能处理写请求。Leader 收到提案后，通过 Raft 模块输出待转发给 Follower 节点的消息和待持久化的日志条目，日志条目则封装了我们上面所说的 put hello 提案内容。</p><p>etcdserver 从 Raft 模块获取到以上消息和日志条目后，作为 Leader，它会将 put 提案消息广播给集群各个节点，同时需要把集群 Leader 任期号、投票信息、已提交索引、提案内容持久化到一个 WAL（Write Ahead Log）日志文件中，用于保证集群的一致性、可恢复性，也就是我们图中的流程五模块。</p><p>WAL 日志结构是怎样的呢？</p><p><img src="https://static001.geekbang.org/resource/image/47/8d/479dec62ed1c31918a7c6cab8e6aa18d.png?wh=1920*1335" alt="img"></p><p>上图是 WAL 结构，它由多种类型的 WAL 记录顺序追加写入组成，每个记录由类型、数据、循环冗余校验码组成。不同类型的记录通过 Type 字段区分，Data 为对应记录内容，CRC 为循环校验码信息。</p><p>WAL 记录类型目前支持 5 种，分别是文件元数据记录、日志条目记录、状态信息记录、CRC 记录、快照记录：</p><ul><li>文件元数据记录包含节点 ID、集群 ID 信息，它在 WAL 文件创建的时候写入；</li><li>日志条目记录包含 Raft 日志信息，如 put 提案内容；</li><li>状态信息记录，包含集群的任期号、节点投票信息等，一个日志文件中会有多条，以最后的记录为准；</li><li>CRC 记录包含上一个 WAL 文件的最后的 CRC（循环冗余校验码）信息， 在创建、切割 WAL 文件时，作为第一条记录写入到新的 WAL 文件， 用于校验数据文件的完整性、准确性等；</li><li>快照记录包含快照的任期号、日志索引信息，用于检查快照文件的准确性。</li></ul><p><strong>WAL 模块又是如何持久化一个 put 提案的日志条目类型记录呢?</strong></p><p>首先我们来看看 put 写请求如何封装在 Raft 日志条目里面。下面是 Raft 日志条目的数据结构信息，它由以下字段组成：</p><ul><li>Term 是 Leader 任期号，随着 Leader 选举增加；</li><li>Index 是日志条目的索引，单调递增增加；</li><li>Type 是日志类型，比如是普通的命令日志（EntryNormal）还是集群配置变更日志（EntryConfChange）；</li><li>Data 保存我们上面描述的 put 提案内容。</li></ul><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-keyword">type</span> <span class="hljs-type">Entry</span> struct &#123;<br>   <span class="hljs-type">Term</span>             uint64    `protobuf:&quot;varint，2，opt，name=<span class="hljs-type">Term</span>&quot; json:&quot;<span class="hljs-type">Term</span>&quot;`<br>   <span class="hljs-type">Index</span>            uint64    `protobuf:&quot;varint，3，opt，name=<span class="hljs-type">Index</span>&quot; json:&quot;<span class="hljs-type">Index</span>&quot;`<br>   <span class="hljs-type">Type</span>             <span class="hljs-type">EntryType</span> `protobuf:&quot;varint，1，opt，name=<span class="hljs-type">Type</span>，enum=<span class="hljs-type">Raftpb</span>.<span class="hljs-type">EntryType</span>&quot; json:&quot;<span class="hljs-type">Type</span>&quot;`<br>   <span class="hljs-type">Data</span>             []byte    `protobuf:&quot;bytes，4，opt，name=<span class="hljs-type">Data</span>&quot; json:&quot;<span class="hljs-type">Data</span>，omitempty&quot;`<br>&#125;<br></code></pre></td></tr></table></figure><p>了解完 Raft 日志条目数据结构后，我们再看 WAL 模块如何持久化 Raft 日志条目。它首先先将 Raft 日志条目内容（含任期号、索引、提案内容）序列化后保存到 WAL 记录的 Data 字段， 然后计算 Data 的 CRC 值，设置 Type 为 Entry Type， 以上信息就组成了一个完整的 WAL 记录。</p><p>最后计算 WAL 记录的长度，顺序先写入 WAL 长度（Len Field），然后写入记录内容，调用 fsync 持久化到磁盘，完成将日志条目保存到持久化存储中。</p><p>当一半以上节点持久化此日志条目后， Raft 模块就会通过 channel 告知 etcdserver 模块，put 提案已经被集群多数节点确认，提案状态为已提交，你可以执行此提案内容了。</p><p>于是进入流程六，etcdserver 模块从 channel 取出提案内容，添加到先进先出（FIFO）调度队列，随后通过 Apply 模块按入队顺序，异步、依次执行提案内容。</p><h3 id="Apply-模块"><a href="#Apply-模块" class="headerlink" title="Apply 模块"></a>Apply 模块</h3><p>执行 put 提案内容对应我们架构图中的流程七，其细节图如下。那么 Apply 模块是如何执行 put 请求的呢？若 put 请求提案在执行流程七的时候 etcd 突然 crash 了， 重启恢复的时候，etcd 是如何找回异常提案，再次执行的呢？</p><p><img src="https://static001.geekbang.org/resource/image/7f/5b/7f13edaf28yy7a6698e647104771235b.png?wh=1920*641" alt="img"></p><p>核心就是上面介绍的 WAL 日志，因为提交给 Apply 模块执行的提案已获得多数节点确认、持久化，etcd 重启时，会从 WAL 中解析出 Raft 日志条目内容，追加到 Raft 日志的存储中，并重放已提交的日志提案给 Apply 模块执行。</p><p><strong>然而这又引发了另外一个问题，如何确保幂等性，防止提案重复执行导致数据混乱呢?</strong></p><p>etcd 是个 MVCC 数据库，每次更新都会生成新的版本号。如果没有幂等性保护，同样的命令，一部分节点执行一次，一部分节点遭遇异常故障后执行多次，则系统的各节点一致性状态无法得到保证，导致数据混乱，这是严重故障。</p><p><strong>因此 etcd 必须要确保幂等性。怎么做呢？Apply 模块从 Raft 模块获得的日志条目信息里，是否有唯一的字段能标识这个提案？</strong></p><p>答案就是我们上面介绍 Raft 日志条目中的索引（index）字段。日志条目索引是全局单调递增的，每个日志条目索引对应一个提案， 如果一个命令执行后，我们在 db 里面也记录下当前已经执行过的日志条目索引，是不是就可以解决幂等性问题呢？</p><p>是的。但是这还不够安全，如果执行命令的请求更新成功了，更新 index 的请求却失败了，是不是一样会导致异常？</p><p>因此我们在实现上，还需要将两个操作作为原子性事务提交，才能实现幂等。etcd 通过引入一个 consistent index 的字段，来存储系统当前已经执行过的日志条目索引，实现幂等性。</p><p>Apply 模块在执行提案内容前，首先会判断当前提案是否已经执行过了，如果执行了则直接返回，若未执行同时无 db 配额满告警，则进入到 MVCC 模块，开始与持久化存储模块打交道。</p><h3 id="MVCC"><a href="#MVCC" class="headerlink" title="MVCC"></a>MVCC</h3><p>Apply 模块判断此提案未执行后，就会调用 MVCC 模块来执行提案内容。MVCC 主要由两部分组成，一个是内存索引模块 treeIndex，保存 key 的历史版本号信息，另一个是 boltdb 模块，用来持久化存储 key-value 数据。那么 MVCC 模块执行 put hello 为 world 命令时，它是如何构建内存索引和保存哪些数据到 db 呢？</p><h3 id="treeIndex"><a href="#treeIndex" class="headerlink" title="treeIndex"></a>treeIndex</h3><p>MVCC 的索引模块 treeIndex，当收到更新 key hello 为 world 的时候，此 key 的索引版本号信息是怎么生成的呢？需要维护、持久化存储一个全局版本号吗？</p><p>版本号（revision）在 etcd 里面发挥着重大作用，它是 etcd 的逻辑时钟。etcd 启动的时候默认版本号是 1，随着你对 key 的增、删、改操作而全局单调递增。</p><p>因为 boltdb 中的 key 就包含此信息，所以 etcd 并不需要再去持久化一个全局版本号。我们只需要在启动的时候，从最小值 1 开始枚举到最大值，未读到数据的时候则结束，最后读出来的版本号即是当前 etcd 的最大版本号 currentRevision。</p><p>MVCC 写事务在执行 put hello 为 world 的请求时，会基于 currentRevision 自增生成新的 revision 如{2,0}，然后从 treeIndex 模块中查询 key 的创建版本号、修改次数信息。这些信息将填充到 boltdb 的 value 中，同时将用户的 hello key 和 revision 等信息存储到 B-tree，也就是下面简易写事务图的流程一，整体架构图中的流程八。</p><p><img src="https://static001.geekbang.org/resource/image/a1/ff/a19a06d8f4cc5e488a114090d84116ff.png?wh=1920*1035" alt="img"></p><h3 id="boltdb"><a href="#boltdb" class="headerlink" title="boltdb"></a>boltdb</h3><p>MVCC 写事务自增全局版本号后生成的 revision{2,0}，它就是 boltdb 的 key，通过它就可以往 boltdb 写数据了，进入了整体架构图中的流程九。</p><p>boltdb 上一篇我们提过它是一个基于 B+tree 实现的 key-value 嵌入式 db，它通过提供桶（bucket）机制实现类似 MySQL 表的逻辑隔离。</p><p>在 etcd 里面你通过 put&#x2F;txn 等 KV API 操作的数据，全部保存在一个名为 key 的桶里面，这个 key 桶在启动 etcd 的时候会自动创建。</p><p>除了保存用户 KV 数据的 key 桶，etcd 本身及其它功能需要持久化存储的话，都会创建对应的桶。比如上面我们提到的 etcd 为了保证日志的幂等性，保存了一个名为 consistent index 的变量在 db 里面，它实际上就存储在元数据（meta）桶里面。</p><p>那么写入 boltdb 的 value 含有哪些信息呢？</p><p>写入 boltdb 的 value， 并不是简单的”world”，如果只存一个用户 value，索引又是保存在易失的内存上，那重启 etcd 后，我们就丢失了用户的 key 名，无法构建 treeIndex 模块了。</p><p>因此为了构建索引和支持 Lease 等特性，etcd 会持久化以下信息:</p><ul><li>key 名称；</li><li>key 创建时的版本号（create_revision）、最后一次修改时的版本号（mod_revision）、key 自身修改的次数（version）；</li><li>value 值；</li><li>租约信息（后面介绍）。</li></ul><p>boltdb value 的值就是将含以上信息的结构体序列化成的二进制数据，然后通过 boltdb 提供的 put 接口，etcd 就快速完成了将你的数据写入 boltdb，对应上面简易写事务图的流程二。</p><p>但是 put 调用成功，就能够代表数据已经持久化到 db 文件了吗？</p><p>这里需要注意的是，在以上流程中，etcd 并未提交事务（commit），因此数据只更新在 boltdb 所管理的内存数据结构中。</p><p>事务提交的过程，包含 B+tree 的平衡、分裂，将 boltdb 的脏数据（dirty page）、元数据信息刷新到磁盘，因此事务提交的开销是昂贵的。如果我们每次更新都提交事务，etcd 写性能就会较差。</p><p>那么解决的办法是什么呢？</p><p>etcd 的解决方案是合并再合并。</p><p>首先 boltdb key 是版本号，put&#x2F;delete 操作时，都会基于当前版本号递增生成新的版本号，因此属于顺序写入，可以调整 boltdb 的 bucket.FillPercent 参数，使每个 page 填充更多数据，减少 page 的分裂次数并降低 db 空间。</p><p>其次 etcd 通过合并多个写事务请求，通常情况下，是异步机制定时（默认每隔 100ms）将批量事务一次性提交（pending 事务过多才会触发同步提交）， 从而大大提高吞吐量，对应上面简易写事务图的流程三。</p><p>但是这优化又引发了另外的一个问题， 因为事务未提交，读请求可能无法从 boltdb 获取到最新数据。</p><p>为了解决这个问题，etcd 引入了一个 bucket buffer 来保存暂未提交的事务数据。在更新 boltdb 的时候，etcd 也会同步数据到 bucket buffer。因此 etcd 处理读请求的时候会优先从 bucket buffer 里面读取，其次再从 boltdb 读，通过 bucket buffer 实现读写性能提升，同时保证数据一致性。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>首先我们介绍了 Quota 模块工作原理和我们熟悉的 database space exceeded 错误触发原因，写请求导致 db 大小增加、compact 策略不合理、boltdb Bug 等都会导致 db 大小超限。</p><p>其次介绍了 WAL 模块的存储结构，它由一条条记录顺序写入组成，每个记录含有 Type、CRC、Data，每个提案被提交前都会被持久化到 WAL 文件中，以保证集群的一致性和可恢复性。</p><p>随后我们介绍了 Apply 模块基于 consistent index 和事务实现了幂等性，保证了节点在异常情况下不会重复执行重放的提案。</p><p>最后我们介绍了 MVCC 模块是如何维护索引版本号、重启后如何从 boltdb 模块中获取内存索引结构的。以及 etcd 通过异步、批量提交事务机制，以提升写 QPS 和吞吐量。</p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>02.etcd读的执行流程</title>
    <link href="/2022/10/02/02-etcd%E8%AF%BB%E7%9A%84%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/"/>
    <url>/2022/10/02/02-etcd%E8%AF%BB%E7%9A%84%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="etcd读写请求的执行过程"><a href="#etcd读写请求的执行过程" class="headerlink" title="etcd读写请求的执行过程"></a>etcd读写请求的执行过程</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><h2 id="etcd-读请求的执行过程"><a href="#etcd-读请求的执行过程" class="headerlink" title="etcd 读请求的执行过程"></a>etcd 读请求的执行过程</h2><p>etcd 是典型的读多写少存储，在我们实际业务场景中，读一般占据 2&#x2F;3 以上的请求。为了让你对 etcd 有一个深入的理解，接下来分析一个读请求是如何执行的，了解 etcd 的核心模块，进而由点及线、由线到面地构建 etcd 的全景知识脉络。</p><p>在下面这张架构图中，用序号标识了 etcd 默认读模式（线性读）的执行流程，接下来，我们就按照这个执行流程从头开始说。</p><p><img src="https://static001.geekbang.org/resource/image/45/bb/457db2c506135d5d29a93ef0bd97e4bb.png?wh=1920*1229" alt="img"></p><h3 id="client"><a href="#client" class="headerlink" title="client"></a>client</h3><p>启动完 etcd 集群后，当你用 etcd 的客户端工具 etcdctl 执行一个 get hello 命令（如下）时，对应到图中流程一，etcdctl 是如何工作的呢？</p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs actionscript">etcdctl <span class="hljs-keyword">get</span> hello --endpoints http:<span class="hljs-comment">//127.0.0.1:2379 </span><br>hello <br>world <br></code></pre></td></tr></table></figure><p>首先，etcdctl 会对命令中的参数进行解析。我们来看下这些参数的含义，其中，参数“get”是请求的方法，它是 KVServer 模块的 API；“hello”是我们查询的 key 名；“endpoints”是我们后端的 etcd 地址，通常，生产环境下中需要配置多个 endpoints，这样在 etcd 节点出现故障后，client 就可以自动重连到其它正常的节点，从而保证请求的正常执行。</p><p>在 etcd v3.4.9 版本中，etcdctl 是通过 clientv3 库来访问 etcd server 的，clientv3 库基于 gRPC client API 封装了操作 etcd KVServer、Cluster、Auth、Lease、Watch 等模块的 API，同时还包含了负载均衡、健康探测和故障切换等特性。</p><p>在解析完请求中的参数后，etcdctl 会创建一个 clientv3 库对象，使用 KVServer 模块的 API 来访问 etcd server。</p><p>接下来，就需要为这个 get hello 请求选择一个合适的 etcd server 节点了，这里得用到负载均衡算法。在 etcd 3.4 中，clientv3 库采用的负载均衡算法为 Round-robin。针对每一个请求，Round-robin 算法通过轮询的方式依次从 endpoint 列表中选择一个 endpoint 访问 (长连接)，使 etcd server 负载尽量均衡。</p><p>关于负载均衡算法，你需要特别注意以下两点。</p><ul><li>如果你的 client 版本 &lt;&#x3D; 3.3，那么当你配置多个 endpoint 时，负载均衡算法仅会从中选择一个 IP 并创建一个连接（Pinned endpoint），这样可以节省服务器总连接数。在 heavy usage 场景，这可能会造成 server 负载不均衡。</li><li>在 client 3.4 之前的版本中，负载均衡算法有一个严重的 Bug：如果第一个节点异常了，可能会导致你的 client 访问 etcd server 异常，特别是在 Kubernetes 场景中会导致 APIServer 不可用。不过，该 Bug 已在 Kubernetes 1.16 版本后被修复。</li></ul><p>为请求选择好 etcd server 节点，client 就可调用 etcd server 的 KVServer 模块的 Range RPC 方法，把请求发送给 etcd server。</p><p>这里说明一点，client 和 server 之间的通信，使用的是基于 HTTP&#x2F;2 的 gRPC 协议。相比 etcd v2 的 HTTP&#x2F;1.x，HTTP&#x2F;2 是基于二进制而不是文本、支持多路复用而不再有序且阻塞、支持数据压缩以减少包大小、支持 server push 等特性。因此，基于 HTTP&#x2F;2 的 gRPC 协议具有低延迟、高性能的特点，有效解决了我们在上一讲中提到的 etcd v2 中 HTTP&#x2F;1.x 性能问题。</p><h3 id="KVServer"><a href="#KVServer" class="headerlink" title="KVServer"></a>KVServer</h3><p>client 发送 Range RPC 请求到了 server 后，就开始进入我们架构图中的流程二，也就是 KVServer 模块了。</p><p>etcd 提供了丰富的 metrics、日志、请求行为检查等机制，可记录所有请求的执行耗时及错误码、来源 IP 等，也可控制请求是否允许通过，比如 etcd Learner 节点只允许指定接口和参数的访问，帮助大家定位问题、提高服务可观测性等，而这些特性是怎么非侵入式的实现呢？</p><p>答案就是拦截器。</p><h4 id="拦截器"><a href="#拦截器" class="headerlink" title="拦截器"></a>拦截器</h4><p>etcd server 定义了如下的 Service KV 和 Range 方法，启动的时候它会将实现 KV 各方法的对象注册到 gRPC Server，并在其上注册对应的拦截器。下面的代码中的 Range 接口就是负责读取 etcd key-value 的的 RPC 接口。</p><figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs protobuf"><span class="hljs-keyword">service </span><span class="hljs-title class_">KV</span> &#123;  <br>  <span class="hljs-comment">// Range gets the keys in the range from the key-value store.  </span><br>  <span class="hljs-function"><span class="hljs-keyword">rpc</span> Range(RangeRequest) <span class="hljs-keyword">returns</span> (RangeResponse) </span>&#123;  <br>      <span class="hljs-keyword">option</span> (google.api.http) = &#123;  <br>        post: <span class="hljs-string">&quot;/v3/kv/range&quot;</span>  <br>        body: <span class="hljs-string">&quot;*&quot;</span>  <br>      &#125;;  <br>  &#125;  <br>  ....<br>&#125;  <br></code></pre></td></tr></table></figure><p>拦截器提供了在执行一个请求前后的 hook 能力，除了我们上面提到的 debug 日志、metrics 统计、对 etcd Learner 节点请求接口和参数限制等能力，etcd 还基于它实现了以下特性:</p><ul><li>要求执行一个操作前集群必须有 Leader；</li><li>请求延时超过指定阈值的，打印包含来源 IP 的慢查询日志 (3.5 版本)。</li></ul><p>server 收到 client 的 Range RPC 请求后，根据 ServiceName 和 RPC Method 将请求转发到对应的 handler 实现，handler 首先会将上面描述的一系列拦截器串联成一个执行，在拦截器逻辑中，通过调用 KVServer 模块的 Range 接口获取数据。</p><h3 id="串行读与线性读"><a href="#串行读与线性读" class="headerlink" title="串行读与线性读"></a>串行读与线性读</h3><p>进入 KVServer 模块后，就进入核心的读流程了，对应架构图中的流程三和四。我们知道 etcd 为了保证服务高可用，生产环境一般部署多个节点，那各个节点数据在任意时间点读出来都是一致的吗？什么情况下会读到旧数据呢？</p><p>为了更好的理解读流程，先简单提下写流程。如下图所示，当 client 发起一个更新 hello 为 world 请求后，若 Leader 收到写请求，它会将此请求持久化到 WAL 日志，并广播给各个节点，若一半以上节点持久化成功，则该请求对应的日志条目被标识为已提交，etcdserver 模块异步从 Raft 模块获取已提交的日志条目，应用到状态机 (boltdb 等)。</p><p><img src="https://static001.geekbang.org/resource/image/cf/d5/cffba70a79609f29e1f2ae1f3bd07fd5.png?wh=1920*1074" alt="img"></p><p>此时若 client 发起一个读取 hello 的请求，假设此请求直接从状态机中读取， 如果连接到的是 C 节点，若 C 节点磁盘 I&#x2F;O 出现波动，可能导致它应用已提交的日志条目很慢，则会出现更新 hello 为 world 的写命令，在 client 读 hello 的时候还未被提交到状态机，因此就可能读取到旧数据，如上图查询 hello 流程所示。</p><p>从以上介绍我们可以看出，在多节点 etcd 集群中，各个节点的状态机数据一致性存在差异。而我们不同业务场景的读请求对数据是否最新的容忍度是不一样的，有的场景它可以容忍数据落后几秒甚至几分钟，有的场景要求必须读到反映集群共识的最新数据。</p><p><strong>对数据敏感度较低的场景。</strong></p><p>假如老板让你做一个旁路数据统计服务，希望你每分钟统计下 etcd 里的服务、配置信息等，这种场景其实对数据时效性要求并不高，读请求可直接从节点的状态机获取数据。即便数据落后一点，也不影响业务，毕竟这是一个定时统计的旁路服务而已。这种直接读状态机数据返回、无需通过 Raft 协议与集群进行交互的模式，在 etcd 里叫做串行 (Serializable) 读，它具有低延时、高吞吐量的特点，适合对数据一致性要求不高的场景。</p><p><strong>对数据敏感性高的场景。</strong></p><p>当你发布服务，更新服务的镜像的时候，提交的时候显示更新成功，结果你一刷新页面，发现显示的镜像的还是旧的，再刷新又是新的，这就会导致混乱。再比如说一个转账场景，Alice 给 Bob 转账成功，钱被正常扣出，一刷新页面发现钱又回来了，这也是令人不可接受的。以上的业务场景就对数据准确性要求极高了，在 etcd 里面，提供了一种线性读模式来解决对数据一致性要求高的场景。</p><p><strong>什么是线性读?</strong></p><p>可以理解一旦一个值更新成功，随后任何通过线性读的 client 都能及时访问到。虽然集群中有多个节点，但 client 通过线性读就如访问一个节点一样。etcd 默认读模式是线性读，因为它需要经过 Raft 协议模块，反应的是集群共识，因此在延时和吞吐量上相比串行读略差一点，适用于对数据一致性要求高的场景。如果你的 etcd 读请求显示指定了是串行读，就不会经过架构图流程中的流程三、四。默认是线性读，因此接下来我们看看读请求进入线性读模块，它是如何工作的。</p><p><strong>线性读之 ReadIndex</strong></p><p>前面我们聊到串行读时提到，它之所以能读到旧数据，主要原因是 Follower 节点收到 Leader 节点同步的写请求后，应用日志条目到状态机是个异步过程，那么我们能否有一种机制在读取的时候，确保最新的数据已经应用到状态机中？</p><p><img src="https://static001.geekbang.org/resource/image/1c/cc/1c065788051c6eaaee965575a04109cc.png?wh=1920*1095" alt="img"></p><p>其实这个机制就是叫 <strong>ReadIndex</strong>，它是在 etcd 3.1 中引入的，我把简化后的原理图放在了上面。当收到一个线性读请求时，它首先会从 Leader 获取集群最新的已提交的日志索引 (committed index)，如上图中的流程二所示。</p><p>Leader 收到 ReadIndex 请求时，为防止脑裂等异常场景，会向 Follower 节点发送心跳确认，一半以上节点确认 Leader 身份后才能将已提交的索引 (committed index) 返回给节点 C(上图中的流程三)。</p><p>C 节点则会等待，直到状态机已应用索引 (applied index) 大于等于 Leader 的已提交索引时 (committed Index)(上图中的流程四)，然后去通知读请求，数据已赶上 Leader，你可以去状态机中访问数据了 (上图中的流程五)。</p><p>以上就是线性读通过 ReadIndex 机制保证数据一致性原理， 当然还有其它机制也能实现线性读，如在早期 etcd 3.0 中读请求通过走一遍 Raft 协议保证一致性， 这种 Raft log read 机制依赖磁盘 IO， 性能相比 ReadIndex 较差。</p><p>总体而言，KVServer 模块收到线性读请求后，通过架构图中流程三向 Raft 模块发起 ReadIndex 请求，Raft 模块将 Leader 最新的已提交日志索引封装在流程四的 ReadState 结构体，通过 channel 层层返回给线性读模块，线性读模块等待本节点状态机追赶上 Leader 进度，追赶完成后，就通知 KVServer 模块，进行架构图中流程五，与状态机中的 MVCC 模块进行进行交互了。</p><h3 id="MVCC"><a href="#MVCC" class="headerlink" title="MVCC"></a>MVCC</h3><p>流程五中的多版本并发控制 (Multiversion concurrency control) 模块是为了解决 etcd v2 不支持保存 key 的历史版本、不支持多 key 事务等问题而产生的。</p><p>它核心由内存树形索引模块 (treeIndex) 和嵌入式的 KV 持久化存储库 boltdb 组成。</p><p>首先我们需要简单了解下 boltdb，它是个基于 B+ tree 实现的 key-value 键值库，支持事务，提供 Get&#x2F;Put 等简易 API 给 etcd 操作。</p><p>那么 etcd 如何基于 boltdb 保存一个 key 的多个历史版本呢?</p><p>比如我们现在有以下方案：方案 1 是一个 key 保存多个历史版本的值；方案 2 每次修改操作，生成一个新的版本号 (revision)，以版本号为 key， value 为用户 key-value 等信息组成的结构体。</p><p>很显然方案 1 会导致 value 较大，存在明显读写放大、并发冲突等问题，而方案 2 正是 etcd 所采用的。boltdb 的 key 是全局递增的版本号 (revision)，value 是用户 key、value 等字段组合成的结构体，然后通过 treeIndex 模块来保存用户 key 和版本号的映射关系。</p><p>treeIndex 与 boltdb 关系如下面的读事务流程图所示，从 treeIndex 中获取 key hello 的版本号，再以版本号作为 boltdb 的 key，从 boltdb 中获取其 value 信息。</p><p><img src="https://static001.geekbang.org/resource/image/4e/a3/4e2779c265c1da1f7209b5293e3789a3.png?wh=1920*1124" alt="img"></p><h4 id="treeIndext"><a href="#treeIndext" class="headerlink" title="treeIndext"></a>treeIndext</h4><p>reeIndex 模块是基于 Google 开源的内存版 btree 库实现的，为什么 etcd 选择上图中的 B-tree 数据结构保存用户 key 与版本号之间的映射关系，而不是哈希表、二叉树呢？在后面的文章再仔细介绍。</p><p>treeIndex 模块只会保存用户的 key 和相关版本号信息，用户 key 的 value 数据存储在 boltdb 里面，相比 ZooKeeper 和 etcd v2 全内存存储，etcd v3 对内存要求更低。</p><p>简单介绍了 etcd 如何保存 key 的历史版本后，架构图中流程六也就非常容易理解了， 它需要从 treeIndex 模块中获取 hello 这个 key 对应的版本号信息。treeIndex 模块基于 B-tree 快速查找此 key，返回此 key 对应的索引项 keyIndex 即可。索引项中包含版本号等信息。</p><h3 id="buffer"><a href="#buffer" class="headerlink" title="buffer"></a>buffer</h3><p>在获取到版本号信息后，就可从 boltdb 模块中获取用户的 key-value 数据了。不过有一点你要注意，并不是所有请求都一定要从 boltdb 获取数据。</p><p>etcd 出于数据一致性、性能等考虑，在访问 boltdb 前，首先会从一个内存读事务 buffer 中，二分查找你要访问 key 是否在 buffer 里面，若命中则直接返回。</p><h3 id="boltdb"><a href="#boltdb" class="headerlink" title="boltdb"></a>boltdb</h3><p>若 buffer 未命中，此时就真正需要向 boltdb 模块查询数据了，进入了流程七。</p><p>我们知道 MySQL 通过 table 实现不同数据逻辑隔离，那么在 boltdb 是如何隔离集群元数据与用户数据的呢？答案是 bucket。boltdb 里每个 bucket 类似对应 MySQL 一个表，用户的 key 数据存放的 bucket 名字的是 key，etcd MVCC 元数据存放的 bucket 是 meta。</p><p>因 boltdb 使用 B+ tree 来组织用户的 key-value 数据，获取 bucket key 对象后，通过 boltdb 的游标 Cursor 可快速在 B+ tree 找到 key hello 对应的 value 数据，返回给 client。</p><p>到这里，一个读请求之路执行完成。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>一个读请求从 client 通过 Round-robin 负载均衡算法，选择一个 etcd server 节点，发出 gRPC 请求，经过 etcd server 的 KVServer 模块、线性读模块、MVCC 的 treeIndex 和 boltdb 模块紧密协作，完成了一个读请求。</p><p>通过一个读请求，我带你初步了解了 etcd 的基础架构以及各个模块之间是如何协作的。</p><p>在这过程中，特别总结下 client 的节点故障自动转移和线性读。</p><p>一方面， client 的通过负载均衡、错误处理等机制实现了 etcd 节点之间的故障的自动转移，它可助你的业务实现服务高可用，建议使用 etcd 3.4 分支的 client 版本。</p><p>另一方面，我详细解释了 etcd 提供的两种读机制 (串行读和线性读) 原理和应用场景。通过线性读，对业务而言，访问多个节点的 etcd 集群就如访问一个节点一样简单，能简洁、快速的获取到集群最新共识数据。</p><p>早期 etcd 线性读使用的 Raft log read，也就是说把读请求像写请求一样走一遍 Raft 的协议，基于 Raft 的日志的有序性，实现线性读。但此方案读涉及磁盘 IO 开销，性能较差，后来实现了 ReadIndex 读机制来提升读性能，满足了 Kubernetes 等业务的诉求。</p><p><strong>etcd 在执行读请求过程中涉及磁盘 IO 吗？如果涉及，是什么模块在什么场景下会触发呢？如果不涉及，又是什么原因呢？</strong></p><p>etcd 在启动的时候会通过 mmap 机制将 etcd db 文件映射到 etcd 进程地址空间，并设置了 mmap 的 MAP_POPULATE flag，它会告诉 Linux 内核预读文件，Linux 内核会将文件内容拷贝到物理内存中，此时会产生磁盘 I&#x2F;O。节点内存足够的请求下，后续处理读请求过程中就不会产生磁盘 I&#x2F;IO 了。</p><p>若 etcd 节点内存不足，可能会导致 db 文件对应的内存页被换出，当读请求命中的页未在内存中时，就会产生缺页异常，导致读过程中产生磁盘 IO，你可以通过观察 etcd 进程的 majflt 字段来判断 etcd 是否产生了主缺页中断。</p>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>raft论文翻译</title>
    <link href="/2022/10/01/raft%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"/>
    <url>/2022/10/01/raft%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/</url>
    
    <content type="html"><![CDATA[<h1 id="寻找一种易于理解的一致性算法（扩展版）"><a href="#寻找一种易于理解的一致性算法（扩展版）" class="headerlink" title="寻找一种易于理解的一致性算法（扩展版）"></a>寻找一种易于理解的一致性算法（扩展版）</h1><ul><li><a href="#%E5%AF%BB%E6%89%BE%E4%B8%80%E7%A7%8D%E6%98%93%E4%BA%8E%E7%90%86%E8%A7%A3%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95%E6%89%A9%E5%B1%95%E7%89%88">寻找一种易于理解的一致性算法（扩展版）</a><ul><li><a href="#%E6%91%98%E8%A6%81">摘要</a></li><li><a href="#1-%E4%BB%8B%E7%BB%8D">1 介绍</a></li><li><a href="#2-%E5%A4%8D%E5%88%B6%E7%8A%B6%E6%80%81%E6%9C%BA">2 复制状态机</a></li><li><a href="#3-paxos-%E7%AE%97%E6%B3%95%E7%9A%84%E9%97%AE%E9%A2%98">3 Paxos 算法的问题</a></li><li><a href="#4-%E4%B8%BA%E4%BA%86%E5%8F%AF%E7%90%86%E8%A7%A3%E6%80%A7%E7%9A%84%E8%AE%BE%E8%AE%A1">4 为了可理解性的设计</a></li><li><a href="#5-raft-%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95">5 Raft 一致性算法</a><ul><li><a href="#51-raft-%E5%9F%BA%E7%A1%80">5.1 Raft 基础</a></li><li><a href="#52-%E9%A2%86%E5%AF%BC%E4%BA%BA%E9%80%89%E4%B8%BE">5.2 领导人选举</a></li><li><a href="#53-%E6%97%A5%E5%BF%97%E5%A4%8D%E5%88%B6">5.3 日志复制</a></li><li><a href="#54-%E5%AE%89%E5%85%A8%E6%80%A7">5.4 安全性</a><ul><li><a href="#541-%E9%80%89%E4%B8%BE%E9%99%90%E5%88%B6">5.4.1 选举限制</a></li><li><a href="#542-%E6%8F%90%E4%BA%A4%E4%B9%8B%E5%89%8D%E4%BB%BB%E6%9C%9F%E5%86%85%E7%9A%84%E6%97%A5%E5%BF%97%E6%9D%A1%E7%9B%AE">5.4.2 提交之前任期内的日志条目</a></li><li><a href="#543-%E5%AE%89%E5%85%A8%E6%80%A7%E8%AE%BA%E8%AF%81">5.4.3 安全性论证</a></li></ul></li><li><a href="#55-%E8%B7%9F%E9%9A%8F%E8%80%85%E5%92%8C%E5%80%99%E9%80%89%E4%BA%BA%E5%B4%A9%E6%BA%83">5.5 跟随者和候选人崩溃</a></li><li><a href="#56-%E6%97%B6%E9%97%B4%E5%92%8C%E5%8F%AF%E7%94%A8%E6%80%A7">5.6 时间和可用性</a></li></ul></li><li><a href="#6-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E5%8F%98%E5%8C%96">6 集群成员变化</a></li><li><a href="#7-%E6%97%A5%E5%BF%97%E5%8E%8B%E7%BC%A9">7 日志压缩</a></li><li><a href="#8-%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%BA%A4%E4%BA%92">8 客户端交互</a></li><li><a href="#9-%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E5%92%8C%E8%AF%84%E4%BC%B0">9 算法实现和评估</a><ul><li><a href="#91-%E5%8F%AF%E7%90%86%E8%A7%A3%E6%80%A7">9.1 可理解性</a></li><li><a href="#92-%E6%AD%A3%E7%A1%AE%E6%80%A7">9.2 正确性</a></li><li><a href="#93-%E6%80%A7%E8%83%BD">9.3 性能</a></li></ul></li><li><a href="#10-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C">10 相关工作</a></li><li><a href="#11-%E7%BB%93%E8%AE%BA">11 结论</a></li><li><a href="#12-%E6%84%9F%E8%B0%A2">12 感谢</a></li><li><a href="#%E5%8F%82%E8%80%83">参考</a></li></ul></li></ul><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Raft 是一种为了管理复制日志的一致性算法。它提供了和 Paxos 算法相同的功能和性能，但是它的算法结构和 Paxos 不同，使得 Raft 算法更加容易理解并且更容易构建实际的系统。为了提升可理解性，Raft 将一致性算法分解成了几个关键模块，例如领导人选举、日志复制和安全性。同时它通过实施一个更强的一致性来减少需要考虑的状态的数量。一项用户研究的结果表明，对于学生而言，Raft 算法比 Paxos 算法更加容易学习。Raft 算法还包括一个新的机制来允许集群成员的动态改变，它利用重叠的大多数来保证安全性。</p><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h2><p>一致性算法允许一组机器像一个整体一样工作，即使其中一些机器出现故障也能够继续工作下去。正因为如此，一致性算法在构建可信赖的大规模软件系统中扮演着重要的角色。在过去的 10 年里，Paxos 算法统治着一致性算法这一领域：绝大多数的实现都是基于 Paxos 或者受其影响。同时 Paxos 也成为了教学领域里讲解一致性问题时的示例。</p><p>但是不幸的是，尽管有很多工作都在尝试降低它的复杂性，但是 Paxos 算法依然十分难以理解。并且，Paxos 自身的算法结构需要进行大幅的修改才能够应用到实际的系统中。因此工业界和学术界都对 Paxos 算法感到十分头疼。</p><p>努力研究过 Paxos 算法之后，我们开始寻找一种新的一致性算法，可以为构建实际的系统和教学提供更好的基础。与 Paxos 不同，我们的首要目标是可理解性：我们是否可以在实际系统中定义一个一致性算法，并且比 Paxos 算法更容易学习。此外，我们希望该算法方便系统构建者的直觉的发展。重要的不仅仅是算法能够工作，更重要的是能够很清楚地知道它为什么能工作。</p><p>Raft 一致性算法就是这些工作的结果。在设计 Raft 算法的时候，我们使用一些特别的技巧来提升它的可理解性，包括算法分解（Raft 主要被分成了领导人选举，日志复制和安全三个模块）和减少状态机的状态（相对于 Paxos，Raft 减少了非确定性和服务器互相处于非一致性的方式）。一份针对两所大学 43 个学生的研究表明 Raft 明显比 Paxos 算法更加容易理解。在这些学生同时学习了这两种算法之后，和 Paxos 比起来，其中 33 个学生能够回答有关于 Raft 的问题。</p><p>Raft 算法在许多方面和现有的一致性算法都很相似（主要是 Oki 和 Liskov 的 Viewstamped Replication），但是它也有一些独特的特性：</p><ul><li><strong>强领导人</strong>：和其他一致性算法相比，Raft 使用一种更强的领导能力形式。比如，日志条目只从领导人发送给其他的服务器。这种方式简化了对复制日志的管理并且使得 Raft 算法更加易于理解。</li><li><strong>领导选举</strong>：Raft 算法使用一个随机计时器来选举领导人。这种方式只是在任何一致性算法都必须实现的心跳机制上增加了一点机制。在解决冲突的时候会更加简单快捷。</li><li><strong>成员关系调整</strong>：Raft 使用一种共同一致的方法来处理集群成员变换的问题，在这种方法下，处于调整过程中的两种不同的配置集群中大多数机器会有重叠，这就使得集群在成员变换的时候依然可以继续工作。</li></ul><p>我们相信，Raft 算法不论出于教学目的还是作为实践项目的基础都是要比 Paxos 或者其他一致性算法要优异的。它比其他算法更加简单，更加容易理解；它的算法描述足以实现一个现实的系统；它有好多开源的实现并且在很多公司里使用；它的安全特性已经被正式定义和证明；它的效率和其他算法比起来也不相上下。</p><p>接下来，这篇论文会介绍以下内容：复制状态机问题（第 2 节），讨论 Paxos 的优点和缺点（第 3 节），讨论我们为了可理解性而采取的方法（第 4 节），阐述 Raft 一致性算法（第 5-8 节），评价 Raft 算法（第 9 节），以及一些相关的工作（第 10 节）。</p><h2 id="2-复制状态机"><a href="#2-复制状态机" class="headerlink" title="2 复制状态机"></a>2 复制状态机</h2><p>一致性算法是从复制状态机的背景下提出的（参考英文原文引用37）。在这种方法中，一组服务器上的状态机产生相同状态的副本，并且在一些机器宕掉的情况下也可以继续运行。复制状态机在分布式系统中被用于解决很多容错的问题。例如，大规模的系统中通常都有一个集群领导人，像 GFS、HDFS 和 RAMCloud，典型应用就是一个独立的复制状态机去管理领导选举和存储配置信息并且在领导人宕机的情况下也要存活下来。比如 Chubby 和 ZooKeeper。</p><p><img src="D:\桌面\项目\开源项目\Reading-note\kuberneters\ETCD\images\raft-图1.png" alt="图 1 "></p><blockquote><p>图 1 ：复制状态机的结构。一致性算法管理着来自客户端指令的复制日志。状态机从日志中处理相同顺序的相同指令，所以产生的结果也是相同的。</p></blockquote><p>复制状态机通常都是基于复制日志实现的，如图 1。每一个服务器存储一个包含一系列指令的日志，并且按照日志的顺序进行执行。每一个日志都按照相同的顺序包含相同的指令，所以每一个服务器都执行相同的指令序列。因为每个状态机都是确定的，每一次执行操作都产生相同的状态和同样的序列。</p><p>一致性算法的任务是保证复制日志的一致性。服务器上的一致性模块接收客户端发送的指令然后添加到自己的日志中。它和其他服务器上的一致性模块进行通信来保证每一个服务器上的日志最终都以相同的顺序包含相同的请求，即使有些服务器发生故障。一旦指令被正确的复制，每一个服务器的状态机按照日志顺序处理他们，然后输出结果被返回给客户端。因此，服务器集群看起来形成了一个高可靠的状态机。</p><p>实际系统中使用的一致性算法通常含有以下特性：</p><ul><li>安全性保证（绝对不会返回一个错误的结果）：在非拜占庭错误情况下，包括网络延迟、分区、丢包、重复和乱序等错误都可以保证正确。</li><li>可用性：集群中只要有大多数的机器可运行并且能够相互通信、和客户端通信，就可以保证可用。因此，一个典型的包含 5 个节点的集群可以容忍两个节点的失败。服务器被停止就认为是失败。它们稍后可能会从可靠存储的状态中恢复并重新加入集群。</li><li>不依赖时序来保证一致性：物理时钟错误或者极端的消息延迟只有在最坏情况下才会导致可用性问题。</li><li>通常情况下，一条指令可以尽可能快的在集群中大多数节点响应一轮远程过程调用时完成。小部分比较慢的节点不会影响系统整体的性能。</li></ul><h2 id="3-Paxos-算法的问题"><a href="#3-Paxos-算法的问题" class="headerlink" title="3 Paxos 算法的问题"></a>3 Paxos 算法的问题</h2><p>在过去的 10 年里，Leslie Lamport 的 Paxos 算法几乎已经成为一致性的代名词：Paxos 是在课程教学中最经常使用的算法，同时也是大多数一致性算法实现的起点。Paxos 首先定义了一个能够达成单一决策一致的协议，比如单条的复制日志项。我们把这一子集叫做单决策 Paxos。然后通过组合多个 Paxos 协议的实例来促进一系列决策的达成。Paxos 保证安全性和活性，同时也支持集群成员关系的变更。Paxos 的正确性已经被证明，在通常情况下也很高效。</p><p>不幸的是，Paxos 有两个明显的缺点。第一个缺点是 Paxos 算法特别的难以理解。完整的解释是出了名的不透明；通过极大的努力之后，也只有少数人成功理解了这个算法。因此，有了几次用更简单的术语来解释 Paxos 的尝试。尽管这些解释都只关注了单决策的子集问题，但依然很具有挑战性。在 2012 年 NSDI 的会议中的一次调查显示，很少有人对 Paxos 算法感到满意，甚至在经验老道的研究者中也是如此。我们自己也尝试去理解 Paxos；我们一直没能理解 Paxos 直到我们读了很多对 Paxos 的简化解释并且设计了我们自己的算法之后，这一过程花了近一年时间。</p><p>我们假设 Paxos 的不透明性来自它选择单决策问题作为它的基础。单决策 Paxos 是晦涩微妙的，它被划分成了两种没有简单直观解释和无法独立理解的情景。因此，这导致了很难建立起直观的感受为什么单决策 Paxos 算法能够工作。构成多决策 Paxos 增加了很多错综复杂的规则。我们相信，在多决策上达成一致性的问题（一份日志而不是单一的日志记录）能够被分解成其他的方式并且更加直接和明显。</p><p>Paxos算法的第二个问题就是它没有提供一个足够好的用来构建一个现实系统的基础。一个原因是还没有一种被广泛认同的多决策问题的算法。Lamport 的描述基本上都是关于单决策 Paxos 的；他简要描述了实施多决策 Paxos 的方法，但是缺乏很多细节。当然也有很多具体化 Paxos 的尝试，但是他们都互相不一样，和 Paxos 的概述也不同。例如 Chubby 这样的系统实现了一个类似于 Paxos 的算法，但是大多数的细节并没有被公开。</p><p>而且，Paxos 算法的结构也不是十分易于构建实践的系统；单决策分解也会产生其他的结果。例如，独立地选择一组日志条目然后合并成一个序列化的日志并没有带来太多的好处，仅仅增加了不少复杂性。围绕着日志来设计一个系统是更加简单高效的；新日志条目以严格限制的顺序增添到日志中去。另一个问题是，Paxos 使用了一种对等的点对点的方式作为它的核心（尽管它最终提议了一种弱领导人的方法来优化性能）。在只有一个决策会被制定的简化世界中是很有意义的，但是很少有现实的系统使用这种方式。如果有一系列的决策需要被制定，首先选择一个领导人，然后让他去协调所有的决议，会更加简单快速。</p><p>因此，实际的系统中很少有和 Paxos 相似的实践。每一种实现都是从 Paxos 开始研究，然后发现很多实现上的难题，再然后开发了一种和 Paxos 明显不一样的结构。这样是非常费时和容易出错的，并且理解 Paxos 的难度使得这个问题更加糟糕。Paxos 算法在理论上被证明是正确可行的，但是现实的系统和 Paxos 差别是如此的大，以至于这些证明没有什么太大的价值。下面来自 Chubby 实现非常典型：</p><blockquote><p>在Paxos算法描述和实现现实系统中间有着巨大的鸿沟。最终的系统建立在一种没有经过证明的算法之上。</p></blockquote><p>由于以上问题，我们认为 Paxos 算法既没有提供一个良好的基础给实践的系统，也没有给教学很好的帮助。基于一致性问题在大规模软件系统中的重要性，我们决定看看我们是否可以设计一个拥有更好特性的替代 Paxos 的一致性算法。Raft 算法就是这次实验的结果。</p><h2 id="4-为了可理解性的设计"><a href="#4-为了可理解性的设计" class="headerlink" title="4 为了可理解性的设计"></a>4 为了可理解性的设计</h2><p>设计 Raft 算法我们有几个初衷：它必须提供一个完整的实际的系统实现基础，这样才能大大减少开发者的工作；它必须在任何情况下都是安全的并且在大多数的情况下都是可用的；并且它的大部分操作必须是高效的。但是我们最重要也是最大的挑战是可理解性。它必须保证对于普遍的人群都可以十分容易的去理解。另外，它必须能够让人形成直观的认识，这样系统的构建者才能够在现实中进行必然的扩展。</p><p>在设计 Raft 算法的时候，有很多的点需要我们在各种备选方案中进行选择。在这种情况下，我们评估备选方案基于可理解性原则：解释各个备选方案有多大的难度（例如，Raft 的状态空间有多复杂，是否有微妙的暗示）？对于一个读者而言，完全理解这个方案和暗示是否容易？</p><p>我们意识到对这种可理解性分析上具有高度的主观性；尽管如此，我们使用了两种通常适用的技术来解决这个问题。第一个技术就是众所周知的问题分解：我们尽可能地将问题分解成几个相对独立的，可被解决的、可解释的和可理解的子问题。例如，Raft 算法被我们分成领导人选举，日志复制，安全性和成员变更几个部分。</p><p>我们使用的第二个方法是通过减少状态的数量来简化需要考虑的状态空间，使得系统更加连贯并且在可能的时候消除不确定性。特别的，所有的日志是不允许有空洞的，并且 Raft 限制了日志之间变成不一致状态的可能。尽管在大多数情况下我们都试图消除不确定性，但是也有一些情况下不确定性可以提升可理解性。尤其是，随机化方法增加了不确定性，但是他们有利于减少状态空间数量，通过处理所有可能选择时使用相似的方法。我们使用随机化来简化 Raft 中领导人选举算法。</p><h2 id="5-Raft-一致性算法"><a href="#5-Raft-一致性算法" class="headerlink" title="5 Raft 一致性算法"></a>5 Raft 一致性算法</h2><p>Raft 是一种用来管理章节 2 中描述的复制日志的算法。图 2 为了参考之用，总结这个算法的简略版本，图 3 列举了这个算法的一些关键特性。图中的这些元素会在剩下的章节逐一介绍。</p><p>Raft 通过选举一个杰出的领导人，然后给予他全部的管理复制日志的责任来实现一致性。领导人从客户端接收日志条目（log entries），把日志条目复制到其他服务器上，并告诉其他的服务器什么时候可以安全地将日志条目应用到他们的状态机中。拥有一个领导人大大简化了对复制日志的管理。例如，领导人可以决定新的日志条目需要放在日志中的什么位置而不需要和其他服务器商议，并且数据都从领导人流向其他服务器。一个领导人可能会发生故障，或者和其他服务器失去连接，在这种情况下一个新的领导人会被选举出来。</p><p>通过领导人的方式，Raft 将一致性问题分解成了三个相对独立的子问题，这些问题会在接下来的子章节中进行讨论：</p><ul><li><strong>领导选举</strong>：当现存的领导人发生故障的时候, 一个新的领导人需要被选举出来（章节 5.2）</li><li><strong>日志复制</strong>：领导人必须从客户端接收日志条目（log entries）然后复制到集群中的其他节点，并强制要求其他节点的日志和自己保持一致。</li><li><strong>安全性</strong>：在 Raft 中安全性的关键是在图 3 中展示的状态机安全：如果有任何的服务器节点已经应用了一个确定的日志条目到它的状态机中，那么其他服务器节点不能在同一个日志索引位置应用一个不同的指令。章节 5.4 阐述了 Raft 算法是如何保证这个特性的；这个解决方案涉及到选举机制（5.2 节）上的一个额外限制。</li></ul><p>在展示一致性算法之后，这一章节会讨论一些可用性的问题和计时在系统中的作用。</p><p><strong>状态</strong>：</p><p>所有服务器上的持久性状态<br>(在响应 RPC 请求之前，已经更新到了稳定的存储设备)</p><table><thead><tr><th>参数</th><th>解释</th></tr></thead><tbody><tr><td>currentTerm</td><td>服务器已知最新的任期（在服务器首次启动时初始化为0，单调递增）</td></tr><tr><td>votedFor</td><td>当前任期内收到选票的 candidateId，如果没有投给任何候选人 则为空</td></tr><tr><td>log[]</td><td>日志条目；每个条目包含了用于状态机的命令，以及领导人接收到该条目时的任期（初始索引为1）</td></tr></tbody></table><p>所有服务器上的易失性状态</p><table><thead><tr><th>参数</th><th>解释</th></tr></thead><tbody><tr><td>commitIndex</td><td>已知已提交的最高的日志条目的索引（初始值为0，单调递增）</td></tr><tr><td>lastApplied</td><td>已经被应用到状态机的最高的日志条目的索引（初始值为0，单调递增）</td></tr></tbody></table><p>领导人（服务器）上的易失性状态<br>(选举后已经重新初始化)</p><table><thead><tr><th>参数</th><th>解释</th></tr></thead><tbody><tr><td>nextIndex[]</td><td>对于每一台服务器，发送到该服务器的下一个日志条目的索引（初始值为领导人最后的日志条目的索引+1）</td></tr><tr><td>matchIndex[]</td><td>对于每一台服务器，已知的已经复制到该服务器的最高日志条目的索引（初始值为0，单调递增）</td></tr></tbody></table><p><strong>追加条目（AppendEntries）RPC</strong>：</p><p>由领导人调用，用于日志条目的复制，同时也被当做心跳使用</p><table><thead><tr><th>参数</th><th>解释</th></tr></thead><tbody><tr><td>term</td><td>领导人的任期</td></tr><tr><td>leaderId</td><td>领导人 ID 因此跟随者可以对客户端进行重定向（译者注：跟随者根据领导人 ID 把客户端的请求重定向到领导人，比如有时客户端把请求发给了跟随者而不是领导人）</td></tr><tr><td>prevLogIndex</td><td>紧邻新日志条目之前的那个日志条目的索引</td></tr><tr><td>prevLogTerm</td><td>紧邻新日志条目之前的那个日志条目的任期</td></tr><tr><td>entries[]</td><td>需要被保存的日志条目（被当做心跳使用时，则日志条目内容为空；为了提高效率可能一次性发送多个）</td></tr><tr><td>leaderCommit</td><td>领导人的已知已提交的最高的日志条目的索引</td></tr></tbody></table><table><thead><tr><th>返回值</th><th>解释</th></tr></thead><tbody><tr><td>term</td><td>当前任期，对于领导人而言 它会更新自己的任期</td></tr><tr><td>success</td><td>如果跟随者所含有的条目和 prevLogIndex 以及 prevLogTerm 匹配上了，则为 true</td></tr></tbody></table><p>接收者的实现：</p><ol><li>返回假 如果领导人的任期小于接收者的当前任期（译者注：这里的接收者是指跟随者或者候选人）（5.1 节）</li><li>返回假 如果接收者日志中没有包含这样一个条目 即该条目的任期在 prevLogIndex 上能和 prevLogTerm 匹配上<br>（译者注：在接收者日志中 如果能找到一个和 prevLogIndex 以及 prevLogTerm 一样的索引和任期的日志条目 则继续执行下面的步骤 否则返回假）（5.3 节）</li><li>如果一个已经存在的条目和新条目（译者注：即刚刚接收到的日志条目）发生了冲突（因为索引相同，任期不同），那么就删除这个已经存在的条目以及它之后的所有条目 （5.3 节）</li><li>追加日志中尚未存在的任何新条目</li><li>如果领导人的已知已提交的最高日志条目的索引大于接收者的已知已提交最高日志条目的索引（<code>leaderCommit &gt; commitIndex</code>），则把接收者的已知已经提交的最高的日志条目的索引commitIndex 重置为 领导人的已知已经提交的最高的日志条目的索引 leaderCommit 或者是 上一个新条目的索引 取两者的最小值</li></ol><p><strong>请求投票（RequestVote）RPC</strong>：</p><p>由候选人负责调用用来征集选票（5.2 节）</p><table><thead><tr><th>参数</th><th>解释</th></tr></thead><tbody><tr><td>term</td><td>候选人的任期号</td></tr><tr><td>candidateId</td><td>请求选票的候选人的 ID</td></tr><tr><td>lastLogIndex</td><td>候选人的最后日志条目的索引值</td></tr><tr><td>lastLogTerm</td><td>候选人最后日志条目的任期号</td></tr></tbody></table><table><thead><tr><th>返回值</th><th>解释</th></tr></thead><tbody><tr><td>term</td><td>当前任期号，以便于候选人去更新自己的任期号</td></tr><tr><td>voteGranted</td><td>候选人赢得了此张选票时为真</td></tr></tbody></table><p>接收者实现：</p><ol><li>如果<code>term &lt; currentTerm</code>返回 false （5.2 节）</li><li>如果 votedFor 为空或者为 candidateId，并且候选人的日志至少和自己一样新，那么就投票给他（5.2 节，5.4 节）</li></ol><p><strong>所有服务器需遵守的规则</strong>：</p><p>所有服务器：</p><ul><li>如果<code>commitIndex &gt; lastApplied</code>，则 lastApplied 递增，并将<code>log[lastApplied]</code>应用到状态机中（5.3 节）</li><li>如果接收到的 RPC 请求或响应中，任期号<code>T &gt; currentTerm</code>，则令 <code>currentTerm = T</code>，并切换为跟随者状态（5.1 节）</li></ul><p>跟随者（5.2 节）：</p><ul><li>响应来自候选人和领导人的请求</li><li>如果在超过选举超时时间的情况之前没有收到<strong>当前领导人</strong>（即该领导人的任期需与这个跟随者的当前任期相同）的心跳&#x2F;附加日志，或者是给某个候选人投了票，就自己变成候选人</li></ul><p>候选人（5.2 节）：</p><ul><li>在转变成候选人后就立即开始选举过程<ul><li>自增当前的任期号（currentTerm）</li><li>给自己投票</li><li>重置选举超时计时器</li><li>发送请求投票的 RPC 给其他所有服务器</li></ul></li><li>如果接收到大多数服务器的选票，那么就变成领导人</li><li>如果接收到来自新的领导人的附加日志（AppendEntries）RPC，则转变成跟随者</li><li>如果选举过程超时，则再次发起一轮选举</li></ul><p>领导人：</p><ul><li>一旦成为领导人：发送空的附加日志（AppendEntries）RPC（心跳）给其他所有的服务器；在一定的空余时间之后不停的重复发送，以防止跟随者超时（5.2 节）</li><li>如果接收到来自客户端的请求：附加条目到本地日志中，在条目被应用到状态机后响应客户端（5.3 节）</li><li>如果对于一个跟随者，最后日志条目的索引值大于等于 nextIndex（<code>lastLogIndex ≥ nextIndex</code>），则发送从 nextIndex 开始的所有日志条目：<ul><li>如果成功：更新相应跟随者的 nextIndex 和 matchIndex</li><li>如果因为日志不一致而失败，则 nextIndex 递减并重试</li></ul></li><li>假设存在 N 满足<code>N &gt; commitIndex</code>，使得大多数的 <code>matchIndex[i] ≥ N</code>以及<code>log[N].term == currentTerm</code> 成立，则令 <code>commitIndex = N</code>（5.3 和 5.4 节）</li></ul><p><img src="D:\桌面\项目\开源项目\Reading-note\kuberneters\ETCD\images\raft-图2.png" alt="图 2"></p><blockquote><p>图 2：一个关于 Raft 一致性算法的浓缩总结（不包括成员变换和日志压缩）。</p></blockquote><table><thead><tr><th>特性</th><th>解释</th></tr></thead><tbody><tr><td>选举安全特性</td><td>对于一个给定的任期号，最多只会有一个领导人被选举出来（5.2 节）</td></tr><tr><td>领导人只附加原则</td><td>领导人绝对不会删除或者覆盖自己的日志，只会增加（5.3 节）</td></tr><tr><td>日志匹配原则</td><td>如果两个日志在某一相同索引位置日志条目的任期号相同，那么我们就认为这两个日志从头到该索引位置之间的内容完全一致（5.3 节）</td></tr><tr><td>领导人完全特性</td><td>如果某个日志条目在某个任期号中已经被提交，那么这个条目必然出现在更大任期号的所有领导人中（5.4 节）</td></tr><tr><td>状态机安全特性</td><td>如果某一服务器已将给定索引位置的日志条目应用至其状态机中，则其他任何服务器在该索引位置不会应用不同的日志条目（5.4.3 节）</td></tr></tbody></table><p><img src="D:\桌面\项目\开源项目\Reading-note\kuberneters\ETCD\images\raft-图3.png" alt="图 3 "></p><blockquote><p>图 3：Raft 在任何时候都保证以上的各个特性。</p></blockquote><h3 id="5-1-Raft-基础"><a href="#5-1-Raft-基础" class="headerlink" title="5.1 Raft 基础"></a>5.1 Raft 基础</h3><p>一个 Raft 集群包含若干个服务器节点；5 个服务器节点是一个典型的例子，这允许整个系统容忍 2 个节点失效。在任何时刻，每一个服务器节点都处于这三个状态之一：领导人、跟随者或者候选人。在通常情况下，系统中只有一个领导人并且其他的节点全部都是跟随者。跟随者都是被动的：他们不会发送任何请求，只是简单的响应来自领导人或者候选人的请求。领导人处理所有的客户端请求（如果一个客户端和跟随者联系，那么跟随者会把请求重定向给领导人）。第三种状态，候选人，是用来在 5.2 节描述的选举新领导人时使用。图 4 展示了这些状态和他们之间的转换关系；这些转换关系会在接下来进行讨论。</p><p><img src="D:\桌面\项目\开源项目\Reading-note\kuberneters\ETCD\images\raft-图4.png" alt="图 4 "></p><blockquote><p>图 4：服务器状态。跟随者只响应来自其他服务器的请求。如果跟随者接收不到消息，那么他就会变成候选人并发起一次选举。获得集群中大多数选票的候选人将成为领导人。在一个任期内，领导人一直都会是领导人，直到自己宕机了。</p></blockquote><p><img src="D:\桌面\项目\开源项目\Reading-note\kuberneters\ETCD\images\raft-图5.png" alt="图 5"></p><blockquote><p>图 5：时间被划分成一个个的任期，每个任期始于一次选举。在选举成功后，领导人会管理整个集群直到任期结束。有时候选举会失败，那么这个任期就会没有领导人而结束。任期之间的切换可以在不同的时间不同的服务器上观察到。</p></blockquote><p>Raft 把时间分割成任意长度的<strong>任期</strong>，如图 5。任期用连续的整数标记。每一段任期从一次<strong>选举</strong>开始，就像章节 5.2 描述的一样，一个或者多个候选人尝试成为领导人。如果一个候选人赢得选举，然后他就在接下来的任期内充当领导人的职责。在某些情况下，一次选举过程会造成选票的瓜分。在这种情况下，这一任期会以没有领导人结束；一个新的任期（和一次新的选举）会很快重新开始。Raft 保证了在一个给定的任期内，最多只有一个领导人。</p><p>不同的服务器节点可能多次观察到任期之间的转换，但在某些情况下，一个节点也可能观察不到任何一次选举或者整个任期全程。任期在 Raft 算法中充当逻辑时钟的作用，任期使得服务器可以检测一些过期的信息：比如过期的领导人。每个节点存储一个当前任期号，这一编号在整个时期内单调递增。每当服务器之间通信的时候都会交换当前任期号；如果一个服务器的当前任期号比其他人小，那么他会更新自己的任期号到较大的任期号值。如果一个候选人或者领导人发现自己的任期号过期了，那么他会立即恢复成跟随者状态。如果一个节点接收到一个包含过期的任期号的请求，那么他会直接拒绝这个请求。</p><p>Raft 算法中服务器节点之间通信使用远程过程调用（RPCs），并且基本的一致性算法只需要两种类型的 RPCs。请求投票（RequestVote） RPCs 由候选人在选举期间发起（章节  5.2），然后附加条目（AppendEntries）RPCs 由领导人发起，用来复制日志和提供一种心跳机制（章节 5.3）。第 7 节为了在服务器之间传输快照增加了第三种 RPC。当服务器没有及时的收到 RPC 的响应时，会进行重试， 并且他们能够并行的发起 RPCs 来获得最佳的性能。</p><h3 id="5-2-领导人选举"><a href="#5-2-领导人选举" class="headerlink" title="5.2 领导人选举"></a>5.2 领导人选举</h3><p>Raft 使用一种心跳机制来触发领导人选举。当服务器程序启动时，他们都是跟随者身份。一个服务器节点继续保持着跟随者状态只要他从领导人或者候选人处接收到有效的 RPCs。领导人周期性的向所有跟随者发送心跳包（即不包含日志项内容的附加条目（AppendEntries） RPCs）来维持自己的权威。如果一个跟随者在一段时间里没有接收到任何消息，也就是<strong>选举超时</strong>，那么他就会认为系统中没有可用的领导人,并且发起选举以选出新的领导人。</p><p>要开始一次选举过程，跟随者先要增加自己的当前任期号并且转换到候选人状态。然后他会并行地向集群中的其他服务器节点发送请求投票的 RPCs 来给自己投票。候选人会继续保持着当前状态直到以下三件事情之一发生：(a) 他自己赢得了这次的选举，(b) 其他的服务器成为领导人，(c) 一段时间之后没有任何一个获胜的人。这些结果会分别的在下面的段落里进行讨论。</p><p>当一个候选人从整个集群的大多数服务器节点获得了针对同一个任期号的选票，那么他就赢得了这次选举并成为领导人。每一个服务器最多会对一个任期号投出一张选票，按照先来先服务的原则（注意：5.4 节在投票上增加了一点额外的限制）。要求大多数选票的规则确保了最多只会有一个候选人赢得此次选举（图 3 中的选举安全性）。一旦候选人赢得选举，他就立即成为领导人。然后他会向其他的服务器发送心跳消息来建立自己的权威并且阻止发起新的选举。</p><p>在等待投票的时候，候选人可能会从其他的服务器接收到声明它是领导人的附加条目（AppendEntries）RPC。如果这个领导人的任期号（包含在此次的 RPC中）不小于候选人当前的任期号，那么候选人会承认领导人合法并回到跟随者状态。 如果此次 RPC 中的任期号比自己小，那么候选人就会拒绝这次的 RPC 并且继续保持候选人状态。</p><p>第三种可能的结果是候选人既没有赢得选举也没有输：如果有多个跟随者同时成为候选人，那么选票可能会被瓜分以至于没有候选人可以赢得大多数人的支持。当这种情况发生的时候，每一个候选人都会超时，然后通过增加当前任期号来开始一轮新的选举。然而，没有其他机制的话，选票可能会被无限的重复瓜分。</p><p>Raft 算法使用随机选举超时时间的方法来确保很少会发生选票瓜分的情况，就算发生也能很快的解决。为了阻止选票起初就被瓜分，选举超时时间是从一个固定的区间（例如 150-300 毫秒）随机选择。这样可以把服务器都分散开以至于在大多数情况下只有一个服务器会选举超时；然后他赢得选举并在其他服务器超时之前发送心跳包。同样的机制被用在选票瓜分的情况下。每一个候选人在开始一次选举的时候会重置一个随机的选举超时时间，然后在超时时间内等待投票的结果；这样减少了在新的选举中另外的选票瓜分的可能性。9.3 节展示了这种方案能够快速的选出一个领导人。</p><p>领导人选举这个例子，体现了可理解性原则是如何指导我们进行方案设计的。起初我们计划使用一种排名系统：每一个候选人都被赋予一个唯一的排名，供候选人之间竞争时进行选择。如果一个候选人发现另一个候选人拥有更高的排名，那么他就会回到跟随者状态，这样高排名的候选人能够更加容易的赢得下一次选举。但是我们发现这种方法在可用性方面会有一点问题（如果高排名的服务器宕机了，那么低排名的服务器可能会超时并再次进入候选人状态。而且如果这个行为发生得足够快，则可能会导致整个选举过程都被重置掉）。我们针对算法进行了多次调整，但是每次调整之后都会有新的问题。最终我们认为随机重试的方法是更加明显和易于理解的。</p><h3 id="5-3-日志复制"><a href="#5-3-日志复制" class="headerlink" title="5.3 日志复制"></a>5.3 日志复制</h3><p>一旦一个领导人被选举出来，他就开始为客户端提供服务。客户端的每一个请求都包含一条被复制状态机执行的指令。领导人把这条指令作为一条新的日志条目附加到日志中去，然后并行地发起附加条目 RPCs 给其他的服务器，让他们复制这条日志条目。当这条日志条目被安全地复制（下面会介绍），领导人会应用这条日志条目到它的状态机中然后把执行的结果返回给客户端。如果跟随者崩溃或者运行缓慢，再或者网络丢包，领导人会不断的重复尝试附加日志条目 RPCs （尽管已经回复了客户端）直到所有的跟随者都最终存储了所有的日志条目。</p><p><img src="D:\桌面\项目\开源项目\Reading-note\kuberneters\ETCD\images\raft-图6.png" alt="图 6"></p><blockquote><p>图 6：日志由有序序号标记的条目组成。每个条目都包含创建时的任期号（图中框中的数字），和一个状态机需要执行的指令。一个条目当可以安全地被应用到状态机中去的时候，就认为是可以提交了。</p></blockquote><p>日志以图 6 展示的方式组织。每一个日志条目存储一条状态机指令和从领导人收到这条指令时的任期号。日志中的任期号用来检查是否出现不一致的情况，同时也用来保证图 3 中的某些性质。每一条日志条目同时也都有一个整数索引值来表明它在日志中的位置。</p><p>领导人来决定什么时候把日志条目应用到状态机中是安全的；这种日志条目被称为<strong>已提交</strong>。Raft 算法保证所有已提交的日志条目都是持久化的并且最终会被所有可用的状态机执行。在领导人将创建的日志条目复制到大多数的服务器上的时候，日志条目就会被提交（例如在图 6 中的条目 7）。同时，领导人的日志中之前的所有日志条目也都会被提交，包括由其他领导人创建的条目。5.4 节会讨论某些当在领导人改变之后应用这条规则的隐晦内容，同时他也展示了这种提交的定义是安全的。领导人跟踪了最大的将会被提交的日志项的索引，并且索引值会被包含在未来的所有附加日志 RPCs （包括心跳包），这样其他的服务器才能最终知道领导人的提交位置。一旦跟随者知道一条日志条目已经被提交，那么他也会将这个日志条目应用到本地的状态机中（按照日志的顺序）。</p><p>我们设计了 Raft 的日志机制来维护不同服务器日志之间的高层次的一致性。这么做不仅简化了系统的行为也使其更具有可预测性，同时它也是安全性保证的一个重要组件。Raft 维护着以下的特性，这些特性共同组成了图 3 中的<strong>日志匹配特性（Log Matching Property）</strong>：</p><ul><li>如果在不同的日志中的两个条目拥有相同的索引和任期号，那么他们存储了相同的指令。</li><li>如果在不同的日志中的两个条目拥有相同的索引和任期号，那么他们之前的所有日志条目也全部相同。</li></ul><p>第一个特性来自这样的一个事实，领导人最多在一个任期里在指定的一个日志索引位置创建一条日志条目，同时日志条目在日志中的位置也从来不会改变。第二个特性由附加日志 RPC 的一个简单的一致性检查所保证。在发送附加日志 RPC 的时候，领导人会把新的日志条目前紧挨着的条目的索引位置和任期号包含在日志内。如果跟随者在它的日志中找不到包含相同索引位置和任期号的条目，那么他就会拒绝接收新的日志条目。一致性检查就像一个归纳步骤：一开始空的日志状态肯定是满足日志匹配特性的，然后一致性检查在日志扩展的时候保护了日志匹配特性。因此，每当附加日志 RPC 返回成功时，领导人就知道跟随者的日志一定是和自己相同的了。</p><p>在正常的操作中，领导人和跟随者的日志保持一致性，所以附加日志 RPC 的一致性检查从来不会失败。然而，领导人崩溃的情况会使得日志处于不一致的状态（老的领导人可能还没有完全复制所有的日志条目）。这种不一致问题会在领导人和跟随者的一系列崩溃下加剧。图 7 展示了跟随者的日志可能和新的领导人不同。跟随者可能会丢失一些在新的领导人中存在的日志条目，他也可能拥有一些领导人没有的日志条目，或者两者都发生。丢失或者多出日志条目可能会持续多个任期。</p><p><img src="D:\桌面\项目\开源项目\Reading-note\kuberneters\ETCD\images\raft-图7.png" alt="图 7"></p><blockquote><p>图 7：当一个领导人成功当选时，跟随者可能是任何情况（a-f）。每一个盒子表示是一个日志条目；里面的数字表示任期号。跟随者可能会缺少一些日志条目（a-b），可能会有一些未被提交的日志条目（c-d），或者两种情况都存在（e-f）。例如，场景 f 可能会这样发生，某服务器在任期 2 的时候是领导人，已附加了一些日志条目到自己的日志中，但在提交之前就崩溃了；很快这个机器就被重启了，在任期 3 重新被选为领导人，并且又增加了一些日志条目到自己的日志中；在任期 2 和任期 3 的日志被提交之前，这个服务器又宕机了，并且在接下来的几个任期里一直处于宕机状态。</p></blockquote><p>在 Raft 算法中，领导人是通过强制跟随者直接复制自己的日志来处理不一致问题的。这意味着在跟随者中的冲突的日志条目会被领导人的日志覆盖。5.4 节会阐述如何通过增加一些限制来使得这样的操作是安全的。</p><p>要使得跟随者的日志进入和自己一致的状态，领导人必须找到最后两者达成一致的地方，然后删除跟随者从那个点之后的所有日志条目，并发送自己在那个点之后的日志给跟随者。所有的这些操作都在进行附加日志 RPCs 的一致性检查时完成。领导人针对每一个跟随者维护了一个 <strong>nextIndex</strong>，这表示下一个需要发送给跟随者的日志条目的索引地址。当一个领导人刚获得权力的时候，他初始化所有的 nextIndex 值为自己的最后一条日志的 index 加 1（图 7 中的 11）。如果一个跟随者的日志和领导人不一致，那么在下一次的附加日志 RPC 时的一致性检查就会失败。在被跟随者拒绝之后，领导人就会减小 nextIndex 值并进行重试。最终 nextIndex 会在某个位置使得领导人和跟随者的日志达成一致。当这种情况发生，附加日志 RPC 就会成功，这时就会把跟随者冲突的日志条目全部删除并且加上领导人的日志。一旦附加日志 RPC 成功，那么跟随者的日志就会和领导人保持一致，并且在接下来的任期里一直继续保持。</p><blockquote><p>如果需要的话，算法可以通过减少被拒绝的附加日志 RPCs 的次数来优化。例如，当附加日志 RPC 的请求被拒绝的时候，跟随者可以(返回)冲突条目的任期号和该任期号对应的最小索引地址。借助这些信息，领导人可以减小 nextIndex 一次性越过该冲突任期的所有日志条目；这样就变成每个任期需要一次附加条目 RPC 而不是每个条目一次。在实践中，我们十分怀疑这种优化是否是必要的，因为失败是很少发生的并且也不大可能会有这么多不一致的日志。</p></blockquote><p>通过这种机制，领导人在获得权力的时候就不需要任何特殊的操作来恢复一致性。他只需要进行正常的操作，然后日志就能在回复附加日志 RPC 的一致性检查失败的时候自动趋于一致。领导人从来不会覆盖或者删除自己的日志（图 3 的领导人只附加特性）。</p><p>日志复制机制展示出了第 2 节中形容的一致性特性：Raft 能够接受，复制并应用新的日志条目只要大部分的机器是工作的；在通常的情况下，新的日志条目可以在一次 RPC 中被复制给集群中的大多数机器；并且单个的缓慢的跟随者不会影响整体的性能。</p><h3 id="5-4-安全性"><a href="#5-4-安全性" class="headerlink" title="5.4 安全性"></a>5.4 安全性</h3><p>前面的章节里描述了 Raft 算法是如何选举和复制日志的。然而，到目前为止描述的机制并不能充分的保证每一个状态机会按照相同的顺序执行相同的指令。例如，一个跟随者可能会进入不可用状态同时领导人已经提交了若干的日志条目，然后这个跟随者可能会被选举为领导人并且覆盖这些日志条目；因此，不同的状态机可能会执行不同的指令序列。</p><p>这一节通过在领导选举的时候增加一些限制来完善 Raft 算法。这一限制保证了任何的领导人对于给定的任期号，都拥有了之前任期的所有被提交的日志条目（图 3 中的领导人完整特性）。增加这一选举时的限制，我们对于提交时的规则也更加清晰。最终，我们将展示对于<strong>领导人完整特性（Leader Completeness Property）</strong> 的简要证明，并且说明该特性是如何引导复制状态机做出正确行为的。</p><h4 id="5-4-1-选举限制"><a href="#5-4-1-选举限制" class="headerlink" title="5.4.1 选举限制"></a>5.4.1 选举限制</h4><p>在任何基于领导人的一致性算法中，领导人都必须存储所有已经提交的日志条目。在某些一致性算法中，例如 Viewstamped Replication，某个节点即使是一开始并没有包含所有已经提交的日志条目，它也能被选为领导人。这些算法都包含一些额外的机制来识别丢失的日志条目并把他们传送给新的领导人，要么是在选举阶段要么在之后很快进行。不幸的是，这种方法会导致相当大的额外的机制和复杂性。Raft 使用了一种更加简单的方法，它可以保证在选举的时候新的领导人拥有所有之前任期中已经提交的日志条目，而不需要传送这些日志条目给领导人。这意味着日志条目的传送是单向的，只从领导人传给跟随者，并且领导人从不会覆盖自身本地日志中已经存在的条目。</p><p>Raft 使用投票的方式来阻止一个候选人赢得选举，除非这个候选人包含了所有已经提交的日志条目。候选人为了赢得选举必须联系集群中的大部分节点，这意味着每一个已经提交的日志条目在这些服务器节点中肯定存在于至少一个节点上。如果候选人的日志至少和大多数的服务器节点一样新（这个新的定义会在下面讨论），那么他一定持有了所有已经提交的日志条目。请求投票（RequestVote） RPC 实现了这样的限制：RPC 中包含了候选人的日志信息，然后投票人会拒绝掉那些日志没有自己新的投票请求。</p><p>Raft 通过比较两份日志中最后一条日志条目的索引值和任期号定义谁的日志比较新。如果两份日志最后的条目的任期号不同，那么任期号大的日志更加新。如果两份日志最后的条目任期号相同，那么日志比较长的那个就更加新。</p><h4 id="5-4-2-提交之前任期内的日志条目"><a href="#5-4-2-提交之前任期内的日志条目" class="headerlink" title="5.4.2 提交之前任期内的日志条目"></a>5.4.2 提交之前任期内的日志条目</h4><p>如同 5.3 节介绍的那样，领导人知道一条当前任期内的日志记录是可以被提交的，只要它被存储到了大多数的服务器上。如果一个领导人在提交日志条目之前崩溃了，未来后续的领导人会继续尝试复制这条日志记录。然而，一个领导人不能断定一个之前任期里的日志条目被保存到大多数服务器上的时候就一定已经提交了。图 8 展示了一种情况，一条已经被存储到大多数节点上的老日志条目，也依然有可能会被未来的领导人覆盖掉。</p><p><img src="D:\桌面\项目\开源项目\Reading-note\kuberneters\ETCD\images\raft-图8.png" alt="图 8"></p><blockquote><p>图 8：如图的时间序列展示了为什么领导人无法决定对老任期号的日志条目进行提交。在 (a) 中，S1 是领导人，部分的(跟随者)复制了索引位置 2 的日志条目。在 (b) 中，S1 崩溃了，然后 S5 在任期 3 里通过 S3、S4 和自己的选票赢得选举，然后从客户端接收了一条不一样的日志条目放在了索引 2 处。然后到 (c)，S5 又崩溃了；S1 重新启动，选举成功，开始复制日志。在这时，来自任期 2 的那条日志已经被复制到了集群中的大多数机器上，但是还没有被提交。如果 S1 在 (d) 中又崩溃了，S5 可以重新被选举成功（通过来自 S2，S3 和 S4 的选票），然后覆盖了他们在索引 2 处的日志。反之，如果在崩溃之前，S1 把自己主导的新任期里产生的日志条目复制到了大多数机器上，就如 (e) 中那样，那么在后面任期里面这些新的日志条目就会被提交（因为 S5 就不可能选举成功）。 这样在同一时刻就同时保证了，之前的所有老的日志条目就会被提交。</p></blockquote><p>为了消除图 8 里描述的情况，Raft 永远不会通过计算副本数目的方式去提交一个之前任期内的日志条目。只有领导人当前任期里的日志条目通过计算副本数目可以被提交；一旦当前任期的日志条目以这种方式被提交，那么由于日志匹配特性，之前的日志条目也都会被间接的提交。在某些情况下，领导人可以安全的知道一个老的日志条目是否已经被提交（例如，该条目是否存储到所有服务器上），但是 Raft 为了简化问题使用一种更加保守的方法。</p><p>当领导人复制之前任期里的日志时，Raft 会为所有日志保留原始的任期号, 这在提交规则上产生了额外的复杂性。在其他的一致性算法中，如果一个新的领导人要重新复制之前的任期里的日志时，它必须使用当前新的任期号。Raft 使用的方法更加容易辨别出日志，因为它可以随着时间和日志的变化对日志维护着同一个任期编号。另外，和其他的算法相比，Raft 中的新领导人只需要发送更少日志条目（其他算法中必须在他们被提交之前发送更多的冗余日志条目来为他们重新编号）。</p><h4 id="5-4-3-安全性论证"><a href="#5-4-3-安全性论证" class="headerlink" title="5.4.3 安全性论证"></a>5.4.3 安全性论证</h4><p>在给定了完整的 Raft 算法之后，我们现在可以更加精确的讨论领导人完整性特性（这一讨论基于 9.2 节的安全性证明）。我们假设领导人完全性特性是不存在的，然后我们推出矛盾来。假设任期 T 的领导人（领导人 T）在任期内提交了一条日志条目，但是这条日志条目没有被存储到未来某个任期的领导人的日志中。设大于 T 的最小任期 U 的领导人 U 没有这条日志条目。</p><p><img src="D:\桌面\项目\开源项目\Reading-note\kuberneters\ETCD\images\raft-图9.png" alt="图 9"></p><blockquote><p>图 9：如果 S1 （任期 T 的领导人）在它的任期里提交了一条新的日志，然后 S5 在之后的任期 U 里被选举为领导人，那么至少会有一个机器，如 S3，既拥有来自 S1 的日志，也给 S5 投票了。</p></blockquote><ol><li>在领导人 U 选举的时候一定没有那条被提交的日志条目（领导人从不会删除或者覆盖任何条目）。</li><li>领导人 T 复制这条日志条目给集群中的大多数节点，同时，领导人 U 从集群中的大多数节点赢得了选票。因此，至少有一个节点（投票者、选民）同时接受了来自领导人 T 的日志条目，并且给领导人 U 投票了，如图 9。这个投票者是产生这个矛盾的关键。</li><li>这个投票者必须在给领导人 U 投票之前先接受了从领导人 T 发来的已经被提交的日志条目；否则他就会拒绝来自领导人 T 的附加日志请求（因为此时他的任期号会比 T 大）。</li><li>投票者在给领导人 U 投票时依然保存有这条日志条目，因为任何中间的领导人都包含该日志条目（根据上述的假设），领导人从不会删除条目，并且跟随者只有在和领导人冲突的时候才会删除条目。</li><li>投票者把自己选票投给领导人 U 时，领导人 U 的日志必须和投票者自己一样新。这就导致了两者矛盾之一。</li><li>首先，如果投票者和领导人 U 的最后一条日志的任期号相同，那么领导人 U 的日志至少和投票者一样长，所以领导人 U 的日志一定包含所有投票者的日志。这是另一处矛盾，因为投票者包含了那条已经被提交的日志条目，但是在上述的假设里，领导人 U 是不包含的。</li><li>除此之外，领导人 U 的最后一条日志的任期号就必须比投票人大了。此外，他也比 T 大，因为投票人的最后一条日志的任期号至少和 T 一样大（他包含了来自任期 T 的已提交的日志）。创建了领导人 U 最后一条日志的之前领导人一定已经包含了那条被提交的日志（根据上述假设，领导人 U 是第一个不包含该日志条目的领导人）。所以，根据日志匹配特性，领导人 U 一定也包含那条被提交的日志，这里产生矛盾。</li><li>这里完成了矛盾。因此，所有比 T 大的领导人一定包含了所有来自 T 的已经被提交的日志。</li><li>日志匹配原则保证了未来的领导人也同时会包含被间接提交的条目，例如图 8 (e) 中的索引 2。</li></ol><p>通过领导人完全特性，我们就能证明图 3 中的状态机安全特性，即如果服务器已经在某个给定的索引值应用了日志条目到自己的状态机里，那么其他的服务器不会应用一个不一样的日志到同一个索引值上。在一个服务器应用一条日志条目到他自己的状态机中时，他的日志必须和领导人的日志，在该条目和之前的条目上相同，并且已经被提交。现在我们来考虑在任何一个服务器应用一个指定索引位置的日志的最小任期；日志完全特性保证拥有更高任期号的领导人会存储相同的日志条目，所以之后的任期里应用某个索引位置的日志条目也会是相同的值。因此，状态机安全特性是成立的。</p><p>最后，Raft 要求服务器按照日志中索引位置顺序应用日志条目。和状态机安全特性结合起来看，这就意味着所有的服务器会应用相同的日志序列集到自己的状态机中，并且是按照相同的顺序。</p><h3 id="5-5-跟随者和候选人崩溃"><a href="#5-5-跟随者和候选人崩溃" class="headerlink" title="5.5 跟随者和候选人崩溃"></a>5.5 跟随者和候选人崩溃</h3><p>到目前为止，我们都只关注了领导人崩溃的情况。跟随者和候选人崩溃后的处理方式比领导人要简单的多，并且他们的处理方式是相同的。如果跟随者或者候选人崩溃了，那么后续发送给他们的 RPCs 都会失败。Raft 中处理这种失败就是简单地通过无限的重试；如果崩溃的机器重启了，那么这些 RPC 就会完整的成功。如果一个服务器在完成了一个 RPC，但是还没有响应的时候崩溃了，那么在他重新启动之后就会再次收到同样的请求。Raft 的 RPCs 都是幂等的，所以这样重试不会造成任何问题。例如一个跟随者如果收到附加日志请求但是他已经包含了这一日志，那么他就会直接忽略这个新的请求。</p><h3 id="5-6-时间和可用性"><a href="#5-6-时间和可用性" class="headerlink" title="5.6 时间和可用性"></a>5.6 时间和可用性</h3><p>Raft 的要求之一就是安全性不能依赖时间：整个系统不能因为某些事件运行的比预期快一点或者慢一点就产生了错误的结果。但是，可用性（系统可以及时的响应客户端）不可避免的要依赖于时间。例如，如果消息交换比服务器故障间隔时间长，候选人将没有足够长的时间来赢得选举；没有一个稳定的领导人，Raft 将无法工作。</p><p>领导人选举是 Raft 中对时间要求最为关键的方面。Raft 可以选举并维持一个稳定的领导人,只要系统满足下面的时间要求：</p><blockquote><p>广播时间（broadcastTime）  &lt;&lt;  选举超时时间（electionTimeout） &lt;&lt;  平均故障间隔时间（MTBF）</p></blockquote><p>在这个不等式中，广播时间指的是从一个服务器并行的发送 RPCs 给集群中的其他服务器并接收响应的平均时间；选举超时时间就是在 5.2 节中介绍的选举的超时时间限制；然后平均故障间隔时间就是对于一台服务器而言，两次故障之间的平均时间。广播时间必须比选举超时时间小一个量级，这样领导人才能够发送稳定的心跳消息来阻止跟随者开始进入选举状态；通过随机化选举超时时间的方法，这个不等式也使得选票瓜分的情况变得不可能。选举超时时间应该要比平均故障间隔时间小上几个数量级，这样整个系统才能稳定的运行。当领导人崩溃后，整个系统会大约相当于选举超时的时间里不可用；我们希望这种情况在整个系统的运行中很少出现。</p><p>广播时间和平均故障间隔时间是由系统决定的，但是选举超时时间是我们自己选择的。Raft 的 RPCs 需要接收方将信息持久化的保存到稳定存储中去，所以广播时间大约是 0.5 毫秒到 20 毫秒，取决于存储的技术。因此，选举超时时间可能需要在 10 毫秒到 500 毫秒之间。大多数的服务器的平均故障间隔时间都在几个月甚至更长，很容易满足时间的需求。</p><h2 id="6-集群成员变化"><a href="#6-集群成员变化" class="headerlink" title="6 集群成员变化"></a>6 集群成员变化</h2><p>到目前为止，我们都假设集群的配置（加入到一致性算法的服务器集合）是固定不变的。但是在实践中，偶尔是会改变集群的配置的，例如替换那些宕机的机器或者改变复制级别。尽管可以通过暂停整个集群，更新所有配置，然后重启整个集群的方式来实现，但是在更改的时候集群会不可用。另外，如果存在手工操作步骤，那么就会有操作失误的风险。为了避免这样的问题，我们决定自动化配置改变并且将其纳入到 Raft 一致性算法中来。</p><p>为了让配置修改机制能够安全，那么在转换的过程中不能够存在任何时间点使得两个领导人在同一个任期里同时被选举成功。不幸的是，任何服务器直接从旧的配置直接转换到新的配置的方案都是不安全的。一次性原子地转换所有服务器是不可能的，所以在转换期间整个集群存在划分成两个独立的大多数群体的可能性（见图 10）。</p><p><img src="D:\桌面\项目\开源项目\Reading-note\kuberneters\ETCD\images\raft-图10.png" alt="图 10"></p><blockquote><p>图 10：直接从一种配置转到新的配置是十分不安全的，因为各个机器可能在任何的时候进行转换。在这个例子中，集群配额从 3 台机器变成了 5 台。不幸的是，存在这样的一个时间点，两个不同的领导人在同一个任期里都可以被选举成功。一个是通过旧的配置，一个通过新的配置。</p></blockquote><p>为了保证安全性，配置更改必须使用两阶段方法。目前有很多种两阶段的实现。例如，有些系统在第一阶段停掉旧的配置所以集群就不能处理客户端请求；然后在第二阶段在启用新的配置。在 Raft 中，集群先切换到一个过渡的配置，我们称之为共同一致（<em>joint consensus</em>)；一旦共同一致已经被提交了，那么系统就切换到新的配置上。共同一致是老配置和新配置的结合：</p><ul><li>日志条目被复制给集群中新、老配置的所有服务器。</li><li>新、旧配置的服务器都可以成为领导人。</li><li>达成一致（针对选举和提交）需要分别在两种配置上获得大多数的支持。</li></ul><p>共同一致允许独立的服务器在不影响安全性的前提下，在不同的时间进行配置转换过程。此外，共同一致可以让集群在配置转换的过程中依然响应客户端的请求。</p><p>集群配置在复制日志中以特殊的日志条目来存储和通信；图 11 展示了配置转换的过程。当一个领导人接收到一个改变配置从 C-old 到 C-new 的请求，他会为了共同一致存储配置（图中的 C-old,new），以前面描述的日志条目和副本的形式。一旦一个服务器将新的配置日志条目增加到它的日志中，他就会用这个配置来做出未来所有的决定（服务器总是使用最新的配置，无论他是否已经被提交）。这意味着领导人要使用  C-old,new 的规则来决定日志条目 C-old,new 什么时候需要被提交。如果领导人崩溃了，被选出来的新领导人可能是使用 C-old 配置也可能是 C-old,new 配置，这取决于赢得选举的候选人是否已经接收到了 C-old,new 配置。在任何情况下， C-new 配置在这一时期都不会单方面的做出决定。</p><p>一旦 C-old,new 被提交，那么无论是 C-old 还是 C-new，如果不经过另一个配置的允许都不能单独做出决定，并且领导人完全特性保证了只有拥有 C-old,new 日志条目的服务器才有可能被选举为领导人。这个时候，领导人创建一条关于 C-new 配置的日志条目并复制给集群就是安全的了。再者，每个服务器在见到新的配置的时候就会立即生效。当新的配置在 C-new 的规则下被提交，旧的配置就变得无关紧要，同时不使用新的配置的服务器就可以被关闭了。如图 11，C-old 和 C-new 没有任何机会同时做出单方面的决定；这保证了安全性。</p><p><img src="D:\桌面\项目\开源项目\Reading-note\kuberneters\ETCD\images\raft-图11.png" alt="图 11"></p><blockquote><p>图 11：一个配置切换的时间线。虚线表示已经被创建但是还没有被提交的配置日志条目，实线表示最后被提交的配置日志条目。领导人首先创建了 C-old,new 的配置条目在自己的日志中，并提交到 C-old,new 中（C-old 的大多数和  C-new 的大多数）。然后他创建 C-new 条目并提交到 C-new 中的大多数。这样就不存在  C-new 和 C-old 可以同时做出决定的时间点。</p></blockquote><p>在关于重新配置还有三个问题需要提出。第一个问题是，新的服务器可能初始化没有存储任何的日志条目。当这些服务器以这种状态加入到集群中，那么他们需要一段时间来更新追赶，这时还不能提交新的日志条目。为了避免这种可用性的间隔时间，Raft 在配置更新之前使用了一种额外的阶段，在这个阶段，新的服务器以没有投票权身份加入到集群中来（领导人复制日志给他们，但是不考虑他们是大多数）。一旦新的服务器追赶上了集群中的其他机器，重新配置可以像上面描述的一样处理。</p><p>第二个问题是，集群的领导人可能不是新配置的一员。在这种情况下，领导人就会在提交了 C-new 日志之后退位（回到跟随者状态）。这意味着有这样的一段时间，领导人管理着集群，但是不包括他自己；他复制日志但是不把他自己算作是大多数之一。当 C-new 被提交时，会发生领导人过渡，因为这时是最早新的配置可以独立工作的时间点（将总是能够在 C-new 配置下选出新的领导人）。在此之前，可能只能从 C-old 中选出领导人。</p><p>第三个问题是，移除不在 C-new 中的服务器可能会扰乱集群。这些服务器将不会再接收到心跳，所以当选举超时，他们就会进行新的选举过程。他们会发送拥有新的任期号的请求投票 RPCs，这样会导致当前的领导人回退成跟随者状态。新的领导人最终会被选出来，但是被移除的服务器将会再次超时，然后这个过程会再次重复，导致整体可用性大幅降低。</p><p>为了避免这个问题，当服务器确认当前领导人存在时，服务器会忽略请求投票 RPCs。确切地说，当服务器在当前最小选举超时时间内收到一个请求投票 RPC，他不会更新当前的任期号或者投出选票。这不会影响正常的选举，每个服务器在开始一次选举之前，至少等待一个最小选举超时时间。然而，这有利于避免被移除的服务器扰乱：如果领导人能够发送心跳给集群，那么他就不会被更大的任期号废黜。</p><h2 id="7-日志压缩"><a href="#7-日志压缩" class="headerlink" title="7 日志压缩"></a>7 日志压缩</h2><p>Raft 的日志在正常操作中不断地增长，但是在实际的系统中，日志不能无限制地增长。随着日志不断增长，他会占用越来越多的空间，花费越来越多的时间来重置。如果没有一定的机制去清除日志里积累的陈旧的信息，那么会带来可用性问题。</p><p>快照是最简单的压缩方法。在快照系统中，整个系统的状态都以快照的形式写入到稳定的持久化存储中，然后到那个时间点之前的日志全部丢弃。快照技术被使用在 Chubby 和 ZooKeeper 中，接下来的章节会介绍 Raft 中的快照技术。</p><p>增量压缩的方法，例如日志清理或者日志结构合并树，都是可行的。这些方法每次只对一小部分数据进行操作，这样就分散了压缩的负载压力。首先，他们先选择一个已经积累的大量已经被删除或者被覆盖对象的区域，然后重写那个区域还活跃的对象，之后释放那个区域。和简单操作整个数据集合的快照相比，需要增加复杂的机制来实现。状态机可以实现 LSM tree 使用和快照相同的接口，但是日志清除方法就需要修改 Raft 了。</p><p><img src="D:\桌面\项目\开源项目\Reading-note\kuberneters\ETCD\images\raft-图12.png" alt="图 12"></p><blockquote><p>图 12：一个服务器用新的快照替换了从 1 到 5 的条目，快照值存储了当前的状态。快照中包含了最后的索引位置和任期号。</p></blockquote><p>图 12 展示了 Raft 中快照的基础思想。每个服务器独立地创建快照，只包括已经被提交的日志。主要的工作包括将状态机的状态写入到快照中。Raft 也包含一些少量的元数据到快照中：<strong>最后被包含索引</strong>指的是被快照取代的最后的条目在日志中的索引值（状态机最后应用的日志），<strong>最后被包含的任期</strong>指的是该条目的任期号。保留这些数据是为了支持快照后紧接着的第一个条目的附加日志请求时的一致性检查，因为这个条目需要前一日志条目的索引值和任期号。为了支持集群成员更新（第 6 节），快照中也将最后的一次配置作为最后一个条目存下来。一旦服务器完成一次快照，他就可以删除最后索引位置之前的所有日志和快照了。</p><p>尽管通常服务器都是独立地创建快照，但是领导人必须偶尔的发送快照给一些落后的跟随者。这通常发生在当领导人已经丢弃了下一条需要发送给跟随者的日志条目的时候。幸运的是这种情况不是常规操作：一个与领导人保持同步的跟随者通常都会有这个条目。然而一个运行非常缓慢的跟随者或者新加入集群的服务器（第 6 节）将不会有这个条目。这时让这个跟随者更新到最新的状态的方式就是通过网络把快照发送给他们。</p><p><strong>安装快照 RPC</strong>：</p><p>由领导人调用以将快照的分块发送给跟随者。领导人总是按顺序发送分块。</p><table><thead><tr><th>参数</th><th>解释</th></tr></thead><tbody><tr><td>term</td><td>领导人的任期号</td></tr><tr><td>leaderId</td><td>领导人的 ID，以便于跟随者重定向请求</td></tr><tr><td>lastIncludedIndex</td><td>快照中包含的最后日志条目的索引值</td></tr><tr><td>lastIncludedTerm</td><td>快照中包含的最后日志条目的任期号</td></tr><tr><td>offset</td><td>分块在快照中的字节偏移量</td></tr><tr><td>data[]</td><td>从偏移量开始的快照分块的原始字节</td></tr><tr><td>done</td><td>如果这是最后一个分块则为 true</td></tr></tbody></table><table><thead><tr><th>结果</th><th>解释</th></tr></thead><tbody><tr><td>term</td><td>当前任期号（currentTerm），便于领导人更新自己</td></tr></tbody></table><p><strong>接收者实现</strong>：</p><ol><li>如果<code>term &lt; currentTerm</code>就立即回复</li><li>如果是第一个分块（offset 为 0）就创建一个新的快照</li><li>在指定偏移量写入数据</li><li>如果 done 是 false，则继续等待更多的数据</li><li>保存快照文件，丢弃具有较小索引的任何现有或部分快照</li><li>如果现存的日志条目与快照中最后包含的日志条目具有相同的索引值和任期号，则保留其后的日志条目并进行回复</li><li>丢弃整个日志</li><li>使用快照重置状态机（并加载快照的集群配置）</li></ol><p><img src="D:\桌面\项目\开源项目\Reading-note\kuberneters\ETCD\images\raft-图13.png" alt="图 13 "></p><blockquote><p>图 13：一个关于安装快照的简要概述。为了便于传输，快照都是被分成分块的；每个分块都给了跟随者生命的迹象，所以跟随者可以重置选举超时计时器。</p></blockquote><p>在这种情况下领导人使用一种叫做安装快照的新的 RPC 来发送快照给太落后的跟随者；见图 13。当跟随者通过这种  RPC 接收到快照时，他必须自己决定对于已经存在的日志该如何处理。通常快照会包含没有在接收者日志中存在的信息。在这种情况下，跟随者丢弃其整个日志；它全部被快照取代，并且可能包含与快照冲突的未提交条目。如果接收到的快照是自己日志的前面部分（由于网络重传或者错误），那么被快照包含的条目将会被全部删除，但是快照后面的条目仍然有效，必须保留。</p><p>这种快照的方式背离了 Raft 的强领导人原则，因为跟随者可以在不知道领导人情况下创建快照。但是我们认为这种背离是值得的。领导人的存在，是为了解决在达成一致性的时候的冲突，但是在创建快照的时候，一致性已经达成，这时不存在冲突了，所以没有领导人也是可以的。数据依然是从领导人传给跟随者，只是跟随者可以重新组织他们的数据了。</p><p>我们考虑过一种替代的基于领导人的快照方案，即只有领导人创建快照，然后发送给所有的跟随者。但是这样做有两个缺点。第一，发送快照会浪费网络带宽并且延缓了快照处理的时间。每个跟随者都已经拥有了所有产生快照需要的信息，而且很显然，自己从本地的状态中创建快照比通过网络接收别人发来的要经济。第二，领导人的实现会更加复杂。例如，领导人需要发送快照的同时并行的将新的日志条目发送给跟随者，这样才不会阻塞新的客户端请求。</p><p>还有两个问题影响了快照的性能。首先，服务器必须决定什么时候应该创建快照。如果快照创建的过于频繁，那么就会浪费大量的磁盘带宽和其他资源；如果创建快照频率太低，他就要承受耗尽存储容量的风险，同时也增加了从日志重建的时间。一个简单的策略就是当日志大小达到一个固定大小的时候就创建一次快照。如果这个阈值设置的显著大于期望的快照的大小，那么快照对磁盘压力的影响就会很小了。</p><p>第二个影响性能的问题就是写入快照需要花费显著的一段时间，并且我们还不希望影响到正常操作。解决方案是通过写时复制的技术，这样新的更新就可以被接收而不影响到快照。例如，具有函数式数据结构的状态机天然支持这样的功能。另外，操作系统的写时复制技术的支持（如 Linux 上的 fork）可以被用来创建完整的状态机的内存快照（我们的实现就是这样的）。</p><h2 id="8-客户端交互"><a href="#8-客户端交互" class="headerlink" title="8 客户端交互"></a>8 客户端交互</h2><p>这一节将介绍客户端是如何和 Raft 进行交互的，包括客户端如何发现领导人和 Raft 是如何支持线性化语义的。这些问题对于所有基于一致性的系统都存在，并且 Raft 的解决方案和其他的也差不多。</p><p>Raft 中的客户端发送所有请求给领导人。当客户端启动的时候，他会随机挑选一个服务器进行通信。如果客户端第一次挑选的服务器不是领导人，那么那个服务器会拒绝客户端的请求并且提供他最近接收到的领导人的信息（附加条目请求包含了领导人的网络地址）。如果领导人已经崩溃了，那么客户端的请求就会超时；客户端之后会再次重试随机挑选服务器的过程。</p><p>我们 Raft 的目标是要实现线性化语义（每一次操作立即执行，只执行一次，在他调用和收到回复之间）。但是，如上述，Raft 是可能执行同一条命令多次的：例如，如果领导人在提交了这条日志之后，但是在响应客户端之前崩溃了，那么客户端会和新的领导人重试这条指令，导致这条命令就被再次执行了。解决方案就是客户端对于每一条指令都赋予一个唯一的序列号。然后，状态机跟踪每条指令最新的序列号和相应的响应。如果接收到一条指令，它的序列号已经被执行了，那么就立即返回结果，而不重新执行指令。</p><p>只读的操作可以直接处理而不需要记录日志。但是，在不增加任何限制的情况下，这么做可能会冒着返回脏数据的风险，因为响应客户端请求的领导人可能在他不知道的时候已经被新的领导人取代了。线性化的读操作必须不能返回脏数据，Raft 需要使用两个额外的措施在不使用日志的情况下保证这一点。首先，领导人必须有关于被提交日志的最新信息。领导人完全特性保证了领导人一定拥有所有已经被提交的日志条目，但是在他任期开始的时候，他可能不知道哪些是已经被提交的。为了知道这些信息，他需要在他的任期里提交一条日志条目。Raft 中通过领导人在任期开始的时候提交一个空白的没有任何操作的日志条目到日志中去来实现。第二，领导人在处理只读的请求之前必须检查自己是否已经被废黜了（他自己的信息已经变脏了如果一个更新的领导人被选举出来）。Raft 中通过让领导人在响应只读请求之前，先和集群中的大多数节点交换一次心跳信息来处理这个问题。可选的，领导人可以依赖心跳机制来实现一种租约的机制，但是这种方法依赖时间来保证安全性（假设时间误差是有界的）。</p><h2 id="9-算法实现和评估"><a href="#9-算法实现和评估" class="headerlink" title="9 算法实现和评估"></a>9 算法实现和评估</h2><p>我们已经为 RAMCloud 实现了 Raft 算法作为存储配置信息的复制状态机的一部分，并且帮助 RAMCloud 协调故障转移。这个 Raft 实现包含大约 2000 行 C++ 代码，其中不包括测试、注释和空行。这些代码是开源的。同时也有大约 25 个其他独立的第三方的基于这篇论文草稿的开源实现，针对不同的开发场景。同时，很多公司已经部署了基于 Raft 的系统。</p><p>这一节会从三个方面来评估 Raft 算法：可理解性、正确性和性能。</p><h3 id="9-1-可理解性"><a href="#9-1-可理解性" class="headerlink" title="9.1 可理解性"></a>9.1 可理解性</h3><p>为了和 Paxos 比较 Raft 算法的可理解能力，我们针对高层次的本科生和研究生，在斯坦福大学的高级操作系统课程和加州大学伯克利分校的分布式计算课程上，进行了一次学习的实验。我们分别拍了针对 Raft 和 Paxos 的视频课程，并准备了相应的小测验。Raft 的视频讲课覆盖了这篇论文的所有内容除了日志压缩；Paxos 讲课包含了足够的资料来创建一个等价的复制状态机，包括单决策 Paxos，多决策 Paxos，重新配置和一些实际系统需要的性能优化（例如领导人选举）。小测验测试一些对算法的基本理解和解释一些边角的示例。每个学生都是看完第一个视频，回答相应的测试，再看第二个视频，回答相应的测试。大约有一半的学生先进行 Paxos 部分，然后另一半先进行 Raft 部分，这是为了说明两者从第一部分的算法学习中获得的表现和经验的差异。我们计算参加人员的每一个小测验的得分来看参与者是否在 Raft 算法上更加容易理解。</p><p>我们尽可能的使得 Paxos 和 Raft 的比较更加公平。这个实验偏爱 Paxos 表现在两个方面：43 个参加者中有 15 个人在之前有一些  Paxos 的经验，并且 Paxos 的视频要长 14%。如表格 1 总结的那样，我们采取了一些措施来减轻这种潜在的偏见。我们所有的材料都可供审查。</p><table><thead><tr><th>关心</th><th>缓和偏见采取的手段</th><th>可供查看的材料</th></tr></thead><tbody><tr><td>相同的讲课质量</td><td>两者使用同一个讲师。Paxos 使用的是现在很多大学里经常使用的。Paxos 会长 14%。</td><td>视频</td></tr><tr><td>相同的测验难度</td><td>问题以难度分组，在两个测验里成对出现。</td><td>小测验</td></tr><tr><td>公平评分</td><td>使用评价量规。随机顺序打分，两个测验交替进行。</td><td>评价量规（rubric）</td></tr></tbody></table><blockquote><p>表 1：考虑到可能会存在的偏见，对于每种情况的解决方法，和相应的材料。</p></blockquote><p>参加者平均在 Raft 的测验中比 Paxos 高 4.9 分（总分 60，那么 Raft 的平均得分是 25.7，而 Paxos 是 20.8）；图 14 展示了每个参与者的得分。配置t-检验（又称student‘s t-test）表明，在 95% 的可信度下，真实的 Raft 分数分布至少比 Paxos 高 2.5 分。</p><p><img src="D:\桌面\项目\开源项目\Reading-note\kuberneters\ETCD\images\raft-图14.png" alt="图 14"></p><blockquote><p>图 14：一个散点图表示了 43 个学生在 Paxos 和 Raft 的小测验中的成绩。在对角线之上的点表示在 Raft 获得了更高分数的学生。</p></blockquote><p>我们也建立了一个线性回归模型来预测一个新的学生的测验成绩，基于以下三个因素：他们使用的是哪个小测验，之前对 Paxos 的经验，和学习算法的顺序。模型预测，对小测验的选择会产生 12.5 分的差别。这显著的高于之前的 4.9 分，因为很多学生在之前都已经有了对于 Paxos 的经验，这相当明显的帮助 Paxos，对 Raft 就没什么太大影响了。但是奇怪的是，模型预测对于先进行 Paxos 小测验的人而言，Raft的得分低了6.3分; 虽然我们不知道为什么，这似乎在统计上是有意义的。</p><p>我们同时也在测验之后调查了参与者，他们认为哪个算法更加容易实现和解释；这个的结果在图 15 上。压倒性的结果表明 Raft 算法更加容易实现和解释（41 人中的 33个）。但是，这种自己报告的结果不如参与者的成绩更加可信，并且参与者可能因为我们的 Raft 更加易于理解的假说而产生偏见。</p><p><img src="D:\桌面\项目\开源项目\Reading-note\kuberneters\ETCD\images\raft-图15.png" alt="图 15"></p><blockquote><p>图 15：通过一个 5 分制的问题，参与者（左边）被问哪个算法他们觉得在一个高效正确的系统里更容易实现，右边被问哪个更容易向学生解释。</p></blockquote><p>关于 Raft 用户学习有一个更加详细的讨论。</p><h3 id="9-2-正确性"><a href="#9-2-正确性" class="headerlink" title="9.2 正确性"></a>9.2 正确性</h3><p>在第 5 节，我们已经制定了正式的规范，和对一致性机制的安全性证明。这个正式规范使用 TLA+ 规范语言使图 2 中总结的信息非常清晰。它长约400行，并作为证明的主题。同时对于任何想实现 Raft 的人也是十分有用的。我们通过 TLA 证明系统非常机械的证明了日志完全特性。然而，这个证明依赖的约束前提还没有被机械证明（例如，我们还没有证明规范的类型安全）。而且，我们已经写了一个非正式的证明关于状态机安全性是完备的，并且是相当清晰的（大约 3500 个词）。</p><h3 id="9-3-性能"><a href="#9-3-性能" class="headerlink" title="9.3 性能"></a>9.3 性能</h3><p>Raft 和其他一致性算法例如 Paxos 有着差不多的性能。在性能方面，最重要的关注点是，当领导人被选举成功时，什么时候复制新的日志条目。Raft 通过很少数量的消息包（一轮从领导人到集群大多数机器的消息）就达成了这个目的。同时，进一步提升 Raft 的性能也是可行的。例如，很容易通过支持批量操作和管道操作来提高吞吐量和降低延迟。对于其他一致性算法已经提出过很多性能优化方案；其中有很多也可以应用到 Raft 中来，但是我们暂时把这个问题放到未来的工作中去。</p><p>我们使用我们自己的 Raft 实现来衡量 Raft 领导人选举的性能并且回答两个问题。首先，领导人选举的过程收敛是否快速？第二，在领导人宕机之后，最小的系统宕机时间是多久？</p><p><img src="D:\桌面\项目\开源项目\Reading-note\kuberneters\ETCD\images\raft-图16.png" alt="图 16"></p><blockquote><p>图 16：发现并替换一个已经崩溃的领导人的时间。上面的图考察了在选举超时时间上的随机化程度，下面的图考察了最小选举超时时间。每条线代表了 1000 次实验（除了 150-150 毫秒只试了 100 次），和相应的确定的选举超时时间。例如，150-155 毫秒意思是，选举超时时间从这个区间范围内随机选择并确定下来。这个实验在一个拥有 5 个节点的集群上进行，其广播时延大约是 15 毫秒。对于 9 个节点的集群，结果也差不多。</p></blockquote><p>为了衡量领导人选举，我们反复的使一个拥有五个节点的服务器集群的领导人宕机，并计算需要多久才能发现领导人已经宕机并选出一个新的领导人（见图 16）。为了构建一个最坏的场景，在每一的尝试里，服务器都有不同长度的日志，意味着有些候选人是没有成为领导人的资格的。另外，为了促成选票瓜分的情况，我们的测试脚本在终止领导人之前同步的发送了一次心跳广播（这大约和领导人在崩溃前复制一个新的日志给其他机器很像）。领导人均匀的随机的在心跳间隔里宕机，也就是最小选举超时时间的一半。因此，最小宕机时间大约就是最小选举超时时间的一半。</p><p>图 16 中上面的图表明，只需要在选举超时时间上使用很少的随机化就可以大大避免选票被瓜分的情况。在没有随机化的情况下，在我们的测试里，选举过程往往都需要花费超过 10 秒钟由于太多的选票瓜分的情况。仅仅增加 5 毫秒的随机化时间，就大大的改善了选举过程，现在平均的宕机时间只有 287 毫秒。增加更多的随机化时间可以大大改善最坏情况：通过增加 50 毫秒的随机化时间，最坏的完成情况（1000 次尝试）只要 513 毫秒。</p><p>图 16 中下面的图显示，通过减少选举超时时间可以减少系统的宕机时间。在选举超时时间为 12-24 毫秒的情况下，只需要平均 35 毫秒就可以选举出新的领导人（最长的一次花费了 152 毫秒）。然而，进一步降低选举超时时间的话就会违反 Raft 的时间不等式需求：在选举新领导人之前，领导人就很难发送完心跳包。这会导致没有意义的领导人改变并降低了系统整体的可用性。我们建议使用更为保守的选举超时时间，比如 150-300 毫秒；这样的时间不大可能导致没有意义的领导人改变，而且依然提供不错的可用性。</p><h2 id="10-相关工作"><a href="#10-相关工作" class="headerlink" title="10 相关工作"></a>10 相关工作</h2><p>已经有很多关于一致性算法的工作被发表出来，其中很多都可以归到下面的类别中：</p><ul><li>Lamport 关于 Paxos 的原始描述，和尝试描述的更清晰。</li><li>关于 Paxos 的更详尽的描述，补充遗漏的细节并修改算法，使得可以提供更加容易的实现基础。</li><li>实现一致性算法的系统，例如 Chubby，ZooKeeper 和 Spanner。对于 Chubby 和 Spanner 的算法并没有公开发表其技术细节，尽管他们都声称是基于 Paxos 的。ZooKeeper 的算法细节已经发表，但是和 Paxos 着实有着很大的差别。</li><li>Paxos 可以应用的性能优化。</li><li>Oki 和 Liskov 的 Viewstamped Replication（VR），一种和 Paxos 差不多的替代算法。原始的算法描述和分布式传输协议耦合在了一起，但是核心的一致性算法在最近的更新里被分离了出来。VR 使用了一种基于领导人的方法，和 Raft 有很多相似之处。</li></ul><p>Raft 和 Paxos 最大的不同之处就在于 Raft 的强领导特性：Raft 使用领导人选举作为一致性协议里必不可少的部分，并且将尽可能多的功能集中到了领导人身上。这样就可以使得算法更加容易理解。例如，在 Paxos 中，领导人选举和基本的一致性协议是正交的：领导人选举仅仅是性能优化的手段，而且不是一致性所必须要求的。但是，这样就增加了多余的机制：Paxos 同时包含了针对基本一致性要求的两阶段提交协议和针对领导人选举的独立的机制。相比较而言，Raft 就直接将领导人选举纳入到一致性算法中，并作为两阶段一致性的第一步。这样就减少了很多机制。</p><p>像 Raft 一样，VR 和 ZooKeeper 也是基于领导人的，因此他们也拥有一些 Raft 的优点。但是，Raft 比 VR 和 ZooKeeper 拥有更少的机制因为 Raft 尽可能的减少了非领导人的功能。例如，Raft 中日志条目都遵循着从领导人发送给其他人这一个方向：附加条目 RPC 是向外发送的。在 VR 中，日志条目的流动是双向的（领导人可以在选举过程中接收日志）；这就导致了额外的机制和复杂性。根据 ZooKeeper 公开的资料看，它的日志条目也是双向传输的，但是它的实现更像 Raft。</p><p>和上述我们提及的其他基于一致性的日志复制算法中，Raft 的消息类型更少。例如，我们数了一下 VR 和 ZooKeeper 使用的用来基本一致性需要和成员改变的消息数（排除了日志压缩和客户端交互，因为这些都比较独立且和算法关系不大）。VR 和 ZooKeeper 都分别定义了 10 种不同的消息类型，相对的，Raft 只有 4 种消息类型（两种 RPC 请求和对应的响应）。Raft 的消息都稍微比其他算法的要信息量大，但是都很简单。另外，VR 和 ZooKeeper 都在领导人改变时传输了整个日志；所以为了能够实践中使用，额外的消息类型就很必要了。</p><p>Raft 的强领导人模型简化了整个算法，但是同时也排斥了一些性能优化的方法。例如，平等主义 Paxos （EPaxos）在某些没有领导人的情况下可以达到很高的性能。平等主义 Paxos 充分发挥了在状态机指令中的交换性。任何服务器都可以在一轮通信下就提交指令，除非其他指令同时被提出了。然而，如果指令都是并发的被提出，并且互相之间不通信沟通，那么 EPaxos 就需要额外的一轮通信。因为任何服务器都可以提交指令，所以 EPaxos 在服务器之间的负载均衡做的很好，并且很容易在 WAN 网络环境下获得很低的延迟。但是，他在 Paxos 上增加了非常明显的复杂性。</p><p>一些集群成员变换的方法已经被提出或者在其他的工作中被实现，包括 Lamport 的原始的讨论，VR 和 SMART。我们选择使用共同一致的方法因为他对一致性协议的其他部分影响很小，这样我们只需要很少的一些机制就可以实现成员变换。Lamport 的基于 α 的方法之所以没有被 Raft 选择是因为它假设在没有领导人的情况下也可以达到一致性。和 VR 和 SMART 相比较，Raft 的重新配置算法可以在不限制正常请求处理的情况下进行；相比较的，VR 需要停止所有的处理过程，SMART 引入了一个和 α 类似的方法，限制了请求处理的数量。Raft 的方法同时也需要更少的额外机制来实现，和 VR、SMART 比较而言。</p><h2 id="11-结论"><a href="#11-结论" class="headerlink" title="11 结论"></a>11 结论</h2><p>算法的设计通常会把正确性，效率或者简洁作为主要的目标。尽管这些都是很有意义的目标，但是我们相信，可理解性也是一样的重要。在开发者把算法应用到实际的系统中之前，这些目标没有一个会被实现，这些都会必然的偏离发表时的形式。除非开发人员对这个算法有着很深的理解并且有着直观的感觉，否则将会对他们而言很难在实现的时候保持原有期望的特性。</p><p>在这篇论文中，我们尝试解决分布式一致性问题，但是一个广为接受但是十分令人费解的算法 Paxos 已经困扰了无数学生和开发者很多年了。我们创造了一种新的算法 Raft，显而易见的比 Paxos 要容易理解。我们同时也相信，Raft 也可以为实际的实现提供坚实的基础。把可理解性作为设计的目标改变了我们设计 Raft 的方式；随着设计的进展，我们发现自己重复使用了一些技术，比如分解问题和简化状态空间。这些技术不仅提升了 Raft 的可理解性，同时也使我们坚信其正确性。</p><h2 id="12-感谢"><a href="#12-感谢" class="headerlink" title="12 感谢"></a>12 感谢</h2><p>这项研究必须感谢以下人员的支持：Ali Ghodsi，David Mazie`res，和伯克利 CS 294-91 课程、斯坦福 CS 240 课程的学生。Scott Klemmer 帮我们设计了用户调查，Nelson Ray 建议我们进行统计学的分析。在用户调查时使用的关于 Paxos 的幻灯片很大一部分是从 Lorenzo Alvisi 的幻灯片上借鉴过来的。特别的，非常感谢 DavidMazieres 和 Ezra Hoch，他们找到了 Raft 中一些难以发现的漏洞。许多人提供了关于这篇论文十分有用的反馈和用户调查材料，包括 Ed Bugnion，Michael Chan，Hugues Evrard，Daniel Giffin，Arjun Gopalan，Jon Howell，Vimalkumar Jeyakumar，Ankita Kejriwal，Aleksandar Kracun，Amit Levy，Joel Martin，Satoshi Matsushita，Oleg Pesok，David Ramos，Robbert van Renesse，Mendel Rosenblum，Nicolas Schiper，Deian Stefan，Andrew Stone，Ryan Stutsman，David Terei，Stephen Yang，Matei Zaharia 以及 24 位匿名的会议审查人员（可能有重复），并且特别感谢我们的领导人 Eddie Kohler。Werner Vogels 发了一条早期草稿链接的推特，给 Raft 带来了极大的关注。我们的工作由 Gigascale 系统研究中心和 Multiscale 系统研究中心给予支持，这两个研究中心由关注中心研究程序资金支持，一个是半导体研究公司的程序，由 STARnet 支持，一个半导体研究公司的程序由 MARCO 和 DARPA 支持，在国家科学基金会的 0963859 号批准，并且获得了来自 Facebook，Google，Mellanox，NEC，NetApp，SAP 和 Samsung 的支持。Diego Ongaro 由 Junglee 公司，斯坦福的毕业团体支持。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>略</p>]]></content>
    
    
    <categories>
      
      <category>raft</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
      <tag>raft</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>01.etcd基础架构</title>
    <link href="/2022/10/01/01.etcd%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84/"/>
    <url>/2022/10/01/01.etcd%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84/</url>
    
    <content type="html"><![CDATA[<h1 id="ETCD基础架构"><a href="#ETCD基础架构" class="headerlink" title="ETCD基础架构"></a>ETCD基础架构</h1><blockquote><p>本文笔记来自：「极客时间ETCD实战课」，原文链接：<a href="https://time.geekbang.org/column/article/354292?cid=100069901">https://time.geekbang.org/column/article/354292?cid=100069901</a></p></blockquote><p>下面是一张 etcd 的简要基础架构图，我们先从宏观上了解一下 etcd 都有哪些功能模块。</p><p><img src="https://static001.geekbang.org/resource/image/34/84/34486534722d2748d8cd1172bfe63084.png?wh=1920*1240" alt="img"></p><p>可以看到，按照分层模型，etcd 可分为 Client 层、API 网络层、Raft 算法层、逻辑层和存储层。这些层的功能如下：</p><ul><li><strong>Client 层</strong>：Client 层包括 client v2 和 v3 两个大版本 API 客户端库，提供了简洁易用的 API，同时支持负载均衡、节点间故障自动转移，可极大降低业务使用 etcd 复杂度，提升开发效率、服务可用性。</li><li><strong>API 网络层</strong>：API 网络层主要包括 client 访问 server 和 server 节点之间的通信协议。一方面，client 访问 etcd server 的 API 分为 v2 和 v3 两个大版本。v2 API 使用 HTTP&#x2F;1.x 协议，v3 API 使用 gRPC 协议。同时 v3 通过 etcd grpc-gateway 组件也支持 HTTP&#x2F;1.x 协议，便于各种语言的服务调用。另一方面，server 之间通信协议，是指节点间通过 Raft 算法实现数据复制和 Leader 选举等功能时使用的 HTTP 协议。</li><li><strong>Raft 算法层</strong>：Raft 算法层实现了 Leader 选举、日志复制、ReadIndex 等核心算法特性，用于保障 etcd 多个节点间的数据一致性、提升服务可用性等，是 etcd 的基石和亮点。</li><li><strong>功能逻辑层</strong>：etcd 核心特性实现层，如典型的 KVServer 模块、MVCC 模块、Auth 鉴权模块、Lease 租约模块、Compactor 压缩模块等，其中 MVCC 模块主要由 treeIndex 模块和 boltdb 模块组成。</li><li><strong>存储</strong>层：存储层包含预写日志 (WAL) 模块、快照 (Snapshot) 模块、boltdb 模块。其中 WAL 可保障 etcd crash 后数据不丢失，boltdb 则保存了集群元数据和用户写入的数据。</li></ul>]]></content>
    
    
    <categories>
      
      <category>etcd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>etcd</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>设计模式-单例模式的六种实现方式</title>
    <link href="/2022/09/22/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%85%AD%E7%A7%8D%E5%B8%B8%E8%A7%81%E5%BD%A2%E5%BC%8F/"/>
    <url>/2022/09/22/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%85%AD%E7%A7%8D%E5%B8%B8%E8%A7%81%E5%BD%A2%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<p>﻿&gt; 最近重新学习了一下单例模式，写博客总结一下，有错误或者另外的方法欢迎指出</p><h1 id="单例模式的六种常见形式"><a href="#单例模式的六种常见形式" class="headerlink" title="单例模式的六种常见形式"></a>单例模式的六种常见形式</h1><h2 id="一、饿汉式：直接创建对象，无线程安全问题"><a href="#一、饿汉式：直接创建对象，无线程安全问题" class="headerlink" title="一、饿汉式：直接创建对象，无线程安全问题"></a>一、饿汉式：直接创建对象，无线程安全问题</h2><p><strong>1.1直接实例化饿汉式（简洁直观）</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.lp.singleton;<br><br><span class="hljs-comment">//1.1直接实例化饿汉式（简洁直观）</span><br><span class="hljs-comment">/*</span><br><span class="hljs-comment">直接创建实例对象</span><br><span class="hljs-comment">构造器私有化</span><br><span class="hljs-comment">自行创建，用静态变量保存</span><br><span class="hljs-comment">final用来强调单例，可以不加</span><br><span class="hljs-comment"> */</span><br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">HungrySingleton1</span> &#123;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span>  <span class="hljs-keyword">final</span>  HungrySingleton1  INSTANCE=<span class="hljs-keyword">new</span> <span class="hljs-title class_">HungrySingleton1</span>();<br>    <span class="hljs-keyword">private</span> <span class="hljs-title function_">HungrySingleton1</span><span class="hljs-params">()</span>&#123;&#125;<br>&#125;<br><br></code></pre></td></tr></table></figure><p><strong>1.2枚举式（最简洁）</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.lp.singleton;<br><br><span class="hljs-comment">/*</span><br><span class="hljs-comment">1.2枚举式（最简洁）</span><br><span class="hljs-comment">只声明一个，表示限定为单例</span><br><span class="hljs-comment">*/</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">enum</span> <span class="hljs-title class_">HungrySingleton2</span>&#123;<br>        INSTANCE<br>    &#125;<br><br><br><br></code></pre></td></tr></table></figure><p><strong>1.3静态代码块饿汉式（适合复杂饿汉式）</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.lp.singleton;<br><span class="hljs-comment">/*</span><br><span class="hljs-comment">* 1.3静态代码块饿汉式（适合复杂饿汉式）</span><br><span class="hljs-comment">* 通过静态代码块创建实例</span><br><span class="hljs-comment">*/</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">HungrySingleton3</span> &#123;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span>  <span class="hljs-keyword">final</span> HungrySingleton3   INSTANCE;<br>    <span class="hljs-keyword">static</span> &#123;<br>        INSTANCE=<span class="hljs-keyword">new</span> <span class="hljs-title class_">HungrySingleton3</span>();<br>    &#125;<br>    <span class="hljs-keyword">private</span> <span class="hljs-title function_">HungrySingleton3</span><span class="hljs-params">()</span>&#123;&#125;<br>&#125;<br><br></code></pre></td></tr></table></figure><h2 id="二、懒汉式：延迟创建对象"><a href="#二、懒汉式：延迟创建对象" class="headerlink" title="二、懒汉式：延迟创建对象"></a>二、懒汉式：延迟创建对象</h2><p><strong>2.1线程不安全（适用于单线程）</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.lp.singleton;<br><br><span class="hljs-comment">/*</span><br><span class="hljs-comment"> *2.1线程不安全（适用于单线程）</span><br><span class="hljs-comment"> * 构造器私有化</span><br><span class="hljs-comment"> * 用静态变量修饰</span><br><span class="hljs-comment"> * 提供静态变量方法获取对象</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">lazySingleton1</span> &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> lazySingleton1 instance;<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-title function_">lazySingleton1</span><span class="hljs-params">()</span> &#123;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> lazySingleton1 <span class="hljs-title function_">getInstance</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">if</span> (instance == <span class="hljs-literal">null</span>) &#123;<br>            instance = <span class="hljs-keyword">new</span> <span class="hljs-title class_">lazySingleton1</span>();<br>        &#125;<br>        <span class="hljs-keyword">return</span> instance;<br>    &#125;<br>&#125;<br><br></code></pre></td></tr></table></figure><p><strong>2.2线程安全（适用于多线程）</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.lp.singleton;<br><br><span class="hljs-comment">/*</span><br><span class="hljs-comment"> *2.2线程安全（适用于多线程）</span><br><span class="hljs-comment"> * 构造器私有化</span><br><span class="hljs-comment"> * 用静态变量修饰</span><br><span class="hljs-comment"> * 提供静态变量方法获取对象</span><br><span class="hljs-comment"> * 使用synchronized锁住对象</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">lazySingleton2</span> &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> lazySingleton2 instance;<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-title function_">lazySingleton2</span><span class="hljs-params">()</span> &#123;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> lazySingleton2 <span class="hljs-title function_">getInstance</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">if</span>(instance == <span class="hljs-literal">null</span>) &#123;<br>            <span class="hljs-keyword">synchronized</span> (lazySingleton2.class) &#123;<br>                <span class="hljs-keyword">if</span> (instance == <span class="hljs-literal">null</span>) &#123;<br>                    instance = <span class="hljs-keyword">new</span> <span class="hljs-title class_">lazySingleton2</span>();<br>                &#125;<br>            &#125;<br>        &#125;<br>        <span class="hljs-keyword">return</span> instance;<br>    &#125;<br>&#125;<br><br><br><br></code></pre></td></tr></table></figure><p><strong>2.3静态内部类（适用于多线程）</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.lp.singleton;<br><br><br><span class="hljs-comment">/*</span><br><span class="hljs-comment"> *2.3静态内部类（适用于多线程）</span><br><span class="hljs-comment"> * 构造器私有化</span><br><span class="hljs-comment"> * 用静态变量修饰</span><br><span class="hljs-comment"> *在静态内部类被加载和初始化时，才创建INSTANCE实例对象</span><br><span class="hljs-comment"> * 静态内部类单独加载和初始化，不会随着外部类的加载和初始化而初始化</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">lazySingleton3</span> &#123;<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-title function_">lazySingleton3</span><span class="hljs-params">()</span> &#123;<br>    &#125;<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Inner</span> &#123;<br>        <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span>  <span class="hljs-keyword">final</span> lazySingleton3   INSTANCE=<span class="hljs-keyword">new</span> <span class="hljs-title class_">lazySingleton3</span>();<br>    &#125;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> lazySingleton3 <span class="hljs-title function_">getInstance</span><span class="hljs-params">()</span>&#123;<br>        <span class="hljs-keyword">return</span> Inner.INSTANCE;<br>    &#125;<br>&#125;<br><br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>设计模式</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>java</tag>
      
      <tag>设计模式</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>关于Spring框架的总结（三、Spring AOP）</title>
    <link href="/2022/09/22/%E5%85%B3%E4%BA%8ESpring%E6%A1%86%E6%9E%B6%E7%9A%84%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%89%E3%80%81Spring-AOP%EF%BC%89/"/>
    <url>/2022/09/22/%E5%85%B3%E4%BA%8ESpring%E6%A1%86%E6%9E%B6%E7%9A%84%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%89%E3%80%81Spring-AOP%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>﻿# 关于Spring框架的总结（三、Spring AOP）</p><h2 id="3-Spring-AOP"><a href="#3-Spring-AOP" class="headerlink" title="3.Spring AOP"></a>3.Spring AOP</h2><blockquote><p>Spring AOP<br>3.1.Spring AOP的基本概念<br>3.2.动态代理<br>3.3.AOP的常见术语<br>3.4.基于XML配置开发<br>3.5.基于注解开发</p></blockquote><h2 id="3-1-Spring-AOP的基本概念"><a href="#3-1-Spring-AOP的基本概念" class="headerlink" title="3.1.Spring AOP的基本概念"></a>3.1.Spring AOP的基本概念</h2><p><font color=red>AOP (Aspect- Oriented Programming) 即面向切面编程，通过预编译方式和运行期动态代理实现程序功能的统一维护的一种技术。</font color=red>它与OOP (Object-OtientedProgramming,面向对象编程)相辅相成， 提供了与OOP不同的抽象软件结构的视角。在OOP中，以类作为程序的基本单元，而AOP中的基本单元是Aspect (切面)。利用AOP可以对业务逻辑的各个部分进行隔离,从而使得业务逻辑各部分之间的耦合度降低,提高程序的可重用性，同时提高了开发的效率。</p><p>简单的说它就是把我们程序重复的代码抽取出来，在需要执行的时候，使用动态代理的技术，在不修改源码的基础上，对我们的已有方法进行增强。</p><h2 id="3-2-动态代理"><a href="#3-2-动态代理" class="headerlink" title="3.2.动态代理"></a>3.2.动态代理</h2><p><strong>动态代理常用的有两种方式</strong><br><strong>（1）、JDK动态代理（基于接口的动态代理）</strong><br>1.创建接口以及实现类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">interface</span> <span class="hljs-title class_">IActor</span> &#123;<br><span class="hljs-comment">/**</span><br><span class="hljs-comment">* 基本演出</span><br><span class="hljs-comment">* <span class="hljs-doctag">@param</span> money</span><br><span class="hljs-comment">*/</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">basicAct</span><span class="hljs-params">(<span class="hljs-type">float</span> money)</span>;<br><span class="hljs-comment">/**</span><br><span class="hljs-comment">* 危险演出</span><br><span class="hljs-comment">* <span class="hljs-doctag">@param</span> money</span><br><span class="hljs-comment">*/</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">dangerAct</span><span class="hljs-params">(<span class="hljs-type">float</span> money)</span>;<br>&#125;<br><span class="hljs-comment">/**</span><br><span class="hljs-comment">* 一个演员</span><br><span class="hljs-comment">*/</span><br><span class="hljs-comment">//实现了接口，就表示具有接口中的方法实现。即：符合经纪公司的要求</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Actor</span> <span class="hljs-keyword">implements</span> <span class="hljs-title class_">IActor</span>&#123;<br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">basicAct</span><span class="hljs-params">(<span class="hljs-type">float</span> money)</span>&#123;<br>System.out.println(<span class="hljs-string">&quot;拿到钱，开始基本的表演：&quot;</span>+money);<br>&#125;<br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">dangerAct</span><span class="hljs-params">(<span class="hljs-type">float</span> money)</span>&#123;<br>System.out.println(<span class="hljs-string">&quot;拿到钱，开始危险的表演：&quot;</span>+money);<br>&#125; <br>&#125;<br></code></pre></td></tr></table></figure><p>2.创建代理类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Client</span> &#123;<br><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> &#123;<br><span class="hljs-comment">//一个剧组找演员：</span><br><span class="hljs-keyword">final</span> <span class="hljs-type">Actor</span> <span class="hljs-variable">actor</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Actor</span>();<span class="hljs-comment">//直接</span><br><span class="hljs-comment">/**</span><br><span class="hljs-comment">* 代理：</span><br><span class="hljs-comment">* 间接。</span><br><span class="hljs-comment">* 获取代理对象：</span><br><span class="hljs-comment">* 要求：</span><br><span class="hljs-comment">* 被代理类最少实现一个接口</span><br><span class="hljs-comment">* 创建的方式</span><br><span class="hljs-comment">* Proxy.newProxyInstance(三个参数)</span><br><span class="hljs-comment">* 参数含义：</span><br><span class="hljs-comment">* ClassLoader：和被代理对象使用相同的类加载器。</span><br><span class="hljs-comment">* Interfaces：和被代理对象具有相同的行为。实现相同的接口。</span><br><span class="hljs-comment">* InvocationHandler：如何代理。</span><br><span class="hljs-comment">* 策略模式：使用场景是：</span><br><span class="hljs-comment">* 数据有了，目的明确。</span><br><span class="hljs-comment">* 如何达成目标，就是策略。</span><br><span class="hljs-comment">* </span><br><span class="hljs-comment">*/</span><br><span class="hljs-type">IActor</span> <span class="hljs-variable">proxyActor</span> <span class="hljs-operator">=</span> (IActor) Proxy.newProxyInstance(<br>actor.getClass().getClassLoader(), <br>actor.getClass().getInterfaces(), <br><span class="hljs-keyword">new</span> <span class="hljs-title class_">InvocationHandler</span>() &#123;<br><span class="hljs-comment">/**</span><br><span class="hljs-comment">* 执行被代理对象的任何方法，都会经过该方法。</span><br><span class="hljs-comment">* 此方法有拦截的功能。</span><br><span class="hljs-comment">* </span><br><span class="hljs-comment">* 参数：</span><br><span class="hljs-comment">* proxy：代理对象的引用。不一定每次都用得到</span><br><span class="hljs-comment">* method：当前执行的方法对象</span><br><span class="hljs-comment">* args：执行方法所需的参数</span><br><span class="hljs-comment">* 返回值：</span><br><span class="hljs-comment">* 当前执行方法的返回值</span><br><span class="hljs-comment">*/</span><br><span class="hljs-meta">@Override</span><br><span class="hljs-keyword">public</span> Object <span class="hljs-title function_">invoke</span><span class="hljs-params">(Object proxy, Method method, Object[] args)</span> <br><span class="hljs-keyword">throws</span> Throwable &#123;<br><span class="hljs-type">String</span> <span class="hljs-variable">name</span> <span class="hljs-operator">=</span> method.getName();<br><span class="hljs-type">Float</span> <span class="hljs-variable">money</span> <span class="hljs-operator">=</span> (Float) args[<span class="hljs-number">0</span>];<br><span class="hljs-type">Object</span> <span class="hljs-variable">rtValue</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<br><span class="hljs-comment">//每个经纪公司对不同演出收费不一样，此处开始判断</span><br><span class="hljs-keyword">if</span>(<span class="hljs-string">&quot;basicAct&quot;</span>.equals(name))&#123;<br><span class="hljs-comment">//基本演出，没有 2000 不演</span><br><span class="hljs-keyword">if</span>(money &gt; <span class="hljs-number">2000</span>)&#123;<br><span class="hljs-comment">//看上去剧组是给了 8000，实际到演员手里只有 4000</span><br><span class="hljs-comment">//这就是我们没有修改原来 basicAct 方法源码，对方法进行了增强</span><br>rtValue = method.invoke(actor, money/<span class="hljs-number">2</span>);<br>&#125; &#125;<br><span class="hljs-keyword">if</span>(<span class="hljs-string">&quot;dangerAct&quot;</span>.equals(name))&#123;<br><span class="hljs-comment">//危险演出,没有 5000 不演</span><br><span class="hljs-keyword">if</span>(money &gt; <span class="hljs-number">5000</span>)&#123;<br><span class="hljs-comment">//看上去剧组是给了 50000，实际到演员手里只有 25000</span><br><span class="hljs-comment">//这就是我们没有修改原来 dangerAct 方法源码，对方法进行了增强</span><br>rtValue = method.invoke(actor, money/<span class="hljs-number">2</span>);<br>&#125; &#125;<br><span class="hljs-keyword">return</span> rtValue;<br>&#125;<br>&#125;);<br><span class="hljs-comment">//没有经纪公司的时候，直接找演员。</span><br><span class="hljs-comment">// actor.basicAct(1000f);</span><br><span class="hljs-comment">// actor.dangerAct(5000f);</span><br><span class="hljs-comment">//剧组无法直接联系演员，而是由经纪公司找的演员</span><br>proxyActor.basicAct(<span class="hljs-number">8000f</span>);<br>proxyActor.dangerAct(<span class="hljs-number">50000f</span>);<br>&#125; &#125;<br></code></pre></td></tr></table></figure><p><strong>（2）、CGLIB动态代理（基于子类的动态代理）</strong><br>1.创建目标类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Actor</span>&#123;<span class="hljs-comment">//目标类</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">basicAct</span><span class="hljs-params">(<span class="hljs-type">float</span> money)</span>&#123;<br>System.out.println(<span class="hljs-string">&quot;拿到钱，开始基本的表演：&quot;</span>+money);<br>&#125;<br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">dangerAct</span><span class="hljs-params">(<span class="hljs-type">float</span> money)</span>&#123;<br>System.out.println(<span class="hljs-string">&quot;拿到钱，开始危险的表演：&quot;</span>+money);<br>&#125; <br>&#125;<br></code></pre></td></tr></table></figure><p>2.创建代理类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Client</span> &#123;<br><span class="hljs-comment">/**</span><br><span class="hljs-comment">* 基于子类的动态代理</span><br><span class="hljs-comment">* 要求：</span><br><span class="hljs-comment">* 被代理对象不能是最终类</span><br><span class="hljs-comment">* 用到的类：</span><br><span class="hljs-comment">* Enhancer</span><br><span class="hljs-comment">* 用到的方法：</span><br><span class="hljs-comment">* create(Class, Callback)</span><br><span class="hljs-comment">* 方法的参数：</span><br><span class="hljs-comment">* Class：被代理对象的字节码</span><br><span class="hljs-comment">* Callback：如何代理</span><br><span class="hljs-comment">* <span class="hljs-doctag">@param</span> args</span><br><span class="hljs-comment">*/</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> &#123;<br><span class="hljs-keyword">final</span> <span class="hljs-type">Actor</span> <span class="hljs-variable">actor</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Actor</span>();<br><span class="hljs-type">Actor</span> <span class="hljs-variable">cglibActor</span> <span class="hljs-operator">=</span> (Actor) Enhancer.create(actor.getClass(),<br><span class="hljs-keyword">new</span> <span class="hljs-title class_">MethodInterceptor</span>() &#123;<br><span class="hljs-comment">/**</span><br><span class="hljs-comment">* 执行被代理对象的任何方法，都会经过该方法。在此方法内部就可以对被代理对象的任何</span><br><span class="hljs-comment">方法进行增强。</span><br><span class="hljs-comment">* </span><br><span class="hljs-comment">* 参数：</span><br><span class="hljs-comment">* 前三个和基于接口的动态代理是一样的。</span><br><span class="hljs-comment">* MethodProxy：当前执行方法的代理对象。</span><br><span class="hljs-comment">* 返回值：</span><br><span class="hljs-comment">* 当前执行方法的返回值</span><br><span class="hljs-comment">*/</span><br><span class="hljs-meta">@Override</span><br><span class="hljs-keyword">public</span> Object <span class="hljs-title function_">intercept</span><span class="hljs-params">(Object proxy, Method method, Object[] args, </span><br><span class="hljs-params">MethodProxy methodProxy)</span> <span class="hljs-keyword">throws</span> Throwable &#123;<br><span class="hljs-type">String</span> <span class="hljs-variable">name</span> <span class="hljs-operator">=</span> method.getName();<br><span class="hljs-type">Float</span> <span class="hljs-variable">money</span> <span class="hljs-operator">=</span> (Float) args[<span class="hljs-number">0</span>];<br><span class="hljs-type">Object</span> <span class="hljs-variable">rtValue</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<br><span class="hljs-keyword">if</span>(<span class="hljs-string">&quot;basicAct&quot;</span>.equals(name))&#123;<br><span class="hljs-comment">//基本演出</span><br><span class="hljs-keyword">if</span>(money &gt; <span class="hljs-number">2000</span>)&#123;<br>rtValue = method.invoke(actor, money/<span class="hljs-number">2</span>);<br>&#125;<br>&#125;<br><span class="hljs-keyword">if</span>(<span class="hljs-string">&quot;dangerAct&quot;</span>.equals(name))&#123;<br><span class="hljs-comment">//危险演出</span><br><span class="hljs-keyword">if</span>(money &gt; <span class="hljs-number">5000</span>)&#123;<br>rtValue = method.invoke(actor, money/<span class="hljs-number">2</span>);<br>&#125; &#125;<br><span class="hljs-keyword">return</span> rtValue;<br>&#125;<br>&#125;);<br>cglibActor.basicAct(<span class="hljs-number">10000</span>);<br>cglibActor.dangerAct(<span class="hljs-number">100000</span>);<br>&#125;<br> &#125;<br></code></pre></td></tr></table></figure><h2 id="3-3-AOP的常见术语"><a href="#3-3-AOP的常见术语" class="headerlink" title="3.3.AOP的常见术语"></a>3.3.AOP的常见术语</h2><p>➊<strong>切面</strong></p><p>切面(Aspect) 是指封装横切到系统功能(例如事务处理)的类。</p><p>❷<strong>连接点</strong></p><p>连接点(Joinpoint)是指程序运行中的一些时间点， 例如方法的调用或异常的抛出。</p><p>❸<strong>切入点</strong></p><p>切入点(Poineu)是指需要处理的连接点。在Spring AOP中，所有的方法执行都是连接点，而切入点是一个描述信息， 它修饰的是连接点。</p><p>❹<strong>通知</strong></p><p>通知(Advice) 是由切面添加到特定的连接点(满足切入点规则)的一段代码，即在定义好的切入点处所要执行的程序代码，可以将其理解为切面开启后切面的方法，因此通知是切面的具体实现。</p><p>❺<strong>引入</strong></p><p>引入( Introduction)允许在现有的实现类中添加自定义的方法和属性。</p><p>❻<strong>目标对象</strong></p><p>目标对象(Target Object)是指所有被通知的对象。如果AOP框架使用运行时代理的方式(动态的AOP)来实现切面，那么通知对象总是一个代理对象。</p><p>❼<strong>代理</strong></p><p>代理(Proxy) 是通知应用到目标对象之后被动态创建的对象。</p><p>❽<strong>织人</strong></p><p>织入( Weaving)是将切面代码插入到目标对象上，从而生成代理对象的过程。根据不同的实现技术，AOP 织入有3种方式:编译期织入，需要有特殊的Java 编译器;类装载期织入，需要有特殊的类装载器:动态代理织入，在运行期为目标类添加通知生成子类的方式。Spring AOP框架默认采用动态代理织入，而AspectJ (基于Java语言的AOP框架)</p><p><strong><a herf="https://blog.csdn.net/qq_42009262/article/details/105013992">Spring通知类型介绍</strong></p><p>根据Spring中通知在目标类方法的链接点位置，可以分为6种类型：</p><p><strong>（1）环绕通知：</strong></p><table><thead><tr><th align="center">实现接口</th><th align="right">功能描述</th></tr></thead><tbody><tr><td align="center">org.aopalliance.itercept.MethodInterceptor</td><td align="right">在目标方法执行前和执行后实施增强。可以用于日志记录、事务处理等功能。</td></tr></tbody></table><p><strong>（2）前置通知：</strong></p><table><thead><tr><th align="center">实现接口</th><th align="right">功能描述</th></tr></thead><tbody><tr><td align="center">org.springframework.aop.MethodBeforeAdvice</td><td align="right">在目标方法执行前实施增强。可以用于权限管理等功能。</td></tr></tbody></table><p><strong>（3）后置返回通知</strong>：</p><table><thead><tr><th align="center">实现接口</th><th align="right">功能描述</th></tr></thead><tbody><tr><td align="center">org.springframework.aop.AfterReturningAdvice</td><td align="right">在目标方法成功执行后实施增强。可以用于关闭流、删除临时文件等功能。</td></tr></tbody></table><p><strong>（4）后置（最终）通知：</strong></p><table><thead><tr><th align="center">实现接口</th><th align="right">功能描述</th></tr></thead><tbody><tr><td align="center">org.springframework.aop.AfterAdvice</td><td align="right">在目标方法执行后实施增强。与后置返回通知不同的是，不管是否发生异常都要执行该通知，可应用于释放资源。</td></tr></tbody></table><p><strong>（5）异常通知：</strong></p><table><thead><tr><th align="center">实现接口</th><th align="right">功能描述</th></tr></thead><tbody><tr><td align="center">org.springframework.aop.ThrowsAdvice</td><td align="right">在方法抛出异常后实施增强。可以用于异常处理、记录日志等功能。</td></tr></tbody></table><p><strong>（6）引入通知：</strong></p><table><thead><tr><th align="center">实现接口</th><th align="right">功能描述</th></tr></thead><tbody><tr><td align="center">org.springframework.aop.IntroductInterceptor</td><td align="right">在目标类中添加一些新的方法和属性。可以用于修改目标类（增强类）。</td></tr></tbody></table><h2 id="3-4-基于XML配置开发"><a href="#3-4-基于XML配置开发" class="headerlink" title="3.4.基于XML配置开发"></a>3.4.基于XML配置开发</h2><p><strong>1.把通知类用 bean 标签配置起来</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs java">&lt;!-- 配置通知 --&gt;<br> &lt;bean id=<span class="hljs-string">&quot;txManager&quot;</span> class=<span class="hljs-string">&quot;com.itheima.utils.TransactionManager&quot;</span>&gt; <br>&lt;property name=<span class="hljs-string">&quot;dbAssit&quot;</span> ref=<span class="hljs-string">&quot;dbAssit&quot;</span>&gt;&lt;/property&gt;<br>&lt;/bean&gt;<br></code></pre></td></tr></table></figure><p><strong>2.使用 aop:config 声明 aop 配置</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs java">作用：用于声明开始 aop 的配置<br>&lt;aop:config&gt;<br>&lt;!-- 配置的代码都写在此处 --&gt;<br>&lt;/aop:config&gt;<br></code></pre></td></tr></table></figure><p><strong>3.使用 aop:aspect 配置切面</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs java">aop:aspect:<br>作用：<br>用于配置切面。<br>属性：<br>id：给切面提供一个唯一标识。<br>ref：引用配置好的通知类 bean 的 id。<br> &lt;aop:aspect id=<span class="hljs-string">&quot;txAdvice&quot;</span> ref=<span class="hljs-string">&quot;txManager&quot;</span>&gt;<br>&lt;!--配置通知的类型要写在此处--&gt;<br>&lt;/aop:aspect&gt;<br></code></pre></td></tr></table></figure><p><strong>4.使用 aop:pointcut 配置切入点表达式</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs java">aop:pointcut：<br>作用：<br>用于配置切入点表达式。就是指定对哪些类的哪些方法进行增强。<br>属性：<br>expression：用于定义切入点表达式。<br>id：用于给切入点表达式提供一个唯一标识<br>&lt;aop:pointcut id=<span class="hljs-string">&quot;pt1&quot;</span> expression=<span class="hljs-string">&quot;execution(</span><br><span class="hljs-string">execution:匹配方法的执行(常用)</span><br><span class="hljs-string">execution(表达式)</span><br><span class="hljs-string">表达式语法：execution([修饰符] 返回值类型 包名.类名.方法名(参数))&quot;</span> /&gt;<br></code></pre></td></tr></table></figure><p><strong>5.使用 aop:xxx 配置对应的通知类型</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs java">aop:before<br>作用：<br>用于配置前置通知。指定增强的方法在切入点方法之前执行<br>属性：<br>method:用于指定通知类中的增强方法名称<br>ponitcut-ref：用于指定切入点的表达式的引用<br>poinitcut：用于指定切入点表达式<br>执行时间点：<br>切入点方法执行之前执行<br>&lt;aop:before method=<span class="hljs-string">&quot;beginTransaction&quot;</span> pointcut-ref=<span class="hljs-string">&quot;pt1&quot;</span>/&gt;<br>aop:after-returning<br>作用：<br>用于配置后置通知<br>属性：<br>method：指定通知中方法的名称。<br>pointct：定义切入点表达式<br>pointcut-ref：指定切入点表达式的引用<br>执行时间点：<br>切入点方法正常执行之后。它和异常通知只能有一个执行<br>&lt;aop:after-returning method=<span class="hljs-string">&quot;commit&quot;</span> pointcut-ref=<span class="hljs-string">&quot;pt1&quot;</span>/&gt;<br>aop:after-throwing<br>作用：<br>用于配置异常通知<br>属性：<br>method：指定通知中方法的名称。<br>pointct：定义切入点表达式<br>pointcut-ref：指定切入点表达式的引用<br>执行时间点：<br>切入点方法执行产生异常后执行。它和后置通知只能执行一个<br>&lt;aop:after-throwing method=<span class="hljs-string">&quot;rollback&quot;</span> pointcut-ref=<span class="hljs-string">&quot;pt1&quot;</span>/&gt;<br>aop:after<br>作用：<br>用于配置最终通知<br>属性：<br>method：指定通知中方法的名称。<br>pointct：定义切入点表达式<br>pointcut-ref：指定切入点表达式的引用<br>执行时间点：<br>无论切入点方法执行时是否有异常，它都会在其后面执行。<br>&lt;aop:after method=<span class="hljs-string">&quot;release&quot;</span> pointcut-ref=<span class="hljs-string">&quot;pt1&quot;</span>/&gt;<br></code></pre></td></tr></table></figure><h2 id="3-5-基于注解开发"><a href="#3-5-基于注解开发" class="headerlink" title="3.5.基于注解开发"></a>3.5.基于注解开发</h2><p><strong>1.配置类的编写</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Configuration</span><br><span class="hljs-meta">@ComponentScan(basePackages=&quot;包名&quot;)</span><br><span class="hljs-meta">@EnableAspectJAutoProxy</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">SpringConfiguration</span> &#123;<br>&#125;<br></code></pre></td></tr></table></figure><p><strong>2.通知类的编写</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Component(&quot;txManager&quot;)</span><br><span class="hljs-meta">@Aspect</span><span class="hljs-comment">//表明当前类是一个切面类</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">TransactionManager</span> &#123;<br><span class="hljs-comment">//定义一个 DBAssit</span><br><span class="hljs-meta">@Autowired</span><br><span class="hljs-keyword">private</span> DBAssit dbAssit ;<br> &#125;<br></code></pre></td></tr></table></figure><p><strong>3.在增强的方法上使用注解配置通知</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Before</span><br>作用：<br>把当前方法看成是前置通知。<br>属性：<br>value：用于指定切入点表达式，还可以指定切入点表达式的引用。<br><span class="hljs-comment">//开启事务</span><br><span class="hljs-meta">@Before(&quot;execution(* com.itheima.service.impl.*.*(..))&quot;)</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">beginTransaction</span><span class="hljs-params">()</span> &#123;<br><span class="hljs-keyword">try</span> &#123;<br>dbAssit.getCurrentConnection().setAutoCommit(<span class="hljs-literal">false</span>);<br>&#125; <span class="hljs-keyword">catch</span> (SQLException e) &#123;<br>e.printStackTrace();<br>&#125; <br>&#125;<br><span class="hljs-meta">@AfterReturning</span><br>作用：<br>把当前方法看成是后置通知。<br>属性：<br>value：用于指定切入点表达式，还可以指定切入点表达式的引用<br><span class="hljs-comment">//提交事务</span><br><span class="hljs-meta">@AfterReturning(&quot;execution(* com.itheima.service.impl.*.*(..))&quot;)</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">commit</span><span class="hljs-params">()</span> &#123;<br><span class="hljs-keyword">try</span> &#123;<br>dbAssit.getCurrentConnection().commit();<br>&#125; <span class="hljs-keyword">catch</span> (SQLException e) &#123;<br>e.printStackTrace();<br>&#125; &#125;<br><span class="hljs-meta">@AfterThrowing</span><br>作用：<br>把当前方法看成是异常通知。<br>属性：<br>value：用于指定切入点表达式，还可以指定切入点表达式的引用<br><span class="hljs-comment">//回滚事务</span><br><span class="hljs-meta">@AfterThrowing(&quot;execution(* com.itheima.service.impl.*.*(..))&quot;)</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">rollback</span><span class="hljs-params">()</span> &#123;<br><span class="hljs-keyword">try</span> &#123;<br>dbAssit.getCurrentConnection().rollback();<br>&#125; <span class="hljs-keyword">catch</span> (SQLException e) &#123;<br>e.printStackTrace();<br>&#125; &#125;<br><span class="hljs-meta">@After</span><br>作用：<br>把当前方法看成是最终通知。<br>属性：<br>value：用于指定切入点表达式，还可以指定切入点表达式的引用<br><span class="hljs-comment">//释放资源</span><br><span class="hljs-meta">@After(&quot;execution(* com.itheima.service.impl.*.*(..))&quot;)</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">release</span><span class="hljs-params">()</span> &#123;<br><span class="hljs-keyword">try</span> &#123;<br>dbAssit.releaseConnection();<br>&#125; <span class="hljs-keyword">catch</span> (Exception e) &#123;<br>e.printStackTrace();<br>&#125;<br> &#125;<br> <span class="hljs-meta">@Around</span><br>作用：<br>把当前方法看成是环绕通知。<br>属性：<br>value：用于指定切入点表达式，还可以指定切入点表达式的引用。<br><span class="hljs-comment">/**</span><br><span class="hljs-comment">* 环绕通知</span><br><span class="hljs-comment">* <span class="hljs-doctag">@param</span> pjp</span><br><span class="hljs-comment">* <span class="hljs-doctag">@return</span></span><br><span class="hljs-comment">*/</span><br><span class="hljs-meta">@Pointcut</span><br>作用：<br>指定切入点表达式<br>属性：<br>value：指定表达式的内容<br><span class="hljs-meta">@Pointcut(&quot;execution(* com.itheima.service.impl.*.*(..))&quot;)</span><br><span class="hljs-keyword">private</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">pt1</span><span class="hljs-params">()</span> &#123;&#125;<br><span class="hljs-meta">@Around(&quot;pt1()&quot;)</span><br><span class="hljs-keyword">public</span> Object <span class="hljs-title function_">transactionAround</span><span class="hljs-params">(ProceedingJoinPoint pjp)</span> &#123;<br><span class="hljs-comment">//定义返回值</span><br><span class="hljs-type">Object</span> <span class="hljs-variable">rtValue</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<br><span class="hljs-keyword">try</span> &#123;<br><span class="hljs-comment">//获取方法执行所需的参数</span><br>Object[] args = pjp.getArgs();<br><span class="hljs-comment">//前置通知：开启事务</span><br>beginTransaction();<br><span class="hljs-comment">//执行方法</span><br>rtValue = pjp.proceed(args);<br><span class="hljs-comment">//后置通知：提交事务</span><br>commit();<br>&#125;<span class="hljs-keyword">catch</span>(Throwable e) &#123;<br><span class="hljs-comment">//异常通知：回滚事务</span><br>rollback();<br>e.printStackTrace();<br>&#125;<span class="hljs-keyword">finally</span> &#123;<br><span class="hljs-comment">//最终通知：释放资源</span><br>release();<br>&#125;<br><span class="hljs-keyword">return</span> rtValue;<br> &#125;<br></code></pre></td></tr></table></figure><p><strong>参考文献：[1] 陈恒，楼偶俊，张立杰.Java EE框架整和开发入门到实践[M].清华大学出版社，2018-：.39-45</strong><br><strong>案例来自于黑马程序员Spring教学</strong></p>]]></content>
    
    
    <categories>
      
      <category>java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>java</tag>
      
      <tag>spring</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>关于Spring框架的总结(二、Spring IoC)</title>
    <link href="/2022/09/22/%E5%85%B3%E4%BA%8ESpring%E6%A1%86%E6%9E%B6%E7%9A%84%E6%80%BB%E7%BB%93%EF%BC%88%E4%BA%8C%E3%80%81Spring-IoC/"/>
    <url>/2022/09/22/%E5%85%B3%E4%BA%8ESpring%E6%A1%86%E6%9E%B6%E7%9A%84%E6%80%BB%E7%BB%93%EF%BC%88%E4%BA%8C%E3%80%81Spring-IoC/</url>
    
    <content type="html"><![CDATA[<p>﻿# 关于Spring框架的总结（二、Spring IoC）</p><h2 id="2-Spring-IoC"><a href="#2-Spring-IoC" class="headerlink" title="2.Spring IoC"></a>2.Spring IoC</h2><blockquote><p>Spring IoC<br>2.1.Spring IoC的基本概念<br>2.2.Spring IoC容器<br>2.3.Spring IoC中的bean标签<br>2.4.依赖注入</p></blockquote><h2 id="2-1-Spring-IoC的基本概念"><a href="#2-1-Spring-IoC的基本概念" class="headerlink" title="2.1.Spring IoC的基本概念"></a><strong>2.1.Spring IoC的基本概念</strong></h2><p>控制反转(Inversion of Control, IoC)是一个比较抽象的概念，是Spring框架的核心，用来消减计算机程序的精合问题。<font color=red>对于spring框架来说，就是由spring来负责控制对象的生命周期和对象间的关系。</font >依赖注入(Dpendency Injection,DI)是IOC的另一种说法，只是换了种角度来描述相同的概念，下面举一个例子来解释IOC和DI。</p><p>当人们需要一件东西时， 第一反应就是找东西， 例如想吃面包。在没有面包店和有面包店两种情况下，您会怎么做?在没有面包店时，最直观的做法可能是您按照自己的口味制作面包，也就是一个面包需要主动制作。然而时至今日，各种网店、实体店盛行，已经没有必要自己制作面包。想吃面包了，去网店或实体店把自己的口味告诉店家，一会就可以吃到面包了。注意，您并没有制作面包，而是由店家制作，但是完全符合您的口味。</p><p>上面只是列举了一个非常简单的例子，但包含了控制反转的思想，即把制作面包等主动权交给店家。下面通过面向对象编程思想继续探讨这两个概念<p>当某个Java对象(调用者，例如您)需要调用另一个Java 对象(被调用者，即被依赖对象，例如面包)时，在传统编程模式下，调用者通常会采用“new被调用者”的代码方式来创建对象(例如您自已制作面包)。这种方式会增加调用者与被调用者之间的耦合性，不利于后期代码的升级与维护。</p><p><font color=red>当Spring框架出现后，对象的实例不再由调用者来创建，而是由Spring容器(例如面包店)来创建。Spring容器会负责控制程序之间的关系(例如面包店负责控制您与面包的关系)，而不是由调用者的程序代码直接控制。这样，控制权由调用者转移到Spring容器，控制权发生了反转，这就是Spring的控制反转。</p><p>从Spring容器角度来看，Spring容器负责将被依赖对象赋值给调用者的成员变量，相当于为调用者注入它所依赖的实例，这就是Spring的依赖注入。</p><p>综上所述，控制反转是种通过描述 (在 Spring中可以是XML或注解)并通过第三方去产生或获取特定对象的方式。在Spring中实现控制反转的是loC容器，其实现方法是依赖注入。</p><h2 id="2-2-Spring-IoC容器"><a href="#2-2-Spring-IoC容器" class="headerlink" title="2.2.Spring IoC容器"></a><strong>2.2.Spring IoC容器</strong></h2><p><strong>1.Spring IoC容器设计主要是基于BeanFactoty和ApplicationContext两个接口。下图为spring工厂的类结构体</strong><br><img src="https://img-blog.csdnimg.cn/20200712110712194.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyMDA5MjYy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>BeanFactory 是 Spring 容器中的顶层接口，ApplicationContext 是它的子接口。</p><p><strong>2.BeanFactory 和 ApplicationContext 的区别</strong><br>BeanFactory 和 ApplicationContext 的区别：<br>创建对象的时间点不一样：</p><ul><li>（1.）BeanFactory：什么使用什么时候创建对象。</li><li>（2.）ApplicationContext：只要一读取配置文件，默认情况下就会创建对象。</li></ul><p><strong>3.ApplicationContext 接口的实现类</strong></p><ul><li>3.1ClassPathXmlApplicationContext：从类的根路径(src根目录)寻找指定的XML配置文件 ，最常使用。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">//初始化Spring容器ApplicationContext ，加载配置文件</span><br>ApplicationContext applicationContext=<span class="hljs-keyword">new</span> <span class="hljs-title class_">ClassPathXMLApplicationContext</span>（“ApplicationContext.xml”）<br></code></pre></td></tr></table></figure><ul><li>3.2FileSystemXmlApplicationContext：从磁盘的绝对路径中寻找指定的XML配置文件，配置文件可以在磁盘的任意位置（限制比较大，如果换了设备或者文件xml配置位置改变将找不到配置文件，不推荐）。</li></ul>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">//初始化Spring容器ApplicationContext ，加载配置文件</span><br>ApplicationContext applicationContext=<span class="hljs-keyword">new</span> <span class="hljs-title class_">FileSystemXmlApplicationContext</span>（“D:\文件位置\src\ApplicationContext.xml”）<br></code></pre></td></tr></table></figure><ul><li>3.3AnnotationConfigApplicationContext:使用注解配置容器对象时，需要使用此类来创建 spring 容器。用来读取注解。  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">//初始化Spring容器ApplicationContext ，加载配置文件</span><br>ApplicationContext applicationContext=<span class="hljs-keyword">new</span> <span class="hljs-title class_">AnnotationConfigApplicationContext</span>（“注解配置类.class”）<br></code></pre></td></tr></table></figure></li></ul><h2 id="2-3-Spring-IoC中的bean标签"><a href="#2-3-Spring-IoC中的bean标签" class="headerlink" title="2.3.Spring IoC中的bean标签"></a><strong>2.3.Spring IoC中的bean标签</strong></h2><p><strong>1.<code>&lt;bean&gt;</code>元素的常用属性及其子元素</strong></p><table><thead><tr><th>属性或子元素名称</th><th>描述</th></tr></thead><tbody><tr><td>id</td><td>Bean在BeanFactory中的唯一标识，在代码中通过BeanFactory获取Bean实例时需要以此作为索引名称</td></tr><tr><td>class</td><td>Bean的具体实现类，使用类的名</td></tr><tr><td>scope</td><td>指定Bean实例的作用域，具体属性值及含义</td></tr><tr><td><code>&lt;constructor-arg&gt;</code></td><td><code>&lt;bean&gt;</code>元素的子元素，使用构造方法注入，指定构造方法的参数。该元素的index属性指定参数的序号，ref属性指定对BeanFactory中其他Bean的引用关系，type属性指定参数类型，value属性指定参数的常量值</td></tr><tr><td><code>&lt;property&gt;</code></td><td><code>&lt;bean&gt;</code>元素的子元素，用于设置一个属性。该元素的name属性指定Bean实例中相应的属性名称，value 属性指定Bean的属性值，ref 属性指定属性对BeanFactory中其他Bean的引用关系</td></tr><tr><td><code>&lt;list&gt;</code></td><td><code>&lt;property&gt;</code>元素的子元素，用于封装List 或数组类型的依赖注入</td></tr><tr><td><code>&lt;map&gt;</code></td><td><code>&lt;property&gt;</code>元素的子元素，用于封装Map类型的依赖注入</td></tr><tr><td><code>&lt;set&gt;</code></td><td><code>&lt;property&gt;</code>元素的子元素，用于封装Set类型的依赖注入</td></tr><tr><td><code>&lt;entry&gt;</code></td><td><code>&lt;map&gt;</code>元素的子元素，用于设置一个键值对</td></tr></tbody></table><p><strong>2.bean 的作用范围和生命周期</strong><br>scope：指定对象的作用范围。</p><ul><li><font color=red>singleton :默认值，单例的.</li><li><font color=red> prototype :多例的.</li><li>request :WEB 项目中,Spring 创建一个 Bean 的对象,将对象存入到 request 域中.</li><li>session :WEB 项目中,Spring 创建一个 Bean 的对象,将对象存入到 session 域中.</li><li>global session :WEB 项目中,应用在 Portlet 环境.如果没有 Portlet 环境那么<br>globalSession 相当于 session.</li></ul><p><font color=red>单例对象：scope&#x3D;”singleton”</font><br>一个应用只有一个对象的实例。它的作用范围就是整个引用。<br>生命周期：</p><ul><li>对象出生：当应用加载，创建容器时，对象就被创建了。</li><li>对象活着：只要容器在，对象一直活着。</li><li>对象死亡：当应用卸载，销毁容器时，对象就被销毁了。</li></ul><p><font color=red>多例对象：scope&#x3D;”prototype”</font><br>每次访问对象时，都会重新创建对象实例。<br>生命周期：</p><ul><li>对象出生：当使用对象时，创建新的对象实例。</li><li>对象活着：只要对象在使用中，就一直活着。</li><li>对象死亡：当对象长时间不用时，被 java 的垃圾回收器回收了。</li></ul><p><strong>3.bean的实例化</strong></p><ul><li><p>第一种方式：使用默认无参构造函数</p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">&lt;bean id=<span class="hljs-string">&quot;类名&quot;</span> class=<span class="hljs-string">&quot;类位置&quot;</span>/&gt;<br></code></pre></td></tr></table></figure></li><li><p>第二种方式：spring 管理静态工厂-使用静态工厂的方法创建对象</p> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/**</span><br><span class="hljs-comment">* 模拟一个静态工厂，创建业务层实现类</span><br><span class="hljs-comment">*/</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">StaticFactory</span> &#123;<br>        <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> IAccountService <span class="hljs-title function_">createAccountService</span><span class="hljs-params">()</span>&#123;<br>             <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">AccountServiceImpl</span>();<br>&#125; &#125;<br>&lt;!-- 此种方式是:使用 StaticFactory 类中的静态方法 createAccountService 创建对象，并存入 spring 容器<br>id 属性：指定 bean 的 id，用于从容器中获取<br>class 属性：指定静态工厂的全限定类名<br>factory-method 属性：指定生产对象的静态方法<br>--&gt;<br> &lt;bean id=<span class="hljs-string">&quot;accountService&quot;</span>   class=<span class="hljs-string">&quot;com.itheima.factory.StaticFactory&quot;</span>   <br>  factory-method=<span class="hljs-string">&quot;createAccountService&quot;</span>&gt;&lt;/bean&gt;<br></code></pre></td></tr></table></figure></li><li><p>第三种方式：spring 管理实例工厂-使用实例工厂的方法创建对象</p> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/**</span><br><span class="hljs-comment">* 模拟一个实例工厂，创建业务层实现类</span><br><span class="hljs-comment">* 此工厂创建对象，必须现有工厂实例对象，再调用方法</span><br><span class="hljs-comment">*/</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">InstanceFactory</span> &#123;<br><span class="hljs-keyword">public</span> IAccountService <span class="hljs-title function_">createAccountService</span><span class="hljs-params">()</span>&#123;<br><span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">AccountServiceImpl</span>();<br>&#125; &#125;<br>&lt;!-- 此种方式是：<br>先把工厂的创建交给 spring 来管理。<br>然后在使用工厂的 bean 来调用里面的方法<br>factory-bean 属性：用于指定实例工厂 bean 的 id。<br>factory-method 属性：用于指定实例工厂中创建对象的方法。<br>--&gt;<br> &lt;bean id=<span class="hljs-string">&quot;instancFactory&quot;</span> class=<span class="hljs-string">&quot;com.itheima.factory.InstanceFactory&quot;</span>&gt;&lt;/bean&gt;<br>  &lt;bean id=<span class="hljs-string">&quot;accountService&quot;</span>  factory-bean=<span class="hljs-string">&quot;instancFactory&quot;</span><br> factory-method=<span class="hljs-string">&quot;createAccountService&quot;</span>&gt;&lt;/bean&gt;<br></code></pre></td></tr></table></figure></li></ul><h2 id="2-4-依赖注入"><a href="#2-4-依赖注入" class="headerlink" title="2.4.依赖注入"></a><strong>2.4.依赖注入</strong></h2><p><strong>在Spring中实现IoC容器的方法是依赖注入，依赖注入的作用是在使用Spring框架创建对象时动态地将其所依赖的对象(例如属性值)注入Bean组件中。Spring框架的依赖注入通常有两种实现方式，一种是使用构造方法注入，另一种是使用属性的setter方法注入。</strong></p><p><strong>1.使用构造方法注入</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs java">&lt;!-- 使用构造函数的方式，给 service 中的属性传值<br>要求：<br>类中需要提供一个对应参数列表的构造函数。<br>涉及的标签：<br>constructor-arg<br>属性：<br>index:指定参数在构造函数参数列表的索引位置<br>type:指定参数在构造函数中的数据类型<br>name:指定参数在构造函数中的名称 用这个找给谁赋值<br>=======上面三个都是找给谁赋值，下面两个指的是赋什么值的==============<br>value:它能赋的值是基本数据类型和 String 类型<br>ref:它能赋的值是其他 bean 类型，也就是说，必须得是在配置文件中配置过的 bean<br>--&gt; <br>&lt;bean id=<span class="hljs-string">&quot;类&quot;</span> class=<span class="hljs-string">&quot;类的位置&quot;</span>&gt; <br>&lt;constructor-arg name=<span class="hljs-string">&quot;属性名&quot;</span> value=<span class="hljs-string">&quot;属性值&quot;</span>&gt;&lt;/constructor-arg&gt; <br>&lt;constructor-arg name=<span class="hljs-string">&quot;属性名&quot;</span> ref=<span class="hljs-string">&quot;其他bean类型的id&quot;</span>&gt;&lt;/constructor-arg&gt;<br>&lt;/bean&gt; <br>&lt;bean id=<span class="hljs-string">&quot;其他bean类型的id&quot;</span> class=<span class="hljs-string">&quot;类型&quot;</span>&gt;<br>&lt;/bean&gt;<br></code></pre></td></tr></table></figure><p><strong>2.使用属性的setter方法注入</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs java">&lt;!-- 通过配置文件给 bean 中的属性传值：使用 set 方法的方式<br>涉及的标签：<br>property<br>name：找的是类中 set 方法后面的部分<br>ref：给属性赋值是其他 bean 类型的<br>value：给属性赋值是基本数据类型和 string 类型的<br>实际开发中，此种方式用的较多。<br>--&gt; <br>&lt;bean id=<span class="hljs-string">&quot;类&quot;</span> class=<span class="hljs-string">&quot;类的位置&quot;</span>&gt; <br>&lt;property name=<span class="hljs-string">&quot;属性名&quot;</span> value=<span class="hljs-string">&quot;属性值&quot;</span>&gt;&lt;/property&gt; <br>&lt;property name=<span class="hljs-string">&quot;属性名&quot;</span> ref=<span class="hljs-string">&quot;其他bean类型的id&quot;</span>&gt;&lt;/property&gt;<br>&lt;/bean&gt; <br>&lt;bean id=<span class="hljs-string">&quot;其他bean类型的id&quot;</span> class=<span class="hljs-string">&quot;类型&quot;</span>&gt;<br>&lt;/bean&gt;<br></code></pre></td></tr></table></figure><p><strong>参考文献：[1] 陈恒，楼偶俊，张立杰.Java EE框架整和开发入门到实践[M].清华大学出版社，2018-：.12-16</strong><br><strong>案例来自于黑马程序员Spring教学</strong></p>]]></content>
    
    
    <categories>
      
      <category>java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>java</tag>
      
      <tag>spring</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>关于Spring框架的总结（一、spring的简单概述）</title>
    <link href="/2022/09/22/%E5%85%B3%E4%BA%8ESpring%E6%A1%86%E6%9E%B6%E7%9A%84%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%80%E3%80%81spring%E7%9A%84%E7%AE%80%E5%8D%95%E6%A6%82%E8%BF%B0%EF%BC%89/"/>
    <url>/2022/09/22/%E5%85%B3%E4%BA%8ESpring%E6%A1%86%E6%9E%B6%E7%9A%84%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%80%E3%80%81spring%E7%9A%84%E7%AE%80%E5%8D%95%E6%A6%82%E8%BF%B0%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>﻿# 关于Spring框架的总结（一、spring的简单概述）</p><h2 id="1-简介概述"><a href="#1-简介概述" class="headerlink" title="1.简介概述"></a>1.简介概述</h2><blockquote><p>关于Spring框架的简单介绍<br>1.1.什么是spring？<br>1.2.spring框架有哪些特点优势？<br>1.3.spring框架的体系结构</p></blockquote><h2 id="1-1-什么是spring"><a href="#1-1-什么是spring" class="headerlink" title="1.1.什么是spring"></a><strong>1.1.什么是spring</strong></h2>   <p>    Spring是一个轻量级Java开发框架，最早由Rod Johnson创建，目的是为了解决企业级应用开发的业务逻辑层和其他各层的耦合问题。它是一个分层的JavaSE/EE full-stack(-站式)轻量级开源框架，为开发Java应用程序提供全面的基础架构支持。<font color=red>以 IoC（Inverse Of Control：反转控制）和 AOP（Aspect Oriented Programming：面向切面编程）为内核</font color=red>，提供了展现层 Spring MVC 和持久层 Spring JDBC 以及业务层事务管理等众多的企业级应用技术，还能整合开源世界众多著名的第三方框架和类库。Spring 负责基础架构，因此Java开发者可以专注于应用程序的开发。   </p><h2 id="1-2-spring框架有哪些特点优势？"><a href="#1-2-spring框架有哪些特点优势？" class="headerlink" title="1.2.spring框架有哪些特点优势？"></a>1.2.spring框架有哪些特点优势？</h2><p>1.降低组件之间的耦合性，通过 Spring 提供的 IoC 容器，可以将对象间的依赖关系交由 Spring 进行控制，避免硬编码所造成的过度程序耦合。</p><p>2.对AOP编程的支持，方便面向切面编程。</p><p>3.通过声明式方式灵活的进行事务的管理，提高开发效率和质量。</p><p>4.方便集成各种优秀框架，如hibernate,Struts2,JPA等</p><p>5.Spring具有高度可开放性，并不强制依赖于Spring，可以自由选择Spring部分或全部</p><p>6.方便程序的测试，可以用非容器依赖的编程方式进行几乎所有的测试工作。</p><h2 id="1-3-spring框架的体系结构"><a href="#1-3-spring框架的体系结构" class="headerlink" title="1.3.spring框架的体系结构"></a>1.3.spring框架的体系结构</h2><p>Spring框架至今已集成20多个模块，这些模块发布在核心容器（Core Container）、数据访问&#x2F;集成(Data Access&#x2F;Integration) 层. Web层. AOP ( Aspect Oriented Programming,<br>面向切面的编程)模块、植入(Instrumentation) 模块、消息传输(Messaging)和测试(Test)模块中，如下图。</p><p><img src="https://img-blog.csdnimg.cn/20200709115142920.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyMDA5MjYy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p><strong>1、核心容器</strong><br>Spring的核心容器是其他模块建立的基础，由Spring core 、Spring beans、 Sping comtext、Spring context-support和Spring expression (Spring 表达式语言)等模块组成。</p><table><thead><tr><th>模块</th><th>功能作用</th></tr></thead><tbody><tr><td>Spring-core 模块</td><td>提供了框架的基本组成部分，包括控制反转(Inversion of Control,IoC)和依赖注入(Dependency Injection, DI)功能。</td></tr><tr><td>Spring-beans模块</td><td>提供了BeanFactory, 是工厂模式的一个经典实现，Spring将管理对象称为Bean。</td></tr><tr><td>Spring-context 模块</td><td>建立在Core和Beans模块的基础之上，提供一个框架式的对象访问方式，是访问定义和配置的任何对象的媒介。ApplicationContext 接口是Context模块的焦点。</td></tr><tr><td>Spring-context-support 模块</td><td>支持整合第三方库到Spring 应用程序上下文，特别是用于高速缓存(EhCache、JCache)和任务调度(CommonJ、Quartz) 的支持。</td></tr><tr><td>Spring-expression 模块</td><td>提供了强大的表达式语言去支持运行时查询和操作对象图。这是对JSP2.1规范中规定的统一表达式语言 (UnifiedEL) 的扩展。该语言支持设置和获取属性值、属性分配、方法调用、访问数组、集合和索引器的内容、逻辑和算术运算、变量命名以及从Spring的IoC容器中以名称检索对象。它还支持列表投影、选择以及常见的列表聚合。</td></tr></tbody></table><p><strong>2.AOP和Instrumentation</strong></p><table><thead><tr><th>模块</th><th>功能作用</th></tr></thead><tbody><tr><td>Spring-aop 模块</td><td>提供了一个符合AOP要求的面向切面的编程实现，允许定义方法拦截器和切入点，将代码按照功能进行分离，以便干净地解耦。</td></tr><tr><td>Spring-aspects模块</td><td>提供了与AspectJ的集成功能，AspectJ 是一个功能强大且成熟的AOP框架</td></tr><tr><td>Spring istrument模块</td><td>提供了类植入(nstrumentatonon) 支持和类加载器的实现，可以在特定的应用服务器中使用。</td></tr></tbody></table><p><strong>3.消息</strong></p><blockquote><p>spring 4.0以后新增了消息(Spring messaging)模块，该模块提供了对消息传递体系结构和协议的支持</p></blockquote><p><strong>4数据访问&#x2F;集成</strong></p><blockquote><p>数据访间集成层由JDBC、ORM. OXM. JMS和事务模块组成。</p></blockquote><table><thead><tr><th>模块</th><th>功能作用</th></tr></thead><tbody><tr><td>Spring-jdbc模块</td><td>提供了一个 JDBC的抽象层，消除了烦琐的JDBC编码和数据库厂商特有的错误代码解析。</td></tr><tr><td>Spring om模块</td><td>为流行的对象关系映射(Objeet Relational Mapping) API提供集成层，包括JPA和Hibemate.使用Spring orm模块可以将这些O&#x2F;R映射框架与Spring提供的所有其他功能结合使用，例如声明式事务管理功能。</td></tr><tr><td>Spring-oxm 模块</td><td>提供了一个支持对象&#x2F;XML映射的抽象层实现，例如JAXB、Castor、 JiBX和XStream。</td></tr><tr><td>Spring-jms 模块(Java Messaging Serice)</td><td>指Java消息传递服务，包含用于生产和使用消息的功能。自Sping.1以后，提供了与Spring mesgingg模块的集成。</td></tr><tr><td>Sping-tg模块(事务模块)</td><td>支持用于实现特殊接口和所有POIO (普通Java对象)类的编程和事务式声明管理</td></tr></tbody></table><p><strong>5.Web</strong></p><blockquote><p>Web层由Spring-web. Spring webmvc、Spring websocket和Portet模块组成。</p></blockquote><table><thead><tr><th>模块</th><th>功能作用</th></tr></thead><tbody><tr><td>Spring-web模块</td><td>提供了基本的Web开发集成功能， 例如多文件上传功能、使用Servlet监听器初始化一个 loC容器以及Web应用上下文。</td></tr><tr><td>Spring-webmvc模块</td><td>也称为Web-Servlet模块，包含用于Web应用程序的SpringMVC和REST Web Services实现。Spring MVC框架提供了领城模型代码和Web表单之间的清晰分离，并与Spring Framework的所有其他功能集成</td></tr><tr><td>Spring-websocket 模块</td><td>Spring 4.0以后新增的模块，它提供了WebSocket和SockJS的实现。</td></tr><tr><td>Portler模块</td><td>类似于Serlet模块的功能，提供了Porlet环境下的MVC实现。</td></tr></tbody></table><p><strong>6.测试</strong></p><blockquote><p>Spring-test 模块支持使用JUnit或TestNG对Spring组件进行单元测试和集成测试。</p></blockquote><p>参考文献：[1] 陈恒，楼偶俊，张立杰.Java EE框架整和开发入门到实践[M].清华大学出版社，2018-：.1-7</p>]]></content>
    
    
    <categories>
      
      <category>java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>java</tag>
      
      <tag>spring</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Golang内存分配与内存逃逸</title>
    <link href="/2022/09/20/Go%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E4%B8%8E%E5%86%85%E5%AD%98%E9%80%83%E9%80%B8/"/>
    <url>/2022/09/20/Go%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E4%B8%8E%E5%86%85%E5%AD%98%E9%80%83%E9%80%B8/</url>
    
    <content type="html"><![CDATA[<h1 id="Go内存分配与内存逃逸"><a href="#Go内存分配与内存逃逸" class="headerlink" title="Go内存分配与内存逃逸"></a>Go内存分配与内存逃逸</h1><h2 id="内存为什么需要管理"><a href="#内存为什么需要管理" class="headerlink" title="内存为什么需要管理"></a>内存为什么需要管理</h2><p>当存储的东西越来越多，也就发现物理内存的容量依然是不够用，那么对物理内存的利用率和合理的分配，管理就变得非常的重要。</p><p>（1）操作系统就会对内存进行非常详细的管理。</p><p>（2）基于操作系统的基础上，不同语言的内存管理机制也应允而生，有的一些语言并没有提供自动的内存管理模式，有的语言就已经提供了自身程序的内存管理模式，如表2所示。</p><h6 id="表2-自动与非自动内存管理的语言"><a href="#表2-自动与非自动内存管理的语言" class="headerlink" title="表2 自动与非自动内存管理的语言"></a>表2 自动与非自动内存管理的语言</h6><table><thead><tr><th><strong>内存自动管理的语言（部分）</strong></th><th><strong>内存非自动管理的语言（部分）</strong></th></tr></thead><tbody><tr><td>Golang</td><td>C</td></tr><tr><td>Java</td><td>C++</td></tr><tr><td>Python</td><td>Rust</td></tr></tbody></table><p>所以为了降低内存管理的难度，像C、C++这样的编程语言会完全将分配和回收内存的权限交给开发者，而Rust则是通过生命周期限定开发者对非法权限内存的访问来自动回收，因而并没有提供自动管理的一套机制。但是像Golang、Java、Python这类为了完全让开发则关注代码逻辑本身，语言层提供了一套管理模式。因为Golang编程语言给开发者提供了一套内存管理模式，所以开发者有必要了解一下Golang做了哪些助力的功能。</p><p>在理解Golang语言层内存管理之前，应先了解操作系统针对物理内存做了哪些管理的方式。当插上内存条之后，通过操作系统是如何将软件存放在这个绿色的物理内存条中去的。</p><h2 id="为什么需要关心内存分配问题"><a href="#为什么需要关心内存分配问题" class="headerlink" title="为什么需要关心内存分配问题"></a><strong>为什么需要关心内存分配问题</strong></h2><hr><p>每个工程师的时间都如此宝贵，在继续读这篇文章之前，需要你先回答几个问题，如果得到的答案是否定的，那可能本文章里写的内容对你并没有什么帮助。但是，如果你遇到了因内存分配而导致的性能问题，可能这篇文章能带你理解 Golang 的内存分配的冰山一角，带你入个门。</p><p>问题如下：</p><ul><li>你的程序是性能敏感型吗？</li><li>GC 带来的延迟影响到了你的程序性能吗？</li><li>你的程序有过多的堆内存分配吗？</li></ul><p>如果你命中上面问题的其中一个或两个，那这篇文章适合你继续读下去。或你根本不知道如何回答这些问题，可能去了解下 go 性能观测相关的知识（pprof 的使用等）对你更有帮助。</p><p><strong>下面正文开始。</strong></p><h2 id="Golang-简要内存划分"><a href="#Golang-简要内存划分" class="headerlink" title="Golang 简要内存划分"></a><strong>Golang 简要内存划分</strong></h2><hr><p><img src="https://ask.qcloudimg.com/http-save/5469577/b1e2510bc404791b9a0909da0a0f1a99.webp?imageView2/2/w/1620/format/jpg" alt="img"></p><p>可以简单的认为 Golang 程序在启动时，会向操作系统申请一定区域的内存，分为栈（Stack）和堆（Heap）。栈内存会随着函数的调用分配和回收；堆内存由程序申请分配，由垃圾回收器（Garbage Collector）负责回收。性能上，栈内存的使用和回收更迅速一些；尽管Golang 的 GC 很高效，但也不可避免的会带来一些性能损耗。因此，Go 优先使用栈内存进行内存分配。在不得不将对象分配到堆上时，才将特定的对象放到堆中。</p><h2 id="内存分配过程分析"><a href="#内存分配过程分析" class="headerlink" title="内存分配过程分析"></a><strong>内存分配过程分析</strong></h2><hr><p>本部分，将以代码的形式，分别介绍栈内存分配、指针作为参数情况下的栈内存分配、指针作为返回值情况下的栈内存分配并逐步引出逃逸分析和几个内存逃逸的基本原则。</p><p>正文开始，Talk is cheap，show me the code。</p><h2 id="栈内存分配"><a href="#栈内存分配" class="headerlink" title="栈内存分配"></a><strong>栈内存分配</strong></h2><p>我将以一段简单的代码作为示例，分析这段代码的内存分配过程。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs javascript">package main<br><span class="hljs-keyword">import</span> <span class="hljs-string">&quot;fmt&quot;</span><br>func <span class="hljs-title function_">main</span>(<span class="hljs-params"></span>) &#123;  n := <span class="hljs-number">4</span>  n2 := <span class="hljs-title function_">square</span>(n)  fmt.<span class="hljs-title class_">Println</span>(n2)&#125;<br>func <span class="hljs-title function_">square</span>(n int) int&#123;  <span class="hljs-keyword">return</span> n * n&#125;<br></code></pre></td></tr></table></figure><p>复制</p><p>代码的功能很简单，一个 main 函数作为程序入口，定义了一个变量n，定义了另一个函数 squire ，返回乘方操作后的 int 值。最后，将返回的值打印到控制台。程序输出为16。</p><p>下面开始逐行进行分析，解析调用时，go 运行时是如何对内存进行分配的。</p><p><img src="https://ask.qcloudimg.com/http-save/5469577/479e83aa67b17d920bd71f6625afe1c9.webp?imageView2/2/w/1620/format/jpg" alt="img"></p><p>当代码运行到第6行，进入 main 函数时，会在栈上创建一个 Stack frame，存放本函数中的变量信息。包括函数名称，变量等。</p><p><img src="https://ask.qcloudimg.com/developer-images/article/5469577/h4jok0khik.png?imageView2/2/w/1620" alt="img"></p><p>当代码运行到第7行时，go 会在栈中压入一个新的 Stack Frame，用于存放调用 square 函数的信息；包括函数名、变量 n 的值等。此时，计算4 * 4 的值，并返回。</p><p><img src="https://ask.qcloudimg.com/http-save/5469577/e595bc8e8421e68646c147ea1cb411ab.webp?imageView2/2/w/1620/format/jpg" alt="img"></p><p>当 square 函数调用完成，返回16到 main 函数后，将16赋值给 n2变量。注意，原来的 stack frame 并不会被 go 清理掉，而是如栈左侧的箭头所示，被标记为不合法。上图夹在红色箭头和绿色箭头之间的横线可以理解为 go 汇编代码中的 SP 栈寄存器的值，当程序申请或释放栈内存时，只需要修改 SP 寄存器的值，这种栈内存分配方式省掉了清理栈内存空间的耗时【1】。</p><p><img src="https://ask.qcloudimg.com/developer-images/article/5469577/m6ls2qtdbl.png?imageView2/2/w/1620" alt="img"></p><p>接下来，调用 fmt.Println 时，SP 寄存器的值会进一步增加，覆盖掉原来 square 函数的 stack frame，完成 print 后，程序正常退出。</p><h2 id="指针作为参数情况下的栈内存分配"><a href="#指针作为参数情况下的栈内存分配" class="headerlink" title="指针作为参数情况下的栈内存分配"></a><strong>指针作为参数情况下的栈内存分配</strong></h2><p>还是同样的过程，看如下这段代码。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs javascript">package main<br><span class="hljs-keyword">import</span> <span class="hljs-string">&quot;fmt&quot;</span><br>func <span class="hljs-title function_">main</span>(<span class="hljs-params"></span>) &#123;  n := <span class="hljs-number">4</span>  <span class="hljs-title function_">increase</span>(&amp;n)  fmt.<span class="hljs-title class_">Println</span>(n)&#125;<br>func <span class="hljs-title function_">increase</span>(<span class="hljs-params">i *int</span>) &#123;  *i++&#125;<br></code></pre></td></tr></table></figure><p>main 作为程序入口，声明了一个变量 n，赋值为4。声明了一个函数  increase，使用一个 int 类型的指针 i 作为参数，increase 函数内，对指针 i 对应的值进行自增操作。最后 main 函数中打印了 n 的值。程序输出为5。</p><p><img src="https://ask.qcloudimg.com/developer-images/article/5469577/7snaopjz26.png?imageView2/2/w/1620" alt="img"></p><p>当程序运行到 main 函数的第6行时，go 在栈上分配了一个 stack frame ，对变量 n 进行了赋值，n 在内存中对应的地址为0xc0008771，此时程序将继续向下执行，调用 increase 函数。</p><p><img src="https://ask.qcloudimg.com/developer-images/article/5469577/nzbfzfdkup.png?imageView2/2/w/1620" alt="img"></p><p>这时，increase 函数对应的 stack fream 被创建，i 被赋值为变量 n对应的地址值0xc0008771，然后进行自增操作。</p><p><img src="https://ask.qcloudimg.com/developer-images/article/5469577/u14njxdglz.png?imageView2/2/w/1620" alt="img"></p><p>当 increase 函数运行结束后，SP 寄存器会上移，将之前分配的 stack freme 标记为不合法。此时，程序运行正常，并没有因为 SP 寄存器的改动而影响程序的正确性，内存中的值也被正确的修改了。</p><h2 id="指针作为返回值情况下的栈内存分配"><a href="#指针作为返回值情况下的栈内存分配" class="headerlink" title="指针作为返回值情况下的栈内存分配"></a><strong>指针作为返回值情况下的栈内存分配</strong></h2><p>文章之前的部分分别介绍了普通变量作为参数和将指针作为参数情况下的栈内存使用，本部分来介绍将指针作为返回值，返回给调用方的情况下，内存是如何分配的，并引出内存逃逸相关内容。来看这段代码：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs javascript">package main<br><span class="hljs-keyword">import</span> <span class="hljs-string">&quot;fmt&quot;</span><br>func <span class="hljs-title function_">main</span>(<span class="hljs-params"></span>) &#123;  n := <span class="hljs-title function_">initValue</span>()  fmt.<span class="hljs-title class_">Println</span>(*n/<span class="hljs-number">2</span>)&#125;<br>func <span class="hljs-title function_">initValue</span>() *int &#123;  i := <span class="hljs-number">4</span>  <span class="hljs-keyword">return</span> &amp;i&#125;<br></code></pre></td></tr></table></figure><p>main 函数中，调用了 initValue 函数，该函数返回一个 int 指针并赋值给 n，指针对应的值为4。随后，main 函数调用 fmt.Println 打印了指针 n &#x2F; 2对应的值。程序输出为2。</p><p><img src="https://ask.qcloudimg.com/developer-images/article/5469577/945gf1ih2a.png?imageView2/2/w/1620" alt="img"></p><p>程序调用 initValue 后，将 i 的地址赋值给变量 n 。注意，如果这时，变量 i 的位置在栈上，则可能会随时被覆盖掉。</p><p><img src="https://ask.qcloudimg.com/developer-images/article/5469577/8ydk3bi2lv.png?imageView2/2/w/1620" alt="img"></p><p>在调用 fmt.Println 时，Stack Frame 会被重新创建，变量 i 被赋值为*n&#x2F;2也就是2，会覆盖掉原来 n 所指向的变量值。这会导致及其严重的问题。在面对 sharing up 场景时，go 通常会将变量分配到堆中，如下图所示：</p><p><img src="https://ask.qcloudimg.com/http-save/5469577/16ab1617ecd4832e038e07e9a612fd69.webp?imageView2/2/w/1620/format/jpg" alt="img"></p><p>通过上面的分析，可以看到在面对被调用的函数返回一个指针类型时将对象分配到栈上会带来严重的问题，因此 Go 将变量分配到了堆上。这种分配方式保证了程序的安全性，但也不可避免的增加了堆内存创建，并需要在将来的某个时候，需要 GC 将不再使用的内存清理掉。</p><h2 id="内存分配原则"><a href="#内存分配原则" class="headerlink" title="内存分配原则"></a><strong>内存分配原则</strong></h2><hr><p>经过上述分析，可以简单的归纳几条原则。</p><ul><li>Sharing down typically stays on the stack 在调用方创建的变量或对象，通过参数的形式传递给被调用函数，这时，在调用方创建的内存空间通常在栈上。这种在调用方创建内存，在被调用方使用该内存的“内存共享”方式，称之为 Sharing down。</li><li>Sharing up typically escapes to the heap 在被调用函数内创建的对象，以指针的形式返回给调用方的情况下，通常，创建的内存空间在堆上。这种在被调用方创建，在调用方使用的“内存共享”方式，称之为 Sharing up。</li><li>Only the compiler knows 之所以上面两条原则都加了通常，因为具体的分配方式，是由编译器确定的，一些编译器后端优化，可能会突破这两个原则，因此，具体的分配逻辑，只有编译器（或开发编译器的人）知道。</li></ul><h2 id="使用-go-build-命令确定内存逃逸情况"><a href="#使用-go-build-命令确定内存逃逸情况" class="headerlink" title="使用 go build 命令确定内存逃逸情况"></a><strong>使用 go build 命令确定内存逃逸情况</strong></h2><hr><p>值得注意的是，Go 在判断一个变量或对象是否需要逃逸到堆的操作，是在编译器完成的；也就是说，当代码写好后，经过编译器编译后，会在二进制中进行特定的标注，声明指定的变量要被分配到堆或栈。可以使用如下命令在编译期打印出内存分配逻辑，来具体获知特定变量或对象的内存分配位置。</p><p>查看 go help 可以看到 go build 其实是在调用 go tool compile。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs javascript">go help build ... -gcflags <span class="hljs-string">&#x27;[pattern=]arg list&#x27;</span>        <span class="hljs-variable language_">arguments</span> to pass on each go tool compile invocation....<br></code></pre></td></tr></table></figure><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs javascript">go tool compile -h...-m    print optimization decisions...-l    disable inlining...<br></code></pre></td></tr></table></figure><p>其中，需要关心的参数有两个，</p><ul><li>-m 显示优化决策</li><li>-l 禁止使用内联【2】</li></ul><p>代码如下：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs javascript">package main<br>func <span class="hljs-title function_">main</span>(<span class="hljs-params"></span>) &#123;  n := <span class="hljs-title function_">initValue</span>()  <span class="hljs-title function_">println</span>(*n / <span class="hljs-number">2</span>)<br>  o := <span class="hljs-title function_">initObj</span>()  <span class="hljs-title function_">println</span>(o)<br>  f := <span class="hljs-title function_">initFn</span>()  <span class="hljs-title function_">println</span>(f)<br>  num := <span class="hljs-number">5</span>  result := <span class="hljs-title function_">add</span>(num)  <span class="hljs-title function_">println</span>(result)&#125;<br>func <span class="hljs-title function_">initValue</span>() *int &#123;  i := <span class="hljs-number">3</span>                <span class="hljs-comment">// ./main.go:19:2: moved to heap: i  return &amp;i&#125;</span><br>type <span class="hljs-title class_">Obj</span> struct &#123;  i int&#125;<br>func <span class="hljs-title function_">initObj</span>() *<span class="hljs-title class_">Obj</span> &#123;  <span class="hljs-keyword">return</span> &amp;<span class="hljs-title class_">Obj</span>&#123;<span class="hljs-attr">i</span>: <span class="hljs-number">3</span>&#125;      <span class="hljs-comment">// ./main.go:28:9: &amp;Obj literal escapes to heap&#125;</span><br>func <span class="hljs-title function_">initFn</span>() <span class="hljs-title function_">func</span>(<span class="hljs-params"></span>) &#123;  <span class="hljs-keyword">return</span> <span class="hljs-title function_">func</span>(<span class="hljs-params"></span>) &#123;       <span class="hljs-comment">// ./main.go:32:9: func literal escapes to heap    println(&quot;I am a function&quot;)  &#125;&#125;</span><br>func <span class="hljs-title function_">add</span>(i int) int &#123;  <span class="hljs-keyword">return</span> i + <span class="hljs-number">1</span>&#125;<br></code></pre></td></tr></table></figure><p>完整的构建命令和输出如下：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs javascript">go build -gcflags=<span class="hljs-string">&quot;-m -l&quot;</span> <br># _/<span class="hljs-title class_">Users</span>/rocket/workspace/stack-or-heap./main.<span class="hljs-property">go</span>:<span class="hljs-number">19</span>:<span class="hljs-number">2</span>: moved to <span class="hljs-attr">heap</span>: i./main.<span class="hljs-property">go</span>:<span class="hljs-number">24</span>:<span class="hljs-number">9</span>: &amp;<span class="hljs-title class_">Obj</span> literal escapes to heap./main.<span class="hljs-property">go</span>:<span class="hljs-number">28</span>:<span class="hljs-number">9</span>: func literal escapes to heap<br></code></pre></td></tr></table></figure><p>可以看到，sharing up 的情况（initValue，initObj，initFn）内存空间被分配到了堆上。sharing down 的情况（add）内存空间在栈上。</p><p>这里给读者留个问题，大家可以研究下 moved to heap 和 escapes to heap 的区别。</p><h1 id="内存逃逸"><a href="#内存逃逸" class="headerlink" title="内存逃逸"></a>内存逃逸</h1><h2 id="怎么答"><a href="#怎么答" class="headerlink" title="怎么答"></a><strong>怎么答</strong></h2><p><code>golang程序变量</code>会携带有一组校验数据，用来证明它的整个生命周期是否在运行时完全可知。如果变量通过了这些校验，它就可以在<code>栈上</code>分配。否则就说它 <code>逃逸</code> 了，必须在<code>堆上分配</code>。</p><p>能引起变量逃逸到堆上的<strong>典型情况</strong>：</p><ul><li><strong>在方法内把局部变量指针返回</strong> 局部变量原本应该在栈中分配，在栈中回收。但是由于返回时被外部引用，因此其生命周期大于栈，则溢出。</li><li><strong>发送指针或带有指针的值到 channel 中。</strong> 在编译时，是没有办法知道哪个 <a href="https://www.zhihu.com/search?q=goroutine&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:%22145468000%22%7D">goroutine</a> 会在 channel 上接收数据。所以编译器没法知道变量什么时候才会被释放。</li><li><strong>在一个切片上存储指针或带指针的值。</strong> 一个典型的例子就是 []*string 。这会导致切片的内容逃逸。尽管其后面的数组可能是在栈上分配的，但其引用的值一定是在堆上。</li><li><strong>slice 的背后数组被重新分配了，因为 append 时可能会超出其容量( cap )。</strong> slice 初始化的地方在编译时是可以知道的，它最开始会在栈上分配。如果切片背后的存储要基于运行时的数据进行扩充，就会在堆上分配。</li><li><strong>在 interface 类型上调用方法。</strong> 在 interface 类型上调用方法都是动态调度的 —— 方法的真正实现只能在运行时知道。想像一个 io.Reader 类型的变量 r , 调用 r.Read(b) 会使得 r 的值和切片b 的背后存储都逃逸掉，所以会在堆上分配。</li></ul><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a><strong>举例</strong></h2><ul><li>通过一个例子加深理解，接下来尝试下怎么通过 <code>go build -gcflags=-m</code> 查看逃逸的情况。</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">package</span> main<br><span class="hljs-keyword">import</span> <span class="hljs-string">&quot;fmt&quot;</span><br><span class="hljs-keyword">type</span> A <span class="hljs-keyword">struct</span> &#123;<br> s <span class="hljs-type">string</span><br>&#125;<br><span class="hljs-comment">// 这是上面提到的 &quot;在方法内把局部变量指针返回&quot; 的情况</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">foo</span><span class="hljs-params">(s <span class="hljs-type">string</span>)</span></span> *A &#123;<br> a := <span class="hljs-built_in">new</span>(A) <br> a.s = s<br> <span class="hljs-keyword">return</span> a <span class="hljs-comment">//返回局部变量a,在C语言中妥妥野指针，但在go则ok，但a会逃逸到堆</span><br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<br> a := foo(<span class="hljs-string">&quot;hello&quot;</span>)<br> b := a.s + <span class="hljs-string">&quot; world&quot;</span><br> c := b + <span class="hljs-string">&quot;!&quot;</span><br> fmt.Println(c)<br>&#125;<br></code></pre></td></tr></table></figure><p>执行<code>go build -gcflags=-m main.go</code></p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">go</span> build -gcflags=-m main.<span class="hljs-keyword">go</span><br># command-line-arguments<br>./main.<span class="hljs-keyword">go</span>:<span class="hljs-number">7</span>:<span class="hljs-number">6</span>: can inline foo<br>./main.<span class="hljs-keyword">go</span>:<span class="hljs-number">13</span>:<span class="hljs-number">10</span>: inlining call to foo<br>./main.<span class="hljs-keyword">go</span>:<span class="hljs-number">16</span>:<span class="hljs-number">13</span>: inlining call to fmt.Println<br>/<span class="hljs-keyword">var</span>/folders/<span class="hljs-number">45</span>/qx9lfw2s2zzgvhzg3mtzkwzc0000gn/T/<span class="hljs-keyword">go</span>-build409982591/b001/_gomod_.<span class="hljs-keyword">go</span>:<span class="hljs-number">6</span>:<span class="hljs-number">6</span>: can inline init<span class="hljs-number">.0</span><br>./main.<span class="hljs-keyword">go</span>:<span class="hljs-number">7</span>:<span class="hljs-number">10</span>: leaking param: s<br>./main.<span class="hljs-keyword">go</span>:<span class="hljs-number">8</span>:<span class="hljs-number">10</span>: <span class="hljs-built_in">new</span>(A) escapes to heap<br>./main.<span class="hljs-keyword">go</span>:<span class="hljs-number">16</span>:<span class="hljs-number">13</span>: io.Writer(os.Stdout) escapes to heap<br>./main.<span class="hljs-keyword">go</span>:<span class="hljs-number">16</span>:<span class="hljs-number">13</span>: c escapes to heap<br>./main.<span class="hljs-keyword">go</span>:<span class="hljs-number">15</span>:<span class="hljs-number">9</span>: b + <span class="hljs-string">&quot;!&quot;</span> escapes to heap<br>./main.<span class="hljs-keyword">go</span>:<span class="hljs-number">13</span>:<span class="hljs-number">10</span>: main <span class="hljs-built_in">new</span>(A) does not escape<br>./main.<span class="hljs-keyword">go</span>:<span class="hljs-number">14</span>:<span class="hljs-number">11</span>: main a.s + <span class="hljs-string">&quot; world&quot;</span> does not escape<br>./main.<span class="hljs-keyword">go</span>:<span class="hljs-number">16</span>:<span class="hljs-number">13</span>: main []<span class="hljs-keyword">interface</span> &#123;&#125; literal does not escape<br>&lt;autogenerated&gt;:<span class="hljs-number">1</span>: os.(*File).<span class="hljs-built_in">close</span> .this does not escape<br></code></pre></td></tr></table></figure><ul><li><code>./main.go:8:10: new(A) escapes to heap</code> 说明 <code>new(A)</code> 逃逸了,符合上述提到的常见情况中的第一种。</li><li><code>./main.go:14:11: main a.s + &quot; world&quot; does not escape</code> 说明 <code>b</code> 变量没有逃逸，因为它只在方法内存在，会在方法结束时被回收。</li><li><code>./main.go:15:9: b + &quot;!&quot; escapes to heap</code> 说明 <code>c</code> 变量逃逸，通过<code>fmt.Println(a ...interface&#123;&#125;)</code>打印的变量，都会发生逃逸，感兴趣的朋友可以去查查为什么。</li><li>以上操作其实就叫<strong>逃逸分析</strong></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><hr><p>1.因为栈比堆更高效，不需要 GC，因此 Go 会尽可能的将内存分配到栈上。</p><p>2.当分配到栈上可能引起非法内存访问等问题后，会使用堆，主要场景有：</p><ol><li>当一个值可能在函数被调用后访问，这个值极有可能被分配到堆上。</li><li>当编译器检测到某个值过大，这个值会被分配到堆上。</li><li>当编译时，编译器不知道这个值的大小（slice、map…）这个值会被分配到堆上。</li></ol><p>3.Sharing down typically stays on the stack</p><p>4.Sharing up typically escapes to the heap</p><p>5.Don’t guess, Only the compiler knows</p><p>6.Golang中一个函数内局部变量，不管是不是动态new出来的，它会被分配在堆还是栈，是由编译器做逃逸分析之后做出的决定。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a><strong>参考文献</strong></h1><p>【1】Go语言设计与实现：<a href="https://draveness.me/golang/docs/part3-runtime/ch07-memory/golang-stack-management/#%E5%AF%84%E5%AD%98%E5%99%A8">https://draveness.me/golang/docs/part3-runtime/ch07-memory/golang-stack-management/#%E5%AF%84%E5%AD%98%E5%99%A8</a></p><p>【2】Inlining optimisations in Go：<a href="https://dave.cheney.net/2020/04/25/inlining-optimisations-in-go">https://dave.cheney.net/2020/04/25/inlining-optimisations-in-go</a></p><p>【3】Golang FAQ：<a href="https://golang.org/doc/faq#stack_or_heap">https://golang.org/doc/faq#stack_or_heap</a></p><p>【4】知乎：<a href="https://zhuanlan.zhihu.com/p/145468000">https://zhuanlan.zhihu.com/p/145468000</a></p>]]></content>
    
    
    <categories>
      
      <category>golang</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>golang</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Golang为什么不用Java的gc模式</title>
    <link href="/2022/09/12/Golang%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E7%94%A8Java%E7%9A%84gc%E6%A8%A1%E5%BC%8F/"/>
    <url>/2022/09/12/Golang%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E7%94%A8Java%E7%9A%84gc%E6%A8%A1%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="Golang为什么不用Java的gc模式"><a href="#Golang为什么不用Java的gc模式" class="headerlink" title="Golang为什么不用Java的gc模式"></a>Golang为什么不用Java的gc模式</h1><blockquote><p><strong>为什么Go、Julia 和 Rust 等现代语言不需要像 Java C# 那样复杂的垃圾收集器？</strong></p></blockquote><p>为了解释原因，我们需要了解垃圾收集器是如何工作的，以及不同的语言如何以不同的方式分配内存。我们首先了解为什么 Java 特别需要如此复杂的垃圾收集器。</p><p>以下面几个主题为出发点来做相关介绍：</p><ul><li>为什么 Java 如此依赖快速 GC。介绍 Java 语言本身中对 GC 造成很大压力的一些设计选择。</li><li>内存碎片以及它如何影响 GC 设计。为什么这对 Java 很重要，而对 Go 却没有那么重要。</li><li>值类型以及它们如何改变 GC 。</li><li>分代GC以及为什么 Go 不需要。</li><li>逃逸分析——Go 如何用来减少 GC 压力的技巧。</li><li>分代 垃圾收集器——在 Java 世界中很重要，但 Go 以某种方式避免了对它的需求。为什么？</li><li>Concurrent Garbage Collection — Go 如何通过使用多个线程运行并发垃圾收集器来解决许多 GC 挑战。为什么使用 Java 更难做到这一点。</li><li>对 Go GC 的常见批评以及为什么批评背后的许多假设通常是有缺陷或完全错误的。</li><li>为什么低延迟对 Java 也很重要</li></ul><h2 id="为什么-Java-比其他人更需要快速-GC"><a href="#为什么-Java-比其他人更需要快速-GC" class="headerlink" title="为什么 Java 比其他人更需要快速 GC"></a>为什么 Java 比其他人更需要快速 GC</h2><p><strong>背景：</strong>Java 设计工作开始时。垃圾收集器风靡一时。研究看起来很有希望，Java 的设计者将赌注押在高级垃圾收集器上，这些垃圾收集器能够从根本上解决管理内存方面的所有挑战。</p><p>出于这个原因，Java 中的所有对象都设计为在堆上分配，但整数和浮点值等原始类型除外。在谈到内存分配时，我们一般会区分所谓的堆和栈。堆栈使用起来非常快，但空间有限，只能用于在函数调用的生命周期之后不需要存在的对象。它仅适用于局部变量。堆可用于所有对象。Java 基本上忽略了堆栈并选择在堆上分配所有内容，除了整数和浮点数等原语。每当您<code>new Something()</code>使用 Java 编写代码时，都会消耗堆上的内存。</p><p>然而，这种类型的内存管理在内存使用方面实际上是相当昂贵的。你会认为创建一个只有 32 位整数的对象只需要 4 个字节的内存。</p><p>但是，为了让垃圾收集器工作，Java 会存储一个标头，其中包含以下信息：</p><ul><li>类型 — 标识对象的类别或类型。</li><li>Lock — 用于同步语句。</li><li>标记 - 在垃圾收集器的标记和扫描面期间使用。</li></ul><p>该数据通常为 16 个字节。因此，标题数据与实际数据的比率为 4:1。Java 对象的 C++ 源代码定义为：<a href="http://hg.openjdk.java.net/jdk8/jdk8/hotspot/file/87ee5ee27509/src/share/vm/oops/oop.hpp">OpenJDK Base Class</a>。</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs angelscript"><span class="hljs-keyword">class</span> <span class="hljs-symbol">oopDesc</span> &#123;<br>    volatile markOop  _mark;   <span class="hljs-comment">// for mark and sweep</span><br>    Klass*           _klass;   <span class="hljs-comment">// the type</span><br>&#125;<br></code></pre></td></tr></table></figure><h2 id="内存碎片"><a href="#内存碎片" class="headerlink" title="内存碎片"></a>内存碎片</h2><p>当 Java 分配一个对象数组时，它真正做的是创建一个引用数组，指向内存中某个其他位置的对象。这些对象最终可能分散在堆内存周围。这对性能不利，因为现代微处理器不读取单个数据字节。因为启动内存传输很慢，微处理器每次尝试访问一个特定的内存位置时总是读取一个大的连续内存块。</p><p><img src="https://cdn.jsdelivr.net/gh/longpi1/blog-img/20220912124051.png"></p><p>这块内存称为高速缓存行。CPU 有自己的高速内存，称为高速缓存。这比主存储器小得多。它用于存储最近访问的对象，因为这些对象很可能会再次被访问。如果主内存是碎片化的，这意味着高速缓存行将被碎片化，CPU 高速缓存将被大量无用数据填满。</p><p><strong>Java如何克服内存碎片</strong></p><p>为了解决这些主要缺点，Java 维护人员在高级垃圾收集器上投入了大量资金。这些做一些称为<em>压缩</em>的事情。压缩涉及在内存中移动对象并将它们收集到内存中的连续块中。这并不便宜。不仅将块从一个内存位置移动到另一个内存位置会消耗 CPU 周期，而且更新对这些对象的每个引用以指向新位置也会消耗 CPU 周期。</p><p>进行这些更新需要冻结所有线程。您不能在使用它们时更新参考。这通常会导致 Java 程序完全冻结数百毫秒，其中对象移动、引用更新和未使用的内存回收。</p><p><strong>增加复杂性</strong></p><p>为了减少这些长时间的停顿，Java 使用了所谓的<em>分代垃圾收集器</em>. 这些都是基于以下前提：</p><blockquote><p>程序中分配的大多数值很快就会被使用，因此 GC 可以花更多时间查看最近分配的对象。</p></blockquote><p>这就是为什么 Java 将它们分配的对象分成两组：</p><ul><li>旧对象——在 GC 的多次标记和清除操作中幸存下来的对象。每次标记和扫描都会更新生成计数器，以跟踪对象的年龄。</li><li>年轻对象——这些对象的生成计数器较低。这意味着它们最近才被分配。</li></ul><p>Java 更积极地调查最近分配的对象并检查它们是否应该被回收或移动。随着对象年龄的增长，它们会被移出年轻代区域。</p><p>所有这些自然会产生更多的复杂性。它需要更多的发展。</p><p><strong>现代语言如何避免与 Java 相同的陷阱</strong></p><p>现代语言不需要像 Java 和 C# 这样的复杂垃圾收集器。这是因为它们没有被设计成同样程度地依赖它们。</p><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs smali">// Go: Make an an<span class="hljs-built_in"> array </span>of 15 000 Point objects in<br>type Point struct &#123;<br>    X, Y<span class="hljs-built_in"> int</span><br><span class="hljs-built_in"></span>&#125;<br>var points [15000]Point<br></code></pre></td></tr></table></figure><p>在上面的 Go 代码示例中，我们分配了 15000 个<code>Point</code>对象。这只是一个单一的分配，产生一个单一的指针。在 Java 中，这需要 15 000 个单独的分配，每个分配都产生一个必须管理的单独引用。每个<code>Point</code>对象都有我之前写过的 16 字节头开销。在 Go、Julia 或 Rust 中，你都不会得到这个开销。这些对象通常是无标题的。</p><p>在 Java 中，GC 获得它必须跟踪和管理的 15000 个单独的对象。Go 只有 1 个要跟踪的对象。</p><h2 id="值类型"><a href="#值类型" class="headerlink" title="值类型"></a>值类型</h2><p>下面的代码定义了一个矩形，其中一个<code>Min</code>和<code>Max</code>点定义了它的范围。</p><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-keyword">type</span> <span class="hljs-type">Rect</span> struct &#123;<br>   <span class="hljs-type">Min</span>, <span class="hljs-type">Max</span> <span class="hljs-type">Point</span><br>&#125;<br></code></pre></td></tr></table></figure><p>这成为一个连续的内存块。在 Java 中，这将变成一个<code>Rect</code>对象，其中引用了两个单独的对象，Min<code>和</code>Max<code>point 对象。因此在 Java 中，一个 的实例</code>Rect&#96;需要 3 次分配，但在 Go、Rust、C&#x2F;C++ 和 Julia 中只需要 1 次分配。</p><p><img src="https://cdn.jsdelivr.net/gh/longpi1/blog-img/image-20220912125744670.png" alt="image-20220912125744670"></p><p>左边是 Java 风格的内存碎片。在 Go、C&#x2F;C++、Julia 等中可能存在正确的连续内存块。</p><p>在将 Git 移植到 Java 时，缺少值类型会产生重大问题。没有值类型，很难获得良好的性能。正如 Shawn O. Pearce<a href="https://marc.info/?l=git&m=124111702609723">在 JGit 开发者邮件列表中所说</a>：</p><blockquote><p>JGit 苦于没有一种有效的方式来表示 SHA-1。C 可以说<code>unsigned char[20]</code>并将其内联到容器的内存分配中。<code>byte[20]</code>Java 中的A将花费<em>额外</em>的16 字节内存，并且访问速度较慢，因为字节本身与容器对象位于不同的内存区域。我们尝试通过从 a 转换为 5 个整数来解决它<code>byte[20]</code>，但这会花费我们的机器指令。</p></blockquote><p>我们在那里谈论什么？在 Go 中，我可以做与 C&#x2F;C++ 相同的事情并定义如下结构：</p><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-keyword">type</span> <span class="hljs-type">Sha1</span> struct &#123;<br>   data [20]byte<br>&#125;<br></code></pre></td></tr></table></figure><p>然后这些字节将成为一个内存块的一部分。Java 将创建一个指向内存中其他位置的指针。</p><p>Java 开发人员意识到他们搞砸了，并且您确实需要值类型才能获得良好的性能。您可以称该陈述为夸张，但随后您需要解释<a href="https://en.wikipedia.org/wiki/Project_Valhalla_(Java_language)">Project Valhalla</a>。这是 Oracle 为提供 Java 值类型而带头的一项努力，他们阐明这样做的原因正是我在这里所说的。</p><p><strong>值类型还不够</strong></p><p>那么<em>Project Valhalla</em>会解决Java 的问题吗？并不真地。它只会使 Java 与 C# 处于同等地位。C# 在 Java 之后几年问世，并从那时起意识到垃圾收集器并不是每个人都认为的那样神奇。因此，他们添加了值类型。</p><p>但是，在内存管理灵活性方面，这并没有使 C# 和 Java 与 Go 和 C&#x2F;C++ 等语言处于同等地位。Java 不支持真正的指针。在 Go 中，我可以这样写：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Go 指针用法var </span><br>ptr *Point = &amp;rect.Min <span class="hljs-comment">// 将指向 Min 的指针存储在 ptr </span><br>*ptr = Point(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>) <span class="hljs-comment">// 替换 rect.Min</span><br></code></pre></td></tr></table></figure><p>您可以在 Go 中获取对象的地址或对象的字段，就像在 C&#x2F;C++ 中一样，并将其存储在指针中。然后，您可以传递此指针并使用它来修改它指向的字段。这意味着您可以在 Go 中创建大值对象并将其作为指向函数的指针传递以优化性能。使用 C#，情况会好一些，因为它对指针的支持<em>有限。</em>前面的 Go 示例可以用 C# 编写为：</p><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c#"><span class="hljs-comment">// C# 指针用法不安全的 void foo() &#123; </span><br>   Rect* ptr = &amp;rect.Min; <br>   *ptr = <span class="hljs-keyword">new</span> Point(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>); <br>&#125;<br></code></pre></td></tr></table></figure><p>然而，C# 指针支持带有一些不适用于 Go 的警告：</p><ol><li>使用点的代码必须标记为<strong>unsafe</strong>。这会创建安全性较低且更容易崩溃的代码。</li><li>在堆栈上分配的纯值类型（所有结构字段必须是值类型）。</li><li>在已关闭垃圾收集的<strong>固定范围内，使用 fixed 关键字。</strong></li></ol><p>因此，在 C# 中使用值类型的正常且安全的方法是复制它们，因为这不需要定义不安全或固定的代码区域。但是对于较大的值类型，这可能会产生性能问题。Go 没有这些问题。您可以在 Go 中创建指向垃圾收集器管理的对象的指针。您不需要像在 C# 中那样在 Go 中使用指针来隔离代码。</p><h2 id="自定义辅助分配器"><a href="#自定义辅助分配器" class="headerlink" title="自定义辅助分配器"></a>自定义辅助分配器</h2><p>使用正确的指针，您可以做很多只有值类型时无法做到的事情。一个示例是创建辅助分配器。<a href="https://github.com/ordovician/arena">这</a>是使用 Go 泛型创建的 Arena 分配器的示例。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">type</span> Arena[T any] <span class="hljs-keyword">struct</span> &#123;<br>    blocks Stack[*T]<br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(arena *Arena[T])</span></span> Alloc() *T &#123;<br>    <span class="hljs-keyword">if</span> arena.blocks.IsEmpty() &#123;<br>        <span class="hljs-keyword">var</span> blocks [<span class="hljs-number">32</span>]T     <span class="hljs-comment">// allocate 32 elements at a time</span><br>        <span class="hljs-keyword">for</span> i, _ := <span class="hljs-keyword">range</span> blocks &#123;<br>            arena.blocks.Push(&amp;blocks[i])<br>        &#125;<br>    &#125;<br>    b, _ := arena.blocks.Top()<br>    arena.blocks.Pop()<br>    <span class="hljs-keyword">return</span> b<br>&#125;<br></code></pre></td></tr></table></figure><p>为什么这些有用？如果您查看生成二叉树的算法的微基准测试，您通常会发现 Java 比 Go 具有很大优势。这是因为二叉树算法通常用于测试垃圾收集器分配对象的速度。Java 在这方面非常快，因为它使用了我们所说的凹凸指针。它只是增加一个指针，而 Go 将在内存中搜索合适的位置来分配对象。但是，使用 Arena 分配器，您也可以在 Go 中快速构建二叉树。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">import</span> <span class="hljs-string">&quot;golang.org/x/exp/constraints&quot;</span><br><span class="hljs-keyword">type</span> Tree[K constraints.Ordered, V any] <span class="hljs-keyword">struct</span> &#123;<br>    Root      *TreeNode[K, V]<br>    allocator Arena[TreeNode[K, V]]<br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(tree *Tree[K, V])</span></span> NewNode(key K, value V) *TreeNode[K, V] &#123;<br>    n := tree.allocator.Alloc()<br>    n.Key = key<br>    n.Value = value<br>    n.left = <span class="hljs-literal">nil</span><br>    n.right = <span class="hljs-literal">nil</span><br>    <span class="hljs-keyword">return</span> n<br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(tree *Tree[K, V])</span></span> Insert(key K, value V) &#123;<br>    n := tree.NewNode(key, value)<br>    <span class="hljs-keyword">if</span> tree.Root == <span class="hljs-literal">nil</span> &#123;<br>        tree.Root = n<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>        tree.Root.Insert(n)<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>这就是为什么拥有真正的指针有好处的原因。没有它，您无法在连续的内存块中创建指向元素的指针。在该<code>Alloc</code>方法中，我们创建了一个由 32 个元素组成的连续块。然后，我们将指向该块中每个元素的指针存储在一个堆栈上，该堆栈包含一个可用于分配的块列表。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-selector-tag">var</span> blocks <span class="hljs-selector-attr">[32]</span>T <br><span class="hljs-keyword">for</span> <span class="hljs-selector-tag">i</span>, _ := range blocks &#123; <br>    arena<span class="hljs-selector-class">.blocks</span><span class="hljs-selector-class">.Push</span>(&amp;blocks<span class="hljs-selector-attr">[i]</span>) <br>&#125;<br></code></pre></td></tr></table></figure><p>这只是可能的，因为我可以选择任意元素<code>blocks[i]</code>并获取指向该元素的指针<code>&amp;blocks[i]</code>。Java 没有给你这种可能性。</p><p>Java GC 使用的Bump分配器与 Arena 分配器类似，您只需增加一个指针即可获取下一个值。除非您不必自己构建它。这可能看起来更聪明。但这会导致 Go 中避免的几个问题：</p><ol><li>迟早您需要进行<em>压缩</em>，这涉及移动数据和修复指针。Arena 分配器不必这样做。</li><li>在多线程程序中，凹凸分配器需要锁（除非您使用线程本地存储）。这会扼杀它们的性能优势，因为锁会降低性能，或者线程本地存储会导致碎片，需要稍后进行压缩。</li></ol><p>Go 的创建者之一 Ian Lance Taylor<a href="https://groups.google.com/g/golang-nuts/c/KJiyv2mV2pU">阐明了Bump分配器的问题</a>：</p><blockquote><p>一般来说，使用一组每线程缓存分配内存可能会更有效，此时您已经失去了凹凸分配器的优势。所以我要断言，总的来说，有很多警告，今天为多线程程序使用压缩内存分配器并没有真正的优势。</p></blockquote><h2 id="逃逸分析"><a href="#逃逸分析" class="headerlink" title="逃逸分析"></a>逃逸分析</h2><p>Java 垃圾收集器还有很多工作要做，因为它分配了更多的对象。为什么？我们刚刚介绍了这一点。如果没有值对象和真正的指针，在分配大型数组或复杂数据结构时总是会以大量对象告终。因此它需要一个分代GC。</p><p>分配更少对象的需求对 Go 有利。但是 Go 还使用了另一个技巧。Go 和 Java在编译函数时都会进行所谓的<em>转义分析。</em></p><p>转义分析涉及查看在函数内部创建的指针并确定该指针是否曾经转义函数范围。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">escapingPtr</span><span class="hljs-params">()</span></span> []<span class="hljs-type">int</span> &#123; <br>   values := []<span class="hljs-type">int</span>&#123;<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>&#125;<br>   <span class="hljs-keyword">return</span> values<br>&#125; <br><br>fun nonEscapingPtr() <span class="hljs-type">int</span> &#123; <br>    values = []<span class="hljs-type">int</span>&#123;<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>&#125; <br>    <span class="hljs-keyword">var</span> total <span class="hljs-type">int</span> = addUp(values)<br>    <span class="hljs-keyword">return</span> total<br>&#125;<br></code></pre></td></tr></table></figure><p>在第一个示例中，<code>values</code>指向一个切片，它本质上与指向数组的指针相同。它逃脱，因为它被退回。这意味着<code>values</code>必须在堆上分配。</p><p>然而，在第二个例子中，没有指针<code>values</code>离开<code>nonEscapingPtr</code>函数。因此<code>values</code>可以在堆栈上分配，这非常快速且便宜。转义分析本身只是分析指针是否转义。</p><p><strong>Java Escape 分析的局限性</strong></p><p>Java 也确实逃脱了分析，但对其使用有更多限制。来自涵盖 HotSpot VM 的<a href="https://docs.oracle.com/en/java/javase/16/vm/java-hotspot-virtual-machine-performance-enhancements.html#GUID-6BD8FCB5-995B-4AE9-BFAA-B2C7DE2BA5CD">Java SE 16 Oracle 文档：</a></p><blockquote><p>它不会***将***堆分配替换为未全局转义的对象的堆栈分配。</p></blockquote><p>然而，Java 使用了一种称为<em>标量替换的替代技巧，</em>它避免了将对象放在堆栈上的需要。本质上它会爆炸和对象并将其原始成员放在堆栈上。请记住，Java 已经可以将原始值（例如<code>int</code>和<code>float</code>）放在堆栈上。<a href="https://pkolaczk.github.io/">然而，正如Piotr Kołaczkowski</a>在 2021 年发现的那样，在实践中，即使在非常微不足道的情况下，标量替换也不起作用。</p><p>相反，主要优点是避免锁定。如果您知道指针没有在函数外部使用，您还可以确定它不需要锁。</p><p><strong>Go Escape分析的优势</strong></p><p>然而，Go 使用逃逸分析来确定可以在堆栈上分配哪些对象。这显着减少了可以从分代 GC 中受益的短期对象的数量。请记住，分代 GC 的全部意义在于利用最近分配的对象存活时间短的事实。然而，Go 中的大多数对象可能会长期存在，因为短期对象很可能会被逃逸分析捕获。</p><p>与 Java 不同，这也适用于复杂对象。Java 通常只能成功地对字节数组等简单对象进行转义分析。即使是内置的<code>ByteBuffer</code>也不能使用标量替换在堆栈上分配。</p><h2 id="分代-GC-与并发-GC-暂停"><a href="#分代-GC-与并发-GC-暂停" class="headerlink" title="分代 GC 与并发 GC 暂停"></a>分代 GC 与并发 GC 暂停</h2><p>你可以读到很多关于垃圾收集器的专家声称，由于内存碎片，Go 比 Java 更有可能耗尽内存。争论是这样的：因为 Go 没有分代垃圾收集器，内存会随着时间的推移变得碎片化。当内存碎片化时，您将达到将新对象装入内存变得困难的地步。</p><p>但是，由于两个原因，此问题大大减少：</p><ol><li>Go 分配的小对象没有 Java 那么多。它可以将大型对象数组分配为单个内存块。</li><li>现代内存分配器，如 Google 的 TCMalloc 或 Intel 的 Scalable Malloc 不会对内存进行分段。</li></ol><p>在设计 Java 时，内存碎片是内存分配器的一个大问题。人们不认为它可以解决。但早在 1998 年，Java 出现后不久，研究人员就开始解决这个问题。<a href="https://dl.acm.org/doi/10.1145/286860.286864">这是 Mark S. Johnstone 和 Paul R. Wilson 的论文</a>：</p><blockquote><p>这大大加强了我们之前的结果，即内存碎片问题通常被误解，并且好的分配器策略可以为大多数程序提供良好的内存使用。</p></blockquote><p>因此，为 Java 设计内存分配策略的许多假设根本不再适用</p><p>使用分代 GC 的 Java 策略旨在缩短垃圾收集周期。请记住，Java 必须停止一切来移动数据并修复指针。如果持续时间过长，这会降低性能和响应能力。使用分代 GC，每次缩短此时间时要检查的数据更少。</p><p>然而，Go 用多种替代策略解决了同样的问题：</p><ol><li>因为不需要移动内存，也不需要固定指针，所以在 GC 运行期间要做的工作更少。Go GC 只进行标记和扫描：它通过对象图查找应该释放的对象。</li><li>它同时运行。因此，一个单独的 GC 线程可以在不停止其他线程的情况下寻找要释放的对象。</li></ol><p>为什么 Go 可以同时运行它的 GC 而不是 Java？因为 Go 不会修复任何指针或移动内存中的任何对象。因此，不存在尝试访问指向刚刚移动但该指针尚未更新的对象的指针的风险。由于某些并发线程正在运行，不再有任何引用的对象不会突然获得引用。因此，并行移除死对象是没有危险的。</p><p>这是怎么回事？假设你有 4 个线程在 Go 程序中工作。其中一个线程偶尔会在任意时间段<code>T</code>秒内完成总共 4 秒的 GC 工作。</p><p>现在想象一个带有 GC 的 Java 程序执行 GC 工作仅 2 秒。哪个程序挤出最多的性能？谁在<code>T</code>几秒钟内完成最多？听起来像 Java 程序，对吧？错误的！</p><p>Java 程序中的 4 个工作线程将所有工作停止 2 秒。<code>T</code>这意味着 2×4 &#x3D; 8 秒的工作在间隔中丢失。因此，虽然 Go 停止的时间更长，但每次停止都会影响更少的工作，因为所有线程都没有停止。因此，缓慢的并发 GC 可能会胜过依赖于停止所有线程来完成其工作的更快的 GC。</p><p><strong>如果垃圾的创建速度比 Go 清理它的速度快怎么办？</strong></p><p>反对当前垃圾收集器的一个流行论点是，您可能会遇到一种情况，即活动工作线程产生垃圾的速度比垃圾收集器线程收集垃圾的速度要快。在 Java 世界中，这被称为“并发模式故障”。</p><p>声称在这种情况下，运行时别无选择，只能完全停止您的程序并等待 GC 周期完成。因此，当 Go 声称 GC 暂停非常低时，这种说法仅适用于 GC 有足够的 CPU 时间和余量超过主程序的情况。</p><p>但是 Go 有一个巧妙的技巧来解决<a href="https://blog.golang.org/ismmkeynote">Go GC 大师 Rick Hudson 所描述的</a>这个问题。Go 使用所谓的 Pacer。</p><blockquote><p>如果需要，Pacer 会在加快标记速度的同时减慢分配速度。在高层次上，Pacer 会停止执行大量分配的 Goroutine，并将其投入到标记工作中。工作量与 Goroutine 的分配成正比。这加快了垃圾收集器的速度，同时减慢了 mutator 的速度。</p></blockquote><p>Goroutines 有点像在线程池上多路复用的绿色线程。基本上，Go 接管了正在运行产生大量垃圾的工作负载的线程，并将它们用于帮助 GC 清理这些垃圾。它只会继续接管线程，直到 GC 运行得比产生垃圾的例程快。</p><p><strong>简而言之</strong></p><p>虽然高级垃圾收集器解决了 Java 中的实际问题，但 Go 和 Julia 等现代语言一开始就简单地避免了产生这些问题，因此不再需要劳斯莱斯垃圾收集器。当您拥有值类型、转义分析、指针、多核处理器和现代分配器时，Java 设计背后的许多假设都将不复存在。它们不再适用。</p><h2 id="假定的-GC-权衡不再适用"><a href="#假定的-GC-权衡不再适用" class="headerlink" title="假定的 GC 权衡不再适用"></a>假定的 GC 权衡不再适用</h2><p>Mike Hearn 在 Medium 上有一个非常受欢迎的故事，他批评了有关 Go GC 的说法：<a href="https://blog.plan99.net/modern-garbage-collection-911ef4f8bd8e">现代垃圾收集。</a>.</p><p>Hearn 的关键信息是在 GC 设计中总是存在权衡。他提出的观点是，因为 Go 的目标是低延迟收集，所以它们会受到许多其他指标的影响。这是一本有趣的读物，因为它涵盖了很多关于 GC 设计权衡的细节。</p><p>首先，我所说的低延迟是什么意思？与可能花费数百毫秒的各种 Java 收集器相比，Go GC 平均仅暂停大约 0.5 毫秒。</p><p>我从 Mike Hearn 的论点中看到的问题是，它们基于一个有缺陷的前提，即所有语言的内存访问模式都是相同的。正如我在本文中介绍的那样，这根本不是真的。Go 将产生更少的对象来由 GC 管理，并且它会使用逃逸分析及早清理大量对象。</p><p><strong>旧技术天生就不好？</strong></p><p>赫恩提出的论点表明，简单的收集在某种程度上天生就不好：</p><blockquote><p>Stop-the-world (STW) 标记&#x2F;扫描是本科计算机科学课程中最常教授的 GC 算法。在进行工作面试时，我有时会要求应聘者谈谈 GC，而且几乎总是，他们要么将 GC 视为一个黑匣子，对此一无所知，要么认为它现在仍在使用这种非常古老的技术。</p></blockquote><p>是的，它可能已经过时了，但是这种技术允许您同时运行 GC，这是“现代”技术所不允许的。在我们拥有多核的现代硬件世界中，这一点更为重要。</p><p><strong>Go 不是 C#</strong></p><p>另一种说法：</p><blockquote><p>由于 Go 是一种具有值类型的相对普通的命令式语言，它的内存访问模式可能与 C# 相当，其中分代假设肯定成立，因此 .NET 使用分代收集器。</p></blockquote><p>情况并非如此。AC# 开发人员会尽量减少对较大值对象的使用，因为与指针相关的代码无法安全使用。我们必须假设 C# 开发人员更喜欢复制值类型而不是使用指针，因为这可以在 CLR 中安全地完成。这自然会带来更高的开销。</p><p>据我所知，C# 也没有利用逃逸分析来减少堆上短期对象的产生。其次，<a href="https://alexyakunin.medium.com/go-vs-c-part-1-goroutines-vs-async-await-ac909c651c11">C# 并不擅长同时运行大量任务</a>。正如 Pacer 所提到的，Go 可以利用它们的协程来加速并发收集。</p><h2 id="为什么低延迟对-Java-也很重要"><a href="#为什么低延迟对-Java-也很重要" class="headerlink" title="为什么低延迟对 Java 也很重要"></a>为什么低延迟对 Java 也很重要</h2><p>我们生活在一个充满 docker 容器和微服务的世界中。这意味着许多较小的程序相互通信并为彼此工作。想象一下工作需要通过几个服务。每当一条链中的这些服务中的一项出现重大暂停时，就会产生涟漪效应。它会导致所有其他进程停止工作。如果管道中的下一个服务正在等待一个忙于进行垃圾收集的服务，它就无法工作。</p><p>因此，延迟&#x2F;吞吐量的权衡不再是 GC 设计中的权衡。当多个服务一起工作时，高延迟会导致吞吐量下降。Java 对高吞吐量和高延迟 GC 的偏好适用于单体应用程序世界。它不再适用于微服务世界。</p><p>这是 Mike Hearn 的论点的一个基本问题，即没有灵丹妙药，只有权衡取舍。它试图给人的印象是 Java 的权衡是同样有效的。但权衡必须适合我们生活的世界。</p><p>简而言之，我认为可以说围棋做出了许多明智的举动和战略选择。挥舞它，好像它只是任何人都可以做出的权衡一样，并没有削减它</p>]]></content>
    
    
    <categories>
      
      <category>golang vs java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>golang</tag>
      
      <tag>java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes网络模型进阶</title>
    <link href="/2022/09/10/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E8%BF%9B%E9%98%B6/"/>
    <url>/2022/09/10/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E8%BF%9B%E9%98%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="Kubernetes-网络模型进阶"><a href="#Kubernetes-网络模型进阶" class="headerlink" title="Kubernetes 网络模型进阶"></a>Kubernetes 网络模型进阶</h1><h2 id="Underlay-Network-Model"><a href="#Underlay-Network-Model" class="headerlink" title="Underlay Network Model"></a>Underlay Network Model</h2><h3 id="什么是Underlay-Network"><a href="#什么是Underlay-Network" class="headerlink" title="什么是Underlay Network"></a>什么是Underlay Network</h3><p>底层网络 <em>Underlay Network</em> 顾名思义是指网络设备基础设施，如交换机，路由器, <em>DWDM</em> 使用网络介质将其链接成的物理网络拓扑，负责网络之间的数据包传输。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/D727NicjCjMOfSoHgRPZfL1ZzWoqxIyxyxjoDlKBvyPVoThkJ42pKhe4t3iaE0U1VgCcn0jybn1TCx6yicMB1efjA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>​                                                                                  <strong>图：Underlay network topology</strong></p><p><em>Source：</em><a href="https://community.cisco.com/t5/data-center-switches/understanding-underlay-and-overlay-networks/td-p/4295870">https://community.cisco.com/t5/data-center-switches/understanding-underlay-and-overlay-networks/td-p/4295870</a></p><p><em>underlay network</em> 可以是二层，也可以是三层；二层 <em>underlay network</em> 的典型例子是以太网 <em>Ethernet</em>，三层是 <em>underlay network</em> 的典型例子是互联网 <em>Internet</em>。</p><p>而工作与二层的技术是 <em>vlan</em>，工作在三层的技术是由 <em>OSPF</em>, <em>BGP</em> 等协议组成</p><h3 id="kubernetes中的underlay-network"><a href="#kubernetes中的underlay-network" class="headerlink" title="kubernetes中的underlay network"></a>kubernetes中的underlay network</h3><p>在kubernetes中，<em>underlay network</em> 中比较典型的例子是通过将宿主机作为路由器设备，Pod 的网络则通过学习成路由条目从而实现跨节点通讯。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/D727NicjCjMOfSoHgRPZfL1ZzWoqxIyxyqjHDK7dv8WtgBpibmbozUC7wo5zWdEDlIkoK03vxpv5QoR7ciaVNKzTw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>​                                                                                  <strong>图：underlay network topology in kubernetes</strong></p><p>这种模型下典型的有 <em>flannel</em> 的 <em>host-gw</em> 模式与 <em>calico</em> <em>BGP</em> 模式。</p><h4 id="flannel-host-gw-1"><a href="#flannel-host-gw-1" class="headerlink" title="flannel host-gw [1]"></a>flannel host-gw [1]</h4><p><em>flannel host-gw</em> 模式中每个Node需要在同一个二层网络中，并将Node作为一个路由器，跨节点通讯将通过路由表方式进行，这样方式下将网络模拟成一个<em>underlay network</em>。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/D727NicjCjMOfSoHgRPZfL1ZzWoqxIyxyDLPDv7ZTCW14W1wtUs1SbR9yOcibm3ncwCd6dJV7C076XoSovLehaZA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>​                                                                                         <strong>图：layer2 ethernet topology</strong></p><p><em>Source：</em><a href="https://www.auvik.com/franklyit/blog/layer-3-switches-layer-2/">https://www.auvik.com/franklyit/blog/layer-3-switches-layer-2/</a></p><blockquote><p>Notes：因为是通过路由方式，集群的cidr至少要配置16，因为这样可以保证，跨节点的Node作为一层网络，同节点的Pod作为一个网络。如果不是这种用情况，路由表处于相同的网络中，会存在网络不可达</p></blockquote><h4 id="Calico-BGP-2"><a href="#Calico-BGP-2" class="headerlink" title="Calico BGP [2]"></a>Calico BGP [2]</h4><p>BGP（<em>Border Gateway Protocol</em>）是去中心化自治路由协议。它是通过维护IP路由表或’前缀’表来实现AS （<em>Autonomous System</em>）之间的可访问性，属于向量路由协议。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/D727NicjCjMOfSoHgRPZfL1ZzWoqxIyxyGwaHm71fY1JAaOyWcdvfX0gjcO0e61aB56kskOgdjsLSmiayBoaLeQg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>​                                                                                                  <strong>图：BGP network topology</strong></p><p><em>Source：</em><a href="https://infocenter.nokia.com/public/7705SAR214R1A/index.jsp?topic=/com.sar.routing_protocols%25">https://infocenter.nokia.com/public/7705SAR214R1A/index.jsp?topic=%2Fcom.sar.routing_protocols%</a></p><p>与 <em>flannel</em> 不同的是，<em>Calico</em> 提供了的 <em>BGP</em> 网络解决方案，在网络模型上，<em>Calico</em> 与 <em>Flannel host-gw</em> 是近似的，但在软件架构的实现上，<em>flannel</em> 使用 <em>flanneld</em> 进程来维护路由信息；而 <em>Calico</em> 是包含多个守护进程的，其中 <em>Brid</em> 进程是一个 <em>BGP</em> 的客户端 与路由反射器(<em>Router Reflector</em>)，<em>BGP</em> 客户端负责从 <em>Felix</em> 中获取路由并分发到其他 <em>BGP Peer</em>，而反射器在BGP中起了优化的作用。在同一个IBGP中，BGP客户端仅需要和一个 <em>RR</em> 相连，这样减少了<em>AS</em>内部维护的大量的BGP连接。通常情况下，<em>RR</em> 是真实的路由设备，而 <em>Bird</em> 作为 <em>BGP</em> 客户端工作。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/D727NicjCjMOfSoHgRPZfL1ZzWoqxIyxyfEXWkGJTBS8TSt3pksPGe18LTicxXWDvCnTZXPibJrJ5Og5oE1wOHibrQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>​                                                                                                       <strong>图：Calico Network Architecture</strong></p><p><em>Source：</em><a href="https://www.cisco.com/c/en/us/td/docs/dcn/whitepapers/cisco-nx-os-calico-network-design.html">https://www.cisco.com/c/en/us/td/docs/dcn/whitepapers/cisco-nx-os-calico-network-design.html</a></p><h4 id="IPVLAN-amp-MACVLAN-4"><a href="#IPVLAN-amp-MACVLAN-4" class="headerlink" title="IPVLAN &amp; MACVLAN [4]"></a>IPVLAN &amp; MACVLAN [4]</h4><p><em>IPVLAN</em> 和 <em>MACVLAN</em> 是一种网卡虚拟化技术，两者之间的区别为， <em>IPVLAN</em> 允许一个物理网卡拥有多个IP地址，并且所有的虚拟接口用同一个MAC地址；而 <em>MACVLAN</em> 则是相反的，其允许同一个网卡拥有多个MAC地址，而虚拟出的网卡可以没有IP地址。</p><p>因为是网卡虚拟化技术，而不是网络虚拟化技术，本质上来说属于 <em>Overlay network</em>，这种方式在虚拟化环境中与<em>Overlay network</em> 相比最大的特点就是可以将Pod的网络拉平到Node网络同级，从而提供更高的性能、低延迟的网络接口。本质上来说其网络模型属于下图中第二个。</p><ul><li>虚拟网桥：创建一个虚拟网卡对(veth pair)，一头栽容器内，一头栽宿主机的root namespaces内。这样一来容器内发出的数据包可以通过网桥直接进入宿主机网络栈，而发往容器的数据包也可以经过网桥进入容器。</li><li>多路复用：使用一个中间网络设备，暴露多个虚拟网卡接口，容器网卡都可以介入这个中间设备，并通过MAC&#x2F;IP地址来区分packet应该发往哪个容器设备。</li><li>硬件交换，为每个Pod分配一个虚拟网卡，这样一来，Pod与Pod之间的连接关系就会变得非常清晰，因为近乎物理机之间的通信基础。如今大多数网卡都支持SR-IOV功能，该功能将单一的物理网卡虚拟成多个VF接口，每个VF接口都有单独的虚拟PCIe通道，这些虚拟的PCIe通道共用物理网卡的PCIe通道。</li></ul><p><img src="https://mmbiz.qpic.cn/mmbiz_png/D727NicjCjMOfSoHgRPZfL1ZzWoqxIyxyXl7hJSbrKicdnPZLbNhads4BrWPwDS78RZJIAtodUb5v8m0mWtkmiaDQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>​                                                                           <strong>图：Virtual networking modes: bridging, multiplexing and SR-IOV</strong></p><p><em>Source：</em><a href="https://thenewstack.io/hackers-guide-kubernetes-networking/">https://thenewstack.io/hackers-guide-kubernetes-networking/</a></p><p>在kubernetes中 <em>IPVLAN</em> 这种网络模型下典型的CNI有，multus 与 danm。</p><h5 id="multus"><a href="#multus" class="headerlink" title="multus"></a>multus</h5><p><em>multus</em> 是 intel 开源的CNI方案，是由传统的 <em>cni</em> 与 <em>multus</em> 组成，并且提供了 SR-IOV CNI 插件使 K8s pod 能够连接到 SR-IOV VF 。这是使用了 <em>IPVLAN&#x2F;MACVLAN</em> 的功能。</p><p>当创建新的Pod后，SR-IOV 插件开始工作。配置 VF 将被移动到新的 CNI 名称空间。该插件根据 CNI 配置文件中的 “name” 选项设置接口名称。最后将VF状态设置为UP。</p><p>下图是一个 Multus 和 SR-IOV CNI 插件的网络环境，具有三个接口的 pod。</p><ul><li><em>eth0</em> 是 <em>flannel</em> 网络插件，也是作为Pod的默认网络</li><li>VF 是主机的物理端口 <em>ens2f0</em> 的实例化。这是英特尔X710-DA4上的一个端口。在Pod端的 VF 接口名称为 <em>south0</em> 。</li><li>这个VF使用了 DPDK 驱动程序，此 VF 是从主机的物理端口 <em>ens2f1</em> 实例化出的。这个是英特尔® X710-DA4上另外一个端口。Pod 内的 VF 接口名称为 <em>north0</em>。该接口绑定到 DPDK 驱动程序 <em>vfio-pci</em> 。</li></ul><p><img src="https://mmbiz.qpic.cn/mmbiz_png/D727NicjCjMOfSoHgRPZfL1ZzWoqxIyxy1fqm6eakBu9XZT59bervsUvIFp2pF4fteTOULSaV24NIaSTFaTCuYA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>​                                                                      <strong>图：Mutus networking Architecture overlay and SR-IOV</strong></p><p><em>Source：</em><a href="https://builders.intel.com/docs/networkbuilders/enabling_new_features_in_kubernetes_for_NFV.pdf">https://builders.intel.com/docs/networkbuilders/enabling_new_features_in_kubernetes_for_NFV.pdf</a></p><blockquote><p>Notes：terminology</p><ul><li>NIC：network interface card，网卡</li><li>SR-IOV：single root I&#x2F;O virtualization，硬件实现的功能，允许各虚拟机间共享PCIe设备。</li><li>VF：Virtual Function，基于PF，与PF或者其他VF共享一个物理资源。</li><li>PF：PCIe Physical Function，拥有完全控制PCIe资源的能力</li><li>DPDK：Data Plane Development Kit</li></ul></blockquote><p>于此同时，也可以将主机接口直接移动到Pod的网络名称空间，当然这个接口是必须存在，并且不能是与默认网络使用同一个接口。这种情况下，在普通网卡的环境中，就直接将Pod网络与Node网络处于同一个平面内了。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/D727NicjCjMOfSoHgRPZfL1ZzWoqxIyxyBpQH5b5UoWxWhM0YKPIVBW25oZLowbR6BuCu8ZGf0zXeiatS7zxudTw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>​                                                                         <strong>图：Mutus networking Architecture overlay and ipvlan</strong></p><p><em>Source：</em><a href="https://devopstales.github.io/kubernetes/multus/">https://devopstales.github.io/kubernetes/multus/</a></p><h5 id="danm"><a href="#danm" class="headerlink" title="danm"></a>danm</h5><p>DANM是诺基亚开源的CNI项目，目的是将电信级网络引入kubernetes中，与multus相同的是，也提供了SR-IOV&#x2F;DPDK 的硬件技术，并且支持IPVLAN.</p><h2 id="Overlay-Network-Model"><a href="#Overlay-Network-Model" class="headerlink" title="Overlay Network Model"></a>Overlay Network Model</h2><h3 id="什么是Overlay"><a href="#什么是Overlay" class="headerlink" title="什么是Overlay"></a>什么是Overlay</h3><p>叠加网络是使用网络虚拟化技术，在 <em>underlay</em> 网络上构建出的虚拟逻辑网络，而无需对物理网络架构进行更改。本质上来说，<em>overlay network</em> 使用的是一种或多种隧道协议 (<em>tunneling</em>)，通过将数据包封装，实现一个网络到另一个网络中的传输，具体来说隧道协议关注的是数据包（帧）。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/D727NicjCjMOfSoHgRPZfL1ZzWoqxIyxyyGkhSJE7WhbUna4s0mvzghkvGDCgPsPNtmibTUAtIWYfCRLdkGSasGQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>图：overlay network topology</p><p><em>Source：</em><a href="https://www.researchgate.net/figure/Example-Overlay-Network-built-on-top-of-an-Internet-style-Underlay_fig4_230774628">https://www.researchgate.net/figure/Example-Overlay-Network-built-on-top-of-an-Internet-style-Underlay_fig4_230774628</a></p><h3 id="常见的网络隧道技术"><a href="#常见的网络隧道技术" class="headerlink" title="常见的网络隧道技术"></a>常见的网络隧道技术</h3><ul><li>通用路由封装 ( <em>Generic Routing Encapsulation</em> ) 用于将来自 IPv4&#x2F;IPv6的数据包封装为另一个协议的数据包中，通常工作与L3网络层中。</li><li>VxLAN (<em>Virtual Extensible LAN</em>)，是一个简单的隧道协议，本质上是将L2的以太网帧封装为L4中UDP数据包的方法，使用 4789 作为默认端口。<em>VxLAN</em> 也是 <em>VLAN</em> 的扩展对于 4096（212 位 <em>VLAN ID</em>） 扩展为1600万（224 位 <em>VNID</em> ）个逻辑网络。</li></ul><p>这种工作在 <em>overlay</em> 模型下典型的有 <em>flannel</em> 与 <em>calico</em> 中的的 <em>VxLAN</em>, <em>IPIP</em> 模式。</p><h3 id="IPIP"><a href="#IPIP" class="headerlink" title="IPIP"></a>IPIP</h3><p><em>IP in IP</em> 也是一种隧道协议，与 <em>VxLAN</em> 类似的是，<em>IPIP</em> 的实现也是通过Linux内核功能进行的封装。<em>IPIP</em> 需要内核模块 <code>ipip.ko</code> 使用命令查看内核是否加载IPIP模块<code>lsmod | grep ipip</code> ；使用命令<code>modprobe ipip</code> 加载。</p><p><img src="https://cdn.jsdelivr.net/gh/longpi1/blog-img/640" alt="图片"></p><p>图：A simple IPIP network workflow</p><p><em>Source：</em><a href="https://ssup2.github.io/theory_analysis/IPIP_GRE_Tunneling/">https://ssup2.github.io/theory_analysis/IPIP_GRE_Tunneling/</a></p><p>Kubernetes中 <em>IPIP</em> 与 <em>VxLAN</em> 类似，也是通过网络隧道技术实现的。与 <em>VxLAN</em> 差别就是，<em>VxLAN</em> 本质上是一个 UDP包，而 <em>IPIP</em> 则是将包封装在本身的报文包上。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/D727NicjCjMOfSoHgRPZfL1ZzWoqxIyxybmxBiantAKWVD2nlCUNBHAIQkHSOJZcQeM4znchxeRqicRNvh3pcPIMw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>​                                                                                                         <strong>图：IPIP in kubernetes</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/D727NicjCjMOfSoHgRPZfL1ZzWoqxIyxyNfkHJrC9xJw6SzZRFND1XdRacXJ6A7utD0RYvRyj7qOwJPzM60YiaeA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>​                                                                                         <strong>图：IPIP packet with wireshark unpack</strong></p><blockquote><p>Notes：公有云可能不允许IPIP流量，例如Azure</p></blockquote><h3 id="VxLAN"><a href="#VxLAN" class="headerlink" title="VxLAN"></a>VxLAN</h3><p>kubernetes中不管是 <em>flannel</em> 还是 <em>calico</em> VxLAN的实现都是使用Linux内核功能进行的封装，Linux 对 vxlan 协议的支持时间并不久，2012 年 Stephen Hemminger 才把相关的工作合并到 kernel 中，并最终出现在 kernel 3.7.0 版本。为了稳定性和很多的功能，你可以会看到某些软件推荐在 3.9.0 或者 3.10.0 以后版本的 kernel 上使用 <em>VxLAN</em>。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/D727NicjCjMOfSoHgRPZfL1ZzWoqxIyxyVuvdFPpJgI1QG5U2MHUib3DbBGia1HVB6sicRiadIptJxM0B7nUXaCSqYQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>​                                                                                             <strong>图：A simple VxLAN network topology</strong></p><p>在kubernetes中vxlan网络，例如 <em>flannel</em>，守护进程会根据kubernetes的Node而维护 <em>VxLAN</em>，名称为 <code>flannel.1</code> 这是 <em>VNID</em>，并维护这个网络的路由，当发生跨节点的流量时，本地会维护对端 <em>VxLAN</em> 设备的MAC地址，通过这个地址可以知道发送的目的端，这样就可以封包发送到对端，收到包的对端 VxLAN设备 <code>flannel.1</code> 解包后得到真实的目的地址。</p><p>查看 <em>Forwarding database</em> 列表</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-variable">$ </span>bridge fdb <span class="hljs-number">26</span><span class="hljs-symbol">:</span>5<span class="hljs-symbol">e:</span><span class="hljs-number">87</span><span class="hljs-symbol">:</span><span class="hljs-number">90</span><span class="hljs-symbol">:</span><span class="hljs-number">91</span><span class="hljs-symbol">:fc</span> dev flannel.<span class="hljs-number">1</span> dst <span class="hljs-number">10.0</span>.<span class="hljs-number">0.3</span> <span class="hljs-variable language_">self</span> permanent<br></code></pre></td></tr></table></figure><p><img src="https://mmbiz.qpic.cn/mmbiz_png/D727NicjCjMOfSoHgRPZfL1ZzWoqxIyxyib50Tia4cxibibR5uhmL4eO4m158hQFxZsiaWaqYE9vH2Fflee6aEaEACJg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>​                                                                                                      <strong>图：VxLAN in kubernetes</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/D727NicjCjMOfSoHgRPZfL1ZzWoqxIyxyCSReVTSz26R2z2ibGa2HvNuTjwKI8tQHHv14amJr1eoOTw05gpMc5mg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>​                                                                                       <strong>图：VxLAN packet with wireshark unpack</strong></p><blockquote><p>Notes：VxLAN使用的4789端口，wireshark应该是根据端口进行分析协议的，而flannel在linux中默认端口是8472，此时抓包仅能看到是一个UDP包。</p></blockquote><p>通过上述的架构可以看出，隧道实际上是一个抽象的概念，并不是建立的真实的两端的隧道，而是通过将数据包封装成另一个数据包，通过物理设备传输后，经由相同的设备（网络隧道）进行解包实现网络的叠加。</p><h3 id="weave-vxlan-3"><a href="#weave-vxlan-3" class="headerlink" title="weave vxlan [3]"></a>weave vxlan [3]</h3><p>weave也是使用了 <em>VxLAN</em> 技术完成的包的封装，这个技术在 <em>weave</em> 中称之为 *fastdp (fast data path)*，与 <em>calico</em> 和 <em>flannel</em> 中用到的技术不同的，这里使用的是 Linux 内核中的 <em>openvswitch datapath module</em>，并且weave对网络流量进行了加密。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/D727NicjCjMOfSoHgRPZfL1ZzWoqxIyxy1r4xqNRVbh6Ua8kaalhWPbicCYYI0CcbC3tLeuoMGHxLX6zLqmEOiawA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>​                                                                                           <strong>图：weave fastdp network topology</strong></p><p><em>Source：</em><a href="https://www.weave.works/docs/net/latest/concepts/fastdp-how-it-works/">https://www.weave.works/docs/net/latest/concepts/fastdp-how-it-works/</a></p><blockquote><p>Notes：fastdp工作在Linux 内核版本 3.12 及更高版本，如果低于此版本的例如CentOS7，weave将工作在用户空间，weave中称之为 <em>sleeve mode</em></p></blockquote><p>Reference</p><p>[1] flannel host-gw</p><p>[2] calico bgp networking</p><p>[3] calico bgp networking</p><p>[4] sriov network</p><p>[5] danm</p><p>作者：Cylon</p><p>出处：<a href="https://www.cnblogs.com/Cylon/p/16595820.html">https://www.cnblogs.com/Cylon/p/16595820.html</a></p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
      <tag>转载</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Golang对比Java、python为什么要保留指针</title>
    <link href="/2022/09/05/Golang%E5%AF%B9%E6%AF%94Java%E3%80%81python%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BF%9D%E7%95%99%E6%8C%87%E9%92%88/"/>
    <url>/2022/09/05/Golang%E5%AF%B9%E6%AF%94Java%E3%80%81python%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BF%9D%E7%95%99%E6%8C%87%E9%92%88/</url>
    
    <content type="html"><![CDATA[<h1 id="Golang对比Java、python为什么要保留指针"><a href="#Golang对比Java、python为什么要保留指针" class="headerlink" title="Golang对比Java、python为什么要保留指针"></a>Golang对比Java、python为什么要保留指针</h1><h2 id="为什么要用指针？"><a href="#为什么要用指针？" class="headerlink" title="为什么要用指针？"></a>为什么要用指针？</h2><p>平时我们在Golang使用指针一般是为了以下的情况：</p><ul><li><strong>方法直接修改原来对象</strong></li><li><strong>保证参数传递的自由，可以在传递重量级对象时使用指针</strong></li></ul><p>但Go 保留指针不仅仅是为了解决传递参数的问题，还跟它的语言特性有密不可分的联系。</p><h2 id="值语义"><a href="#值语义" class="headerlink" title="值语义"></a>值语义</h2><p>Go 里面的变量是<strong>值语义</strong>，这个跟 C&#x2F;C++是一脉相承的。比如一个结构体变量赋值给另外一个变量就是一次内存拷贝，而不是只拷贝一个指针，因此需要指针来表达引用语义，关于拷贝的具体实现可以了解<a href="https://gfw.go101.org/article/value-part.html">直接值部与间接值部的实现</a>。</p><p>关于值语义(value semantics)：<strong>值语义</strong>指的是对象的拷贝与原对象无关，就像拷贝 int 一样。C++ 的内置类型(bool&#x2F;int&#x2F;double&#x2F;char)都是值语义，标准库里的 complex&lt;&gt; 、pair&lt;&gt;、vector&lt;&gt;、map&lt;&gt;、string 等等类型也都是值语意，拷贝之后就与原对象脱离关系。同样，Java 语言的 primitive types 也是值语义。</p><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><p><strong>复杂的高级类型占用的内存往往相对较大，存储在 <a href="https://www.zhihu.com/search?q=heap&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:1665421830%7D">heap</a> 中，GC 回收频率相对较低，代价也较大，因此传引用&#x2F;指针可以避免进行成本较高的复制操作，并且节省内存，提高程序运行效率。</strong></p><p>为什么要保留值语义，而不是像 Java 或者 Python 一样让复合类型默认都是指针类型呢？因为值语义带来了如下好处：</p><ul><li><strong><a href="https://www.zhihu.com/search?q=%E7%BB%93%E6%9E%84%E4%BD%93&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2242103027%7D">结构体</a>可以直接用来比较相等，而非比较指针，Java 里面的 &#x3D;&#x3D; 操作符除了基本类型有用，其他类型几乎没用。</strong></li><li><strong>与 C 语言更好地交互。Go 可以通过 cgo 与 C 语言无缝交互。Go 里面的结构体基本上不用特殊处理就能传递给 C 的函数使用。主要得益于 Go 的结构体和 C 的一样都是值类型。</strong></li><li><strong>开发者能更好的掌控内存布局。一个结构体数组就是一段连续内存，而不是一个指针数组。</strong></li><li><strong>减轻 GC 压力。紧凑的内存布局减少了 GC 对象的个数，比如一个100w 长度的结构体数组就是一个 GC 对象，而不是100w 个。</strong></li><li><strong>减轻堆内存的分配压力。函数通过传值的方式传递参数后，原变量不会发生逃逸，可以被分配在栈上</strong></li></ul><p>Go 为了内存安全，虽然有指针，但不支持指针算数，但结合 unsafe.Pointer 也可以完成一些非常规情景下的精细内存操作。比如结合 <a href="https://www.zhihu.com/search?q=mmap&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2242103027%7D">mmap</a> 实现堆外内存管理，runtime 里面的内存管理就是这么来的，完全不用另外用 C 语言来实现。 这也是可以使用 Go 语言来写操作系统（<a href="https://link.zhihu.com/?target=https://github.com/icexin/eggos">eggos</a>）的原因。</p><h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><p><strong>Go 的指针一方面提供了引用语义，另一方面像 C 语言一样给了开发者灵活管理内存的能力。</strong></p><p>参考链接：樊冰心：<a href="https://www.zhihu.com/question/399589293/answer/2242103027">https://www.zhihu.com/question/399589293/answer/2242103027</a></p>]]></content>
    
    
    <categories>
      
      <category>golang vs java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>golang</tag>
      
      <tag>java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>编程范式之泛型编程</title>
    <link href="/2022/09/04/%E7%BC%96%E7%A8%8B%E8%8C%83%E5%BC%8F%E4%B9%8B%E6%B3%9B%E5%9E%8B%E7%BC%96%E7%A8%8B/"/>
    <url>/2022/09/04/%E7%BC%96%E7%A8%8B%E8%8C%83%E5%BC%8F%E4%B9%8B%E6%B3%9B%E5%9E%8B%E7%BC%96%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="编程范式之泛型编程"><a href="#编程范式之泛型编程" class="headerlink" title="编程范式之泛型编程"></a>编程范式之泛型编程</h1><h3 id="C-语言的泛型"><a href="#C-语言的泛型" class="headerlink" title="C 语言的泛型"></a>C 语言的泛型</h3><h3 id="一个泛型的示例-swap-函数"><a href="#一个泛型的示例-swap-函数" class="headerlink" title="一个泛型的示例 - swap 函数"></a>一个泛型的示例 - swap 函数</h3><p>好了，我们再看下，C 语言是如何泛型的。C 语言的类型泛型基本上来说就是使用<code>void *</code>关键字或是使用宏定义。</p><p>下面是一个使用了<code>void*</code>泛型版本的 swap 函数。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs C"><span class="hljs-type">void</span> <span class="hljs-title function_">swap</span><span class="hljs-params">(<span class="hljs-type">void</span>* x, <span class="hljs-type">void</span>* y, <span class="hljs-type">size_t</span> size)</span><br>&#123;<br>     <span class="hljs-type">char</span> tmp[size];<br>     <span class="hljs-built_in">memcpy</span>(tmp, y, size);<br>     <span class="hljs-built_in">memcpy</span>(y, x, size);<br>     <span class="hljs-built_in">memcpy</span>(x, tmp, size);<br>&#125;<br></code></pre></td></tr></table></figure><p>上面这个函数几乎完全改变了 int 版的函数的实现方式，这个实现方式有三个重点：</p><ul><li><strong>函数接口中增加了一个<code>size</code>参数</strong>。为什么要这么干呢？因为，用了 <code>void*</code> 后，类型被“抽象”掉了，编译器不能通过类型得到类型的尺寸了，所以，需要我们手动地加上一个类型长度的标识。</li><li><strong>函数的实现中使用了<code>memcpy()</code>函数</strong>。为什么要这样干呢？还是因为类型被“抽象”掉了，所以不能用赋值表达式了，很有可能传进来的参数类型还是一个结构体，因此，为了要交换这些复杂类型的值，我们只能使用内存复制的方法了。</li><li><strong>函数的实现中使用了一个<code>temp[size]</code>数组</strong>。这就是交换数据时需要用的 buffer，用 buffer 来做临时的空间存储。</li></ul><p>于是，新增的<code>size</code>参数，使用的<code>memcpy</code>内存拷贝以及一个 buffer，这增加了编程的复杂度。这就是 C 语言的类型抽象所带来的复杂度的提升。</p><p>在提升复杂度的同时，我们发现还有问题，比如，我们想交换两个字符串数组，类型是：<code>char*</code>，那么，我的<code>swap()</code>函数的<code>x</code>和<code>y</code>参数是不是要用<code>void**</code>了？这样一来，接口就没法定义了。</p><p>除了使用 <code>void*</code> 来做泛型，在 C 语言中，还可以用宏定义来做泛型，如下所示：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">define</span> swap(x, y, size) &#123;\</span><br><span class="hljs-meta">char temp[size]; \</span><br><span class="hljs-meta">memcpy(temp, &amp;y, size); \</span><br><span class="hljs-meta">memcpy(&amp;y,   &amp;x, size); \</span><br><span class="hljs-meta">memcpy(&amp;x, temp, size); \</span><br><span class="hljs-meta">&#125;</span><br></code></pre></td></tr></table></figure><p>但用宏带来的问题就是编译器做字符串替换，因为宏是做字符串替换，所以会导致代码膨胀，导致编译出的执行文件比较大。不过对于 swap 这个简单的函数来说，用<code>void*</code>和宏替换来说都可以达到泛型。</p><p>但是，如果我们不是 swap，而是 min() 或 max() 函数，那么宏替换的问题就会暴露得更多一些。比如，对于下面的这个宏：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">define</span> min(x, y)  （(x)&gt;(y) ? (y) : (x)）</span><br></code></pre></td></tr></table></figure><p>其中一个最大的问题，就是有可能会有<strong>重复执行</strong>的问题。</p><h3 id="C-泛型编程"><a href="#C-泛型编程" class="headerlink" title="C++ 泛型编程"></a>C++ 泛型编程</h3><p>C++ 泛型版的代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-keyword">template</span>&lt;<span class="hljs-keyword">typename</span> T, <span class="hljs-keyword">typename</span> Iter&gt;</span><br><span class="hljs-function">Iter <span class="hljs-title">search</span><span class="hljs-params">(Iter pStart, Iter pEnd, T target)</span> </span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-keyword">for</span>(Iter p = pStart; p != pEnd; p++) &#123;<br><span class="hljs-keyword">if</span> ( *p == target ) <br><span class="hljs-keyword">return</span> p;<br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">NULL</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>在 C++ 的泛型版本中，我们可以看到：</p><ul><li>使用<code>typename T</code>抽象了数据结构中存储数据的类型。</li><li>使用<code>typename Iter</code>，这是不同的数据结构需要自己实现的“迭代器”，这样也就抽象掉了不同类型的数据结构。</li><li>然后，我们对数据容器的遍历使用了<code>Iter</code>中的<code>++</code>方法，这是数据容器需要重载的操作符，这样通过操作符重载也就泛型掉了遍历。</li><li>在函数的入参上使用了<code>pStart</code>和<code>pEnd</code>来表示遍历的起止。</li><li>使用<code>*Iter</code>来取得这个“指针”的内容。这也是通过重载 <code>*</code> 取值操作符来达到的泛型。</li></ul><h3 id="Go-泛型编程"><a href="#Go-泛型编程" class="headerlink" title="Go 泛型编程"></a>Go 泛型编程</h3><p>go1.18开始可以支持泛型</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">package</span> main<br><span class="hljs-keyword">import</span> <span class="hljs-string">&quot;fmt&quot;</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">find</span>[<span class="hljs-title">T</span> <span class="hljs-title">comparable</span>] <span class="hljs-params">(arr []T, elem T)</span></span> <span class="hljs-type">int</span> &#123;<br>  <span class="hljs-keyword">for</span> i, v := <span class="hljs-keyword">range</span> arr &#123;<br>    <span class="hljs-keyword">if</span>  v == elem &#123;<br>      <span class="hljs-keyword">return</span> i<br>    &#125;<br>  &#125;<br>  <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span><br>&#125;<br></code></pre></td></tr></table></figure><p>Go语言的泛型已基本可用了，只不过，还有三个问题：</p><ul><li>一个是 <code>fmt.Printf()</code>中的泛型类型是 <code>%v</code> 还不够好，不能像c++ <code>iostream</code>重载 <code>&gt;&gt;</code> 来获得程序自定义的输出。</li><li>另外一个是，go不支持操作符重载，所以，你也很难在泛型算法中使用“泛型操作符”如：<code>==</code> 等</li><li>最后一个是，上面的 <code>find()</code> 算法依赖于“数组”，对于hash-table、tree、graph、link等数据结构还要重写。也就是说，没有一个像C++ STL那样的一个泛型迭代器（这其中的一部分工作当然也需要通过重载操作符（如：<code>++</code> 来实现）</li></ul><h3 id="Java泛型编程"><a href="#Java泛型编程" class="headerlink" title="Java泛型编程"></a>Java泛型编程</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">//此处T可以随便写为任意标识，常见的如T、E、K、V等形式的参数常用于表示泛型</span><br><span class="hljs-comment">//在实例化泛型类时，必须指定T的具体类型</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Generic</span>&lt;T&gt;&#123; <br>    <span class="hljs-comment">//key这个成员变量的类型为T,T的类型由外部指定  </span><br>    <span class="hljs-keyword">private</span> T key;<br> <br>    <span class="hljs-keyword">public</span> <span class="hljs-title function_">Generic</span><span class="hljs-params">(T key)</span> &#123; <span class="hljs-comment">//泛型构造方法形参key的类型也为T，T的类型由外部指定</span><br>        <span class="hljs-built_in">this</span>.key = key;<br>    &#125;<br> <br>    <span class="hljs-keyword">public</span> T <span class="hljs-title function_">getKey</span><span class="hljs-params">()</span>&#123; <span class="hljs-comment">//泛型方法getKey的返回值类型为T，T的类型由外部指定</span><br>        <span class="hljs-keyword">return</span> key;<br>    &#125;<br>&#125;<br><br></code></pre></td></tr></table></figure><h3 id="类型系统"><a href="#类型系统" class="headerlink" title="类型系统"></a>类型系统</h3><p>在计算机科学中，类型系统用于定义如何将编程语言中的数值和表达式归类为许多不同的类型，以及如何操作这些类型，还有这些类型如何互相作用。类型可以确认一个值或者一组值具有特定的意义和目的。</p><p>一般来说，编程语言会有两种类型，一种是内建类型，如 int、float 和 char 等，一种是抽象类型，如 struct、class 和 function 等。抽象类型在程序运行中，可能不表示为值。类型系统在各种语言之间有非常大的不同，也许，最主要的差异存在于编译时期的语法，以及运行时期的操作实现方式。</p><p>编译器可能使用值的静态类型以最优化所需的存储区，并选取对数值运算时的最佳算法。例如，在许多 C 编译器中，“浮点数”数据类型是以 32 比特表示、与 IEEE 754 规格一致的单精度浮点数。因此，在数值运算上，C 应用了浮点数规范（浮点数加法、乘法等）。</p><p>类型的约束程度以及评估方法，影响了语言的类型。更进一步，编程语言可能就类型多态性部分，对每一个类型都对应了一个针对于这个类型的算法运算。类型理论研究类型系统，尽管实际的编程语言类型系统，起源于计算机架构的实际问题、编译器实现，以及语言设计。</p><p>程序语言的类型系统主要提供如下的功能。</p><ul><li><strong>程序语言的安全性</strong>。使用类型可以让编译器侦测一些代码的错误。例如：可以识别出一个错误无效的表达式。如：<code>“Hello, World” + 3</code>这样的不同数据类型间操作的问题。强类型语言提供更多的安全性，但是并不能保证绝对的安全。</li><li><strong>利于编译器的优化</strong>。 静态类型语言的类型声明，可以让编译器明确地知道程序员的意图。因此，编译器就可以利用这一信息做很多代码优化工作。例如：如果我们指定一个类型是 <code>int</code> ，那么编译就知道，这个类型会以 4 个字节的倍数进行对齐，编译器就可以非常有效地利用更有效率的机器指令。</li><li><strong>代码的可读性</strong>。有类型的编程语言，可以让代码更易读和更易维护。代码的语义也更清楚，代码模块的接口（如函数）也更丰富和清楚。</li><li><strong>抽象化</strong>。类型允许程序设计者对程序以较高层次的方式思考，而不是烦人的低层次实现。例如，我们使用整型或是浮点型来取代底层的字节实现，我们可以将字符串设计成一个值，而不是底层字节的数组。从高层上来说，类型可以用来定义不同模块间的交互协议，比如函数的入参类型和返回类型，从而可以让接口更有语义，而且不同的模块数据交换更为直观和易懂。</li></ul><p>但是，正如前面说的，<strong>类型带来的问题就是我们作用于不同类型的代码，虽然长得非常相似，但是由于类型的问题需要根据不同版本写出不同的算法，如果要做到泛型，就需要涉及比较底层的玩法</strong>。</p><h3 id="泛型的本质"><a href="#泛型的本质" class="headerlink" title="泛型的本质"></a>泛型的本质</h3><p>要了解泛型的本质，就需要了解类型的本质。</p><ul><li>类型是对内存的一种抽象。不同的类型，会有不同的内存布局和内存分配的策略。</li><li>不同的类型，有不同的操作。所以，对于特定的类型，也有特定的一组操作。</li></ul><p>所以，要做到泛型，我们需要做下面的事情。</p><ul><li>标准化掉类型的内存分配、释放和访问。</li><li>标准化掉类型的操作。比如：比较操作，I&#x2F;O 操作，复制操作……</li><li>标准化掉数据容器的操作。比如：查找算法、过滤算法、聚合算法……</li><li>标准化掉类型上特有的操作。需要有标准化的接口来回调不同类型的具体操作……</li></ul><p>所以，C++ 动用了非常繁多和复杂的技术来达到泛型编程的目标。</p><ul><li>通过类中的构造、析构、拷贝构造，重载赋值操作符，标准化（隐藏）了类型的内存分配、释放和复制的操作。</li><li>通过重载操作符，可以标准化类型的比较等操作。</li><li>通过 iostream，标准化了类型的输入输出控制。</li><li>通过模板技术（包括模板的特化），来为不同的类型生成类型专属的代码。</li><li>通过迭代器来标准化数据容器的遍历操作。</li><li>通过面向对象的接口依赖（虚函数技术），来标准化了特定类型在特定算法上的操作。</li><li>通过函数式（函数对象），来标准化对于不同类型的特定操作。</li></ul><p>我理解其本质就是 —— <strong>屏蔽掉数据和操作数据的细节，让算法更为通用，让编程者更多地关注算法的结构，而不是在算法中处理不同的数据类型。</strong></p>]]></content>
    
    
    <categories>
      
      <category>编程范式</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>左耳听风</tag>
      
      <tag>编程范式</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hexo常用操作以及注意事项</title>
    <link href="/2022/09/03/hexo%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/"/>
    <url>/2022/09/03/hexo%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<h2 id="新建文章"><a href="#新建文章" class="headerlink" title="新建文章"></a>新建文章</h2><blockquote><p>命令：<code>hexo new [layout] title</code>或 <code>hexo n [layout] title</code></p></blockquote><p>创建文章前要先选定模板，在hexo中也叫做布局。hexo支持三种布局（layout）：post(默认)、draft、page。我们先介绍如何使用已有布局，后面还将会介绍如何自定义布局。</p><p>在博客目录下输入以下命令时，会默认使用post布局，然后自动在<code>source\_posts</code>目录生成一个text1.md文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo n text1<br></code></pre></td></tr></table></figure><p>当然你还可以指定布局：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo n [layout_name] draft1<br></code></pre></td></tr></table></figure><p>该命令创建了一个使用特定布局的名为draft1的文章。</p><p>打开之前创建的text1.md文件，我们可以看到文章开头包含以下内容：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">text1</span><br><span class="hljs-attr">author:</span> <span class="hljs-string">longpi1</span><br><span class="hljs-attr">tags:</span> <span class="hljs-string">hexo</span><br><span class="hljs-attr">categories:</span> <span class="hljs-string">blog</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><p>上面的内容在hexo被称作<strong>Front-matter，实际上就是该文章的一些变量，用于实现一些特定的功能</strong>。比如使<code>author: longpi1</code>，那么渲染后的文章中将显示文章作者为<code>longpi1</code>。</p><h2 id="本地调试"><a href="#本地调试" class="headerlink" title="本地调试"></a>本地调试</h2><p>启动hexo本地服务器<code>hexo server</code> 或 <code>hexo s</code></p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">$ hexo s INFO  Start processingINFO  Hexo is running <span class="hljs-keyword">at</span> <span class="hljs-keyword">http</span>://localhost:<span class="hljs-number">4000</span>/. Press Ctrl+C <span class="hljs-built_in">to</span> <span class="hljs-built_in">stop</span>.<br></code></pre></td></tr></table></figure><p>在浏览器输入 <a href="http://localhost:4000/">http://localhost:4000/</a> 进行预览，回到Git Bash输入<code>Ctrl+C</code>关闭本地服务器退出预览。</p><p>指定端口：<br><code>hexo s -p 8080</code></p><p>自定义 IP<br>服务器默认运行在 0.0.0.0，您可以覆盖默认的 IP 设置，如下：<br><code>hexo server -i 192.168.1.1</code></p><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p><code>hexo d</code> 或 <code>hexo deploy</code><br><code>hexo d -g</code> 部署之前预先生成静态文件</p><h3 id="关于部署后原来的CNAME文件被覆盖的问题"><a href="#关于部署后原来的CNAME文件被覆盖的问题" class="headerlink" title="关于部署后原来的CNAME文件被覆盖的问题"></a>关于部署后原来的CNAME文件被覆盖的问题</h3><p>解决：CNAME,README,404.html都可以放在Hexo&#x2F;source文件夹下，<code>hexo g</code>生成博客时会被原封不动的拷贝到public文件夹中，部署后自然就到了项目的根目录。</p>]]></content>
    
    
    <categories>
      
      <category>hexo</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分布式系统笔记</title>
    <link href="/2022/08/28/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/08/28/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="传统单体架构和分布式服务化架构的区别"><a href="#传统单体架构和分布式服务化架构的区别" class="headerlink" title="传统单体架构和分布式服务化架构的区别"></a>传统单体架构和分布式服务化架构的区别</h2><p><img src="https://static001.geekbang.org/resource/image/8f/91/8fecccec610626a3e348318b1fd17791.png?wh=1084*724" alt="img"></p><p><strong>存在的问题：</strong></p><ul><li>架构设计变得复杂（尤其是其中的分布式事务）。</li><li>部署单个服务会比较快，但是如果一次部署需要多个服务，流程会变得复杂。</li><li>系统的吞吐量会变大，但是响应时间会变长。</li><li>运维复杂度会因为服务变多而变得很复杂。</li><li>架构复杂导致学习曲线变大。</li><li>测试和查错的复杂度增大。</li><li>技术多元化，这会带来维护和运维的复杂度。</li><li>管理分布式系统中的服务和调度变得困难和复杂。</li></ul><h2 id="分布式系统的目的以及相关技术"><a href="#分布式系统的目的以及相关技术" class="headerlink" title="分布式系统的目的以及相关技术"></a>分布式系统的目的以及相关技术</h2><p><strong>构建分布式系统的目的是增加系统容量，提高系统的可用性，转换成技术方面，也就是完成下面两件事。</strong></p><ul><li><strong>大流量处理。</strong>通过集群技术把大规模并发请求的负载分散到不同的机器上。</li><li><strong>关键业务保护。</strong>提高后台服务的可用性，把故障隔离起来阻止多米诺骨牌效应（雪崩效应）。如果流量过大，需要对业务降级，以保护关键业务流转。</li></ul><h3 id="提高架构的性能"><a href="#提高架构的性能" class="headerlink" title="提高架构的性能"></a>提高架构的性能</h3><p><img src="https://static001.geekbang.org/resource/image/a9/17/a9edeae125a80f381003d8d9d0056317.png?wh=863*321" alt="img"></p><ul><li><strong>缓存系统。</strong>加入缓存系统，可以有效地提高系统的访问能力。从前端的浏览器，到网络，再到后端的服务，底层的数据库、文件系统、硬盘和 CPU，全都有缓存，这是提高快速访问能力最有效的手段。对于分布式系统下的缓存系统，需要的是一个缓存集群。这其中需要一个 Proxy 来做缓存的分片和路由。</li><li><strong>负载均衡系统。</strong>负载均衡系统是水平扩展的关键技术，它可以使用多台机器来共同分担一部分流量请求。</li><li><strong>异步调用。</strong>异步系统主要通过消息队列来对请求做排队处理，这样可以把前端的请求的峰值给“削平”了，而后端通过自己能够处理的速度来处理请求。这样可以增加系统的吞吐量，但是实时性就差很多了。同时，还会引入消息丢失的问题，所以要对消息做持久化，这会造成“有状态”的结点，从而增加了服务调度的难度。</li><li><strong>数据分区和数据镜像。数据分区</strong>是把数据按一定的方式分成多个区（比如通过地理位置），不同的数据区来分担不同区的流量。这需要一个数据路由的中间件，会导致跨库的 Join 和跨库的事务非常复杂。而<strong>数据镜像</strong>是把一个数据库镜像成多份一样的数据，这样就不需要数据路由的中间件了。你可以在任意结点上进行读写，内部会自行同步数据。然而，数据镜像中最大的问题就是数据的一致性问题。</li></ul><h3 id="提高架构的稳定性"><a href="#提高架构的稳定性" class="headerlink" title="提高架构的稳定性"></a>提高架构的稳定性</h3><p><img src="https://static001.geekbang.org/resource/image/be/79/befd21e1b41a257c5028f8c1bc7fa279.png?wh=865*315" alt="img"></p><ul><li><strong>服务拆分。</strong>服务拆分主要有两个目的：一是为了隔离故障，二是为了重用服务模块。但服务拆分完之后，会引入服务调用间的依赖问题。</li><li><strong>服务冗余。</strong>服务冗余是为了去除单点故障，并可以支持服务的弹性伸缩，以及故障迁移。然而，对于一些有状态的服务来说，冗余这些有状态的服务带来了更高的复杂性。其中一个是弹性伸缩时，需要考虑数据的复制或是重新分片，迁移的时候还要迁移数据到其它机器上。</li><li><strong>限流降级。</strong>当系统实在扛不住压力时，只能通过限流或者功能降级的方式来停掉一部分服务，或是拒绝一部分用户，以确保整个架构不会挂掉。这些技术属于保护措施。</li><li><strong>高可用架构。</strong>通常来说高可用架构是从冗余架构的角度来保障可用性。比如，多租户隔离，灾备多活，或是数据可以在其中复制保持一致性的集群。总之，就是为了不出单点故障。</li><li><strong>高可用运维。</strong>高可用运维指的是 DevOps 中的 CI&#x2F;CD（持续集成 &#x2F; 持续部署）。一个良好的运维应该是一条很流畅的软件发布管线，其中做了足够的自动化测试，还可以做相应的灰度发布，以及对线上系统的自动化控制。这样，可以做到“计划内”或是“非计划内”的宕机事件的时长最短。</li></ul><h3 id="分布式系统的关键技术"><a href="#分布式系统的关键技术" class="headerlink" title="分布式系统的关键技术"></a>分布式系统的关键技术</h3><ul><li><strong>服务治理。</strong>服务拆分、服务调用、服务发现、服务依赖、服务的关键度定义……服务治理的最大意义是需要把服务间的依赖关系、服务调用链，以及关键的服务给梳理出来，并对这些服务进行性能和可用性方面的管理。</li><li><strong>架构软件管理。</strong>服务之间有依赖，而且有兼容性问题，所以，整体服务所形成的架构需要有架构版本管理、整体架构的生命周期管理，以及对服务的编排、聚合、事务处理等服务调度功能。</li><li><strong>DevOps。</strong>分布式系统可以更为快速地更新服务，但是对于服务的测试和部署都会是挑战。所以，还需要 DevOps 的全流程，其中包括环境构建、持续集成、持续部署等。</li><li><strong>自动化运维。</strong>有了 DevOps 后，我们就可以对服务进行自动伸缩、故障迁移、配置管理、状态管理等一系列的自动化运维技术了。</li><li><strong>资源调度管理。</strong>应用层的自动化运维需要基础层的调度支持，也就是云计算 IaaS 层的计算、存储、网络等资源调度、隔离和管理。</li><li><strong>整体架构监控。</strong>如果没有一个好的监控系统，那么自动化运维和资源调度管理只可能成为一个泡影，因为监控系统是你的眼睛。没有眼睛，没有数据，就无法进行高效运维。所以说，监控是非常重要的部分。这里的监控需要对三层系统（应用层、中间件层、基础层）进行监控。</li><li><strong>流量控制</strong>。最后是我们的流量控制，负载均衡、服务路由、熔断、降级、限流等和流量相关的调度都会在这里，包括灰度发布之类的功能也在这里。</li></ul><p><img src="https://static001.geekbang.org/resource/image/8e/db/8e92e2dff4f66147c014f930aa678fdb.jpg?wh=2556x1006" alt="img"></p><h3 id="全栈监控"><a href="#全栈监控" class="headerlink" title="全栈监控"></a>全栈监控</h3><p><strong>全栈监控，其实就是三层监控。</strong></p><ul><li><strong>基础层：</strong>监控主机和底层资源。比如：CPU、内存、网络吞吐、硬盘 I&#x2F;O、硬盘使用等。</li><li><strong>中间层：</strong>就是中间件层的监控。比如：Nginx、Redis、ActiveMQ、Kafka、MySQL、Tomcat 等。应用层：</li><li><strong>监控应用层的使用</strong>。比如：HTTP 访问的吞吐量、响应时间、返回码、调用链路分析、性能瓶颈，还包括用户端的监控。</li></ul><p><img src="https://static001.geekbang.org/resource/image/fe/4f/fe3aaf79df1565505cdac32494078a4f.jpg?wh=2145x1152" alt="img"></p><h4 id="什么才是好的监控系统"><a href="#什么才是好的监控系统" class="headerlink" title="什么才是好的监控系统"></a>什么才是好的监控系统</h4><p><strong>监控系统可能存在的问题：</strong></p><p>1.<strong>监控数据是隔离开来的。</strong>因为公司分工的问题，开发、应用运维、系统运维，各管各的，所以很多公司的监控系统之间都有一道墙，完全串不起来。</p><p><strong>2.监控的数据项太多。</strong>有些公司的运维团队把监控的数据项多作为一个亮点到处讲，比如监控指标达到 5 万多个。老实说，这太丢人了。因为信息太多等于没有信息，抓不住重点的监控才会做成这个样子，完全就是使蛮力的做法。</p><p><strong>好的监控系统有以下几个特征：</strong></p><ol><li><strong>关注于整体应用的 SLA（服务级别协议）。</strong>主要从为用户服务的 API 来监控整个系统。</li><li><strong>关联指标聚合。</strong>把有关联的系统及其指标聚合展示。主要是三层系统数据：基础层、平台中间件层和应用层。其中，最重要的是把服务和相关的中间件以及主机关联在一起，服务有可能运行在 Docker 中，也有可能运行在微服务平台上的多个 JVM 中，也有可能运行在 Tomcat 中。总之，无论运行在哪里，我们都需要把服务的具体实例和主机关联在一起，否则，对于一个分布式系统来说，定位问题犹如大海捞针。</li><li><strong>快速故障定位。</strong>对于现有的系统来说，故障总是会发生的，而且还会频繁发生。故障发生不可怕，可怕的是故障的恢复时间过长。所以，快速地定位故障就相当关键。快速定位问题需要对整个分布式系统做一个用户请求跟踪的 trace 监控，我们需要监控到所有的请求在分布式系统中的调用链，这个事最好是做成没有侵入性的。</li></ol><p><strong>以下两大主要功能实现</strong></p><h5 id="“体检”"><a href="#“体检”" class="headerlink" title="“体检”"></a>“体检”</h5><ul><li><strong>容量管理。</strong>提供一个全局的系统运行时数据的展示，可以让工程师团队知道是否需要增加机器或者其它资源。</li><li><strong>性能管理。</strong>可以通过查看大盘，找到系统瓶颈，并有针对性地优化系统和相应代码。</li></ul><h5 id="“急诊”"><a href="#“急诊”" class="headerlink" title="“急诊”"></a>“急诊”</h5><ul><li><strong>定位问题</strong>。可以快速地暴露并找到问题的发生点，帮助技术人员诊断问题。</li><li><strong>性能分析。</strong>当出现非预期的流量提升时，可以快速地找到系统的瓶颈，并帮助开发人员深入代码。</li></ul><p><strong>如何做出一个好的监控系统</strong></p><ul><li><strong>服务调用链跟踪。</strong>这个监控系统应该从对外的 API 开始，然后将后台的实际服务给关联起来，然后再进一步将这个服务的依赖服务关联起来，直到最后一个服务（如 MySQL 或 Redis），这样就可以把整个系统的服务全部都串连起来了。这个事情的最佳实践是 Google Dapper 系统，其对应于开源的实现是 Zipkin。对于 Java 类的服务，我们可以使用字节码技术进行字节码注入，做到代码无侵入式。</li><li><strong>服务调用时长分布。</strong>使用 Zipkin，可以看到一个服务调用链上的时间分布，这样有助于我们知道最耗时的服务是什么。下图是 Zipkin 的服务调用时间分布。</li><li><strong>服务的 TOP N 视图。</strong>所谓 TOP N 视图就是一个系统请求的排名情况。一般来说，这个排名会有三种排名的方法：a）按调用量排名，b) 按请求最耗时排名，c）按热点排名（一个时间段内的请求次数的响应时间和）。</li><li><strong>数据库操作关联。</strong>对于 Java 应用，我们可以很方便地通过 JavaAgent 字节码注入技术拿到 JDBC 执行数据库操作的执行时间。对此，我们可以和相关的请求对应起来。</li><li><strong>服务资源跟踪。</strong>我们的服务可能运行在物理机上，也可能运行在虚拟机里，还可能运行在一个 Docker 的容器里，Docker 容器又运行在物理机或是虚拟机上。我们需要把服务运行的机器节点上的数据（如 CPU、MEM、I&#x2F;O、DISK、NETWORK）关联起来。</li></ul><p><strong>了这些数据上的关联，我们就可以达到如下的目标。</strong></p><ol><li>当一台机器挂掉是因为 CPU 或 I&#x2F;O 过高的时候，我们马上可以知道其会影响到哪些对外服务的 API。</li><li>当一个服务响应过慢的时候，我们马上能关联出来是否在做 Java GC，或是其所在的计算结点上是否有资源不足的情况，或是依赖的服务是否出现了问题。</li><li>当发现一个 SQL 操作过慢的时候，我们能马上知道其会影响哪个对外服务的 API。</li><li>当发现一个消息队列拥塞的时候，我们能马上知道其会影响哪些对外服务的 API。</li></ol><p><strong>一旦了解了这些信息，我们就可以做出调度。比如：</strong></p><ol><li>一旦发现某个服务过慢是因为 CPU 使用过多，我们就可以做弹性伸缩。</li><li>一旦发现某个服务过慢是因为 MySQL 出现了一个慢查询，我们就无法在应用层上做弹性伸缩，只能做流量限制，或是降级操作了。</li></ol><p><strong>实现效果如下图：</strong></p><p><img src="https://static001.geekbang.org/resource/image/6b/33/6b17dd779cfecd62e02924dc8618e833.png?wh=865*381" alt="img"></p><h3 id="服务调度"><a href="#服务调度" class="headerlink" title="服务调度"></a>服务调度</h3><p><strong>微服务是服务依赖最优解的上限，而服务依赖的下限是千万不要有依赖环。</strong>如果系统架构中有服务依赖环，那么表明你的架构设计是错误的。循环依赖有很多的副作用，最大的问题是这是一种极强的耦合，会导致服务部署相当复杂和难解，而且会导致无穷尽的递归故障和一些你意想不到的问题。</p><h4 id="服务状态和生命周期的管理"><a href="#服务状态和生命周期的管理" class="headerlink" title="服务状态和生命周期的管理"></a>服务状态和生命周期的管理</h4><p>服务的生命周期通常会有以下几个状态：</p><ul><li>Provision，代表在供应一个新的服务；</li><li>Ready，表示启动成功了；</li><li>Run，表示通过了服务健康检查；</li><li>Update，表示在升级中；</li><li>Rollback，表示在回滚中；</li><li>Scale，表示正在伸缩中（可以有 Scale-in 和 Scale-out 两种）；</li><li>Destroy，表示在销毁中；</li><li>Failed，表示失败状态。</li></ul><p>这几个状态需要管理好，不然的话，你将不知道这些服务在什么样的状态下。不知道在什么样的状态下，你对整个分布式架构也就无法控制了。</p><h4 id="整个架构的版本管理"><a href="#整个架构的版本管理" class="headerlink" title="整个架构的版本管理"></a>整个架构的版本管理</h4><p>需要一个架构的 manifest，一个服务清单，这个服务清单定义了所有服务的版本运行环境，其中包括但不限于：</p><ul><li>服务的软件版本；</li><li>服务的运行环境——环境变量、CPU、内存、可以运行的节点、文件系统等；</li><li>服务运行的最大最小实例数。</li></ul><h4 id="资源-x2F-服务调度"><a href="#资源-x2F-服务调度" class="headerlink" title="资源 &#x2F; 服务调度"></a>资源 &#x2F; 服务调度</h4><p>服务和资源的调度有点像操作系统。操作系统一方面把用户进程在硬件资源上进行调度，另一方面提供进程间的通信方式，可以让不同的进程在一起协同工作。服务和资源调度的过程，与操作系统调度进程的方式很相似，主要有以下一些关键技术。</p><ul><li>服务状态的维持和拟合。</li><li>服务的弹性伸缩和故障迁移。</li><li>作业和应用调度。</li><li>作业工作流编排。</li><li>服务编排。</li></ul><h4 id="服务状态的维持"><a href="#服务状态的维持" class="headerlink" title="服务状态的维持"></a>服务状态的维持</h4><p>所谓服务状态不是服务中的数据状态，而是服务的运行状态，换句话说就是服务的 Status，而不是 State。也就是上述服务运行时生命周期中的状态——Provision，Ready，Run，Scale，Rollback，Update，Destroy，Failed……服务运行时的状态是非常关键的。</p><p>服务运行过程中，状态也是会有变化的，这样的变化有两种。</p><ul><li>一种是没有预期的变化。比如，服务运行因为故障导致一些服务挂掉，或是别的什么原因出现了服务不健康的状态。而一个好的集群管理控制器应该能够强行维护服务的状态。在健康的实例数变少时，控制器会把不健康的服务给摘除，而又启动几个新的，强行维护健康的服务实例数。</li><li>另外一种是预期的变化。比如，我们需要发布新版本，需要伸缩，需要回滚。这时，集群管理控制器就应该把集群从现有状态迁移到另一个新的状态。这个过程并不是一蹴而就的，集群控制器需要一步一步地向集群发送若干控制命令。这个过程叫“拟合”——从一个状态拟合到另一个状态，而且要穷尽所有的可能，玩命地不断地拟合，直到达到目的。</li></ul><h4 id="服务的弹性伸缩和故障迁移"><a href="#服务的弹性伸缩和故障迁移" class="headerlink" title="服务的弹性伸缩和故障迁移"></a>服务的弹性伸缩和故障迁移</h4><p>有了上述的服务状态拟合的基础工作之后，我们就能很容易地管理服务的生命周期了，甚至可以通过底层的支持进行便利的服务弹性伸缩和故障迁移。</p><p>对于弹性伸缩，在上面我已经给出了一个服务伸缩所需要的操作步骤。还是比较复杂的，其中涉及到了：</p><ul><li>底层资源的伸缩；</li><li>服务的自动化部署；</li><li>服务的健康检查；</li><li>服务发现的注册；</li><li>服务流量的调度。</li></ul><p>而对于故障迁移，也就是服务的某个实例出现问题时，我们需要自动地恢复它。对于服务来说，有两种模式，一种是宠物模式，一种是奶牛模式。</p><ul><li>所谓宠物模式，就是一定要救活，主要是对于 stateful 的服务。</li><li>而奶牛模式，就是不用救活了，重新生成一个实例。</li></ul><p>对于这两种模式，在运行中也是比较复杂的，其中涉及到了：</p><ul><li>服务的健康监控（这可能需要一个 APM 的监控）。</li><li>如果是宠物模式，需要：服务的重新启动和服务的监控报警（如果重试恢复不成功，需要人工介入）。</li><li>如果是奶牛模式，需要：服务的资源申请，服务的自动化部署，服务发现的注册，以及服务的流量调度。</li></ul><p>把传统的服务迁移到 Docker 和 Kubernetes 上来，再加上更上层的对服务生命周期的控制系统的调度，我们就可以做到一个完全自动化的运维架构了。</p><h4 id="服务工作流和编排"><a href="#服务工作流和编排" class="headerlink" title="服务工作流和编排"></a>服务工作流和编排</h4><p>正如上面和操作系统做的类比一样，一个好的操作系统需要能够通过一定的机制把一堆独立工作的进程给协同起来。在分布式的服务调度中，这个工作叫做 <strong>Orchestration</strong>，国内把这个词翻译成<strong>“编排”</strong>。</p><h3 id="流量与数据调度"><a href="#流量与数据调度" class="headerlink" title="流量与数据调度"></a>流量与数据调度</h3><p>关于流量调度，现在很多人都把这个事和服务治理混为一谈了。但是还是应该分开的。</p><ol><li>一方面，服务治理是内部系统的事，而流量调度可以是内部的，更是外部接入层的事。</li><li>另一方面，服务治理是数据中心的事，而流量调度要做得好，应该是数据中心之外的事，也就是我们常说的边缘计算，是应该在类似于 CDN 上完成的事。</li></ol><p>所以，流量调度和服务治理是在不同层面上的，不应该混在一起，所以在系统架构上应该把它们分开。</p><h4 id="流量调度的主要功能"><a href="#流量调度的主要功能" class="headerlink" title="流量调度的主要功能"></a>流量调度的主要功能</h4><p>对于一个流量调度系统来说，其应该具有的主要功能是：</p><ol><li>依据系统运行的情况，自动地进行流量调度，在无需人工干预的情况下，提升整个系统的稳定性；</li><li>让系统应对爆品等突发事件时，在弹性计算扩缩容的较长时间窗口内或底层资源消耗殆尽的情况下，保护系统平稳运行。</li></ol><p>这还是为了提高系统架构的稳定性和高可用性。</p><p>此外，这个流量调度系统还可以完成以下几方面的事情。</p><ul><li><strong>服务流控。</strong>服务发现、服务路由、服务降级、服务熔断、服务保护等。</li><li><strong>流量控制。</strong>负载均衡、流量分配、流量控制、异地灾备（多活）等。</li><li><strong>流量管理。</strong>协议转换、请求校验、数据缓存、数据计算等。</li></ul><p>所有的这些都应该是一个 API Gateway 应该做的事。</p><h4 id="流量调度的关键技术"><a href="#流量调度的关键技术" class="headerlink" title="流量调度的关键技术"></a>流量调度的关键技术</h4><p>一个好的 API Gateway 需要具备以下的关键技术。</p><ul><li><strong>高性能。</strong>API Gateway 必须使用高性能的技术，所以，也就需要使用高性能的语言。</li><li><strong>扛流量。</strong>要能扛流量，就需要使用集群技术。集群技术的关键点是在集群内的各个结点中共享数据。这就需要使用像 Paxos、Raft、Gossip 这样的通讯协议。因为 Gateway 需要部署在广域网上，所以还需要集群的分组技术。</li><li><strong>业务逻辑。</strong>API Gateway 需要有简单的业务逻辑，所以，最好是像 AWS 的 Lambda 服务一样，可以让人注入不同语言的简单业务逻辑。</li><li><strong>服务化。</strong>一个好的 API Gateway 需要能够通过 Admin API 来不停机地管理配置变更，而不是通过一个.conf 文件来人肉地修改配置。</li></ul><h4 id="状态数据调度"><a href="#状态数据调度" class="headerlink" title="状态数据调度"></a>状态数据调度</h4><p>对于服务调度来说，最难办的就是有状态的服务了。这里的状态是 State，也就是说，有些服务会保存一些数据，而这些数据是不能丢失的，所以，这些数据是需要随服务一起调度的。</p><p>一般来说，我们会通过“转移问题”的方法来让服务变成“无状态的服务”。也就是说，会把这些有状态的东西存储到第三方服务上，比如 Redis、MySQL、ZooKeeper，或是 NFS、Ceph 的文件系统中。</p><p>这些“转移问题”的方式把问题转移到了第三方服务上，于是自己的 Java 或 PHP 服务中没有状态，但是 Redis 和 MySQL 上则有了状态。所以，我们可以看到，现在的分布式系统架构中出问题的基本都是这些存储状态的服务。</p><p>因为数据存储结点在 Scale 上比较困难，所以成了一个单点的瓶颈。</p><h4 id="分布式事务一致性的问题"><a href="#分布式事务一致性的问题" class="headerlink" title="分布式事务一致性的问题"></a>分布式事务一致性的问题</h4><p>要解决数据结点的 Scale 问题，也就是让数据服务可以像无状态的服务一样在不同的机器上进行调度，这就会涉及数据的 replication 问题。而数据 replication 则会带来数据一致性的问题，进而对性能带来严重的影响。</p><p>要解决数据不丢失的问题，只能通过数据冗余的方法，就算是数据分区，每个区也需要进行数据冗余处理。这就是数据副本。当出现某个节点的数据丢失时，可以从副本读到。数据副本是分布式系统解决数据丢失异常的唯一手段。简单来说：</p><ul><li>要想让数据有高可用性，就得写多份数据。</li><li>写多份会引起数据一致性的问题。</li><li>数据一致性的问题又会引发性能问题</li></ul><p>在解决数据副本间的一致性问题时，可以使用以下这些技术方案。</p><ul><li>Master-Slave 方案。</li><li>Master-Master 方案。</li><li>两阶段和三阶段提交方案。</li><li>Paxos 方案。</li></ul><p><strong>关于分布式的事务处理：</strong><a href="https://coolshell.cn/articles/10910.html">https://coolshell.cn/articles/10910.html</a></p><h4 id="状态数据调总结"><a href="#状态数据调总结" class="headerlink" title="状态数据调总结"></a>状态数据调总结</h4><ul><li>对于应用层上的分布式事务一致性，只有两阶段提交这样的方式。</li><li>而底层存储可以解决这个问题的方式是通过一些像 Paxos、Raft 或是 NWR 这样的算法和模型来解决。</li><li>状态数据调度应该是由分布式存储系统来解决的，这样会更为完美。但是因为数据存储的 Scheme 太多，所以，导致我们有各式各样的分布式存储系统，有文件对象的，有关系型数据库的，有 NoSQL 的，有时序数据的，有搜索数据的，有队列的……</li></ul><p>数据调度应该是在 IaaS 层的数据存储解决的问题，而不是在 PaaS 层或者 SaaS 层来解决的。</p><p>在 IaaS 层上解决这个问题，一般来说有三种方案，</p><p>一种是使用比较廉价的开源产品，如：NFS、Ceph、TiDB、CockroachDB、ElasticSearch、InfluxDB、MySQL Cluster 和 Redis Cluster 之类的；另一种是用云计算厂商的方案。当然，如果不差钱的话，可以使用更为昂贵的商业网络存储方案。</p><h3 id="Pass平台的本质"><a href="#Pass平台的本质" class="headerlink" title="Pass平台的本质"></a>Pass平台的本质</h3><p><img src="https://s2.loli.net/2022/08/28/ABQ1nR9wt5opvFY.png" alt="Pass平台.png"></p><p>下面这三件事是 PaaS 跟传统中间件最大的差别。</p><ul><li><strong>服务化是 PaaS 的本质</strong>。软件模块重用，服务治理，对外提供能力是 PaaS 的本质。</li><li><strong>分布式是 PaaS 的根本特性</strong>。多租户隔离、高可用、服务编排是 PaaS 的基本特性。</li><li><strong>自动化是 PaaS 的灵魂</strong>。自动化部署安装运维，自动化伸缩调度是 PaaS 的关键。</li></ul><h2 id="PaaS-平台的总体架构"><a href="#PaaS-平台的总体架构" class="headerlink" title="PaaS 平台的总体架构"></a>PaaS 平台的总体架构</h2><p><img src="https://s2.loli.net/2022/08/28/rTUA9lfnSFjOtqC.png" alt="架构图.png"></p><p>在 Docker+Kubernetes 层之上，我们看到了两个相关的 PaaS 层。一个是 PaaS 调度层，很多人将其称为 iPaaS；另一个是 PaaS 能力层，通常被称为 aPaaS。没有 PaaS 调度层，PaaS 能力层很难被管理和运维，而没有 PaaS 能力层，PaaS 就失去了提供实际能力的业务价值。而本文更多的是在讲 PaaS 调度层上的东西。</p><p>一个完整的 PaaS 平台会包括以下几部分。</p><ul><li>PaaS 调度层 – 主要是 PaaS 的自动化和分布式对于高可用高性能的管理。</li><li>PaaS 能力服务层 – 主要是 PaaS 真正提供给用户的服务和能力。</li><li>PaaS 的流量调度 – 主要是与流量调度相关的东西，包括对高并发的管理。</li><li>PaaS 的运营管理 – 软件资源库、软件接入、认证和开放平台门户。</li><li>PaaS 的运维管理 – 主要是 DevOps 相关的东西。</li></ul><h2 id="PaaS-平台的生产和运维"><a href="#PaaS-平台的生产和运维" class="headerlink" title="PaaS 平台的生产和运维"></a>PaaS 平台的生产和运维</h2><p><img src="https://s2.loli.net/2022/08/28/m2DzoMRpQkUT5Ii.png" alt="image-20220814122700725.png"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>传统的单体架构系统容量显然是有上限的。同时，为了应对有计划和无计划的下线时间，系统的可用性也是有其极限的。分布式系统为以上两个问题提供了解决方案，并且还附带有其他优势。但是，要同时解决这两个问题决非易事。为了构建分布式系统，我们面临的主要问题如下。</p><ul><li>分布式系统的硬件故障发生率更高，故障发生是常态，需要尽可能地将运维流程自动化。</li><li>需要良好地设计服务，避免某服务的单点故障对依赖它的其他服务造成大面积影响。</li><li>为了容量的可伸缩性，服务的拆分、自治和无状态变得更加重要，可能需要对老的软件逻辑做大的修改。</li><li>老的服务可能是异构的，此时需要让它们使用标准的协议，以便可以被调度、编排，且互相之间可以通信。</li><li>服务软件故障的处理也变得复杂，需要优化的流程，以加快故障的恢复。</li><li>为了管理各个服务的容量，让分布式系统发挥出最佳性能，需要有流量调度技术。</li><li>分布式存储会让事务处理变得复杂；在事务遇到故障无法被自动恢复的情况下，手动恢复流程也会变得复杂。</li><li>测试和查错的复杂度增大。</li><li>系统的吞吐量会变大，但响应时间会变长。</li></ul><p>为了解决这些问题，我们深入了解了以下这些解决方案。</p><ul><li>需要有完善的监控系统，以便对服务运行状态有全面的了解。</li><li>设计服务时要分析其依赖链；当非关键服务故障时，其他服务要自动降级功能，避免调用该服务。</li><li>重构老的软件，使其能被服务化；可以参考 SOA 和微服务的设计方式，目标是微服务化；使用 Docker 和 Kubernetes 来调度服务。</li><li>为老的服务编写接口逻辑来使用标准协议，或在必要时重构老的服务以使得它们有这些功能。</li><li>自动构建服务的依赖地图，并引入好的处理流程，让团队能以最快速度定位和恢复故障。</li><li>使用一个 API Gateway，它具备服务流向控制、流量控制和管理的功能。</li><li>事务处理建议在存储层实现；根据业务需求，或者降级使用更简单、吞吐量更大的最终一致性方案，或者通过二阶段提交、Paxos、Raft、NWR 等方案之一，使用吞吐量小的强一致性方案。</li><li>通过更真实地模拟生产环境，乃至在生产环境中做灰度发布，从而增加测试强度；同时做充分的单元测试和集成测试以发现和消除缺陷；最后，在服务故障发生时，相关的多个团队同时上线自查服务状态，以最快地定位故障原因。</li><li>通过异步调用来减少对短响应时间的依赖；对关键服务提供专属硬件资源，并优化软件逻辑以缩短响应时间。</li></ul><h3 id="基础理论"><a href="#基础理论" class="headerlink" title="基础理论"></a>基础理论</h3><h2 id="CAP-定理"><a href="#CAP-定理" class="headerlink" title="CAP 定理"></a><a href="https://en.wikipedia.org/wiki/CAP_theorem">CAP 定理</a></h2><p>CAP 定理是分布式系统设计中最基础，也是最为关键的理论。它指出，分布式数据存储不可能同时满足以下三个条件。</p><ul><li><strong>一致性（Consistency）</strong>：每次读取要么获得最近写入的数据，要么获得一个错误。</li><li><strong>可用性（Availability）</strong>：每次请求都能获得一个（非错误）响应，但不保证返回的是最新写入的数据。</li><li><strong>分区容忍（Partition tolerance）</strong>：尽管任意数量的消息被节点间的网络丢失（或延迟），系统仍继续运行。</li></ul><p>也就是说，CAP 定理表明，在存在网络分区的情况下，一致性和可用性必须二选一。而在没有发生网络故障时，即分布式系统正常运行时，一致性和可用性是可以同时被满足的。这里需要注意的是，CAP 定理中的一致性与 ACID 数据库事务中的一致性截然不同。</p><p>掌握 CAP 定理，尤其是能够正确理解 C、A、P 的含义，对于系统架构来说非常重要。因为对于分布式系统来说，网络故障在所难免，如何在出现网络故障的时候，维持系统按照正常的行为逻辑运行就显得尤为重要。你可以结合实际的业务场景和具体需求，来进行权衡。</p><p>例如，对于大多数互联网应用来说（如门户网站），因为机器数量庞大，部署节点分散，网络故障是常态，可用性是必须要保证的，所以只有舍弃一致性来保证服务的 AP。而对于银行等，需要确保一致性的场景，通常会权衡 CA 和 CP 模型，CA 模型网络故障时完全不可用，CP 模型具备部分可用性。</p><p><img src="https://s2.loli.net/2022/08/28/VhY8TjpDHOAs6JX.png" alt="image-20220814124520469.png"></p><ul><li>CA (consistency + availability)，这样的系统关注一致性和可用性，它需要非常严格的全体一致的协议，比如“两阶段提交”（2PC）。CA 系统不能容忍网络错误或节点错误，一旦出现这样的问题，整个系统就会拒绝写请求，因为它并不知道对面的那个结点是否挂掉了，还是只是网络问题。唯一安全的做法就是把自己变成只读的。</li><li>CP (consistency + partition tolerance)，这样的系统关注一致性和分区容忍性。它关注的是系统里大多数人的一致性协议，比如：Paxos 算法（Quorum 类的算法）。这样的系统只需要保证大多数结点数据一致，而少数的结点会在没有同步到最新版本的数据时变成不可用的状态。这样能够提供一部分的可用性。</li><li>AP (availability + partition tolerance)，这样的系统关心可用性和分区容忍性。因此，这样的系统不能达成一致性，需要给出数据冲突，给出数据冲突就需要维护数据版本。Dynamo 就是这样的系统。</li></ul><h4 id="Paxos-算法"><a href="#Paxos-算法" class="headerlink" title="Paxos 算法"></a>Paxos 算法</h4><p>Paxos 算法，是莱斯利·兰伯特（Lesile Lamport）于 1990 年提出来的一种基于消息传递且具有高度容错特性的一致性算法。但是这个算法太过于晦涩，所以，一直以来都属于理论上的论文性质的东西。</p><h4 id="Raft-算法"><a href="#Raft-算法" class="headerlink" title="Raft 算法"></a>Raft 算法</h4><p>因为 Paxos 算法太过于晦涩，而且在实际的实现上有太多的坑，并不太容易写对。所以，有人搞出了另外一个一致性的算法，叫 Raft。其原始论文是<a href="https://raft.github.io/raft.pdf"> In search of an Understandable Consensus Algorithm (Extended Version) </a>寻找一种易于理解的 Raft 算法。这篇论文的译文在 InfoQ 上《<a href="http://www.infoq.com/cn/articles/raft-paper">Raft 一致性算法论文译文</a>》</p><p>Raft 算法和 Paxos 的性能和功能是一样的，但是它和 Paxos 算法的结构不一样，这使 Raft 算法更容易理解并且更容易实现。那么 Raft 是怎样做到的呢？</p><p>Raft 把这个一致性的算法分解成了几个部分，一个是领导选举（Leader Selection），一个是日志复制（Log Replication），一个是安全性（Safety），还有一个是成员变化（Membership Changes）。对于一般人来说，Raft 协议比 Paxos 的学习曲线更低，也更平滑。</p><p>Raft 协议中有一个状态机，每个结点会有三个状态，分别是 Leader、Candidate 和 Follower。Follower 只响应其他服务器的请求，如果没有收到任何信息，它就会成为一个 Candidate，并开始进行选举。收到大多数人同意选票的人会成为新的 Leader。</p><p><img src="https://s2.loli.net/2022/08/28/AZdFOXp17fvj2J9.png" alt="image-20220814125128384.png"></p><p>一旦选举出了一个 Leader，它就开始负责服务客户端的请求。每个客户端的请求都包含一个要被复制状态机执行的指令。Leader 首先要把这个指令追加到 log 中形成一个新的 entry，然后通过 AppendEntries RPC 并行地把该 entry 发给其他服务器（server）。如果其他服务器没发现问题，复制成功后会给 Leader 一个表示成功的 ACK。</p><p>Leader 收到大多数 ACK 后应用该日志，返回客户端执行结果。如果 Follower 崩溃 （crash）或者丢包，Leader 会不断重试 AppendEntries RPC。</p><p><img src="https://s2.loli.net/2022/08/28/9EcGBU8KDSrdah5.png" alt="image-20220814125218440.png"></p><p>几个不错的 Raft 算法的动画演示。</p><ul><li><a href="http://thesecretlivesofdata.com/raft/">Raft – The Secret Lives of Data</a></li><li><a href="https://raft.github.io/">Raft Consensus Algorithm</a></li><li><a href="http://kanaka.github.io/raft.js/">Raft Distributed Consensus Algorithm Visualization</a></li></ul><h4 id="逻辑钟和向量钟"><a href="#逻辑钟和向量钟" class="headerlink" title="逻辑钟和向量钟"></a>逻辑钟和向量钟</h4><p>后面，业内又搞出来一些工程上的东西，比如 Amazon 的 DynamoDB，其论文<a href="http://bnrg.eecs.berkeley.edu/~randy/Courses/CS294.F07/Dynamo.pdf">Dynamo: Amazon’s Highly Available Key Value Store</a> 的影响力也很大。这篇论文中讲述了 Amazon 的 DynamoDB 是如何满足系统的高可用、高扩展和高可靠要求的，其中还展示了系统架构是如何做到数据分布以及数据一致性的。</p><p>GFS 采用的是查表式的数据分布，而 DynamoDB 采用的是计算式的，也是一个改进版的通过虚拟结点减少增加结点带来数据迁移的一致性哈希。另外，这篇论文中还讲述了一个 NRW 模式用于让用户可以灵活地在 CAP 系统中选取其中两项，这使用到了 Vector Clock——向量时钟来检测相应的数据冲突。最后还介绍了使用 Handoff 的机制对可用性的提升。</p><p>这篇文章中有几个关键的概念，一个是 Vector Clock，另一个是 Gossip 协议。</p><p>提到向量时钟就需要提一下逻辑时钟。所谓逻辑时间，也就是在分布系统中为了解决消息有序的问题，由于在不同的机器上有不同的本地时间，这些本地时间的同步很难搞，会导致消息乱序。</p><p>于是 Paxos 算法的发明人兰伯特（Lamport）搞了个向量时钟，每个系统维护一个本地的计数器，这就是所谓的逻辑时钟。每执行一个事件（例如向网络发送消息，或是交付到应用层）都对这个计数器做加 1 操作。当跨系统的时候，在消息体上附着本地计算器，当接收端收到消息时，更新自己的计数器（取对端传来的计数器和自己当成计数器的最大值），也就是调整自己的时钟。</p><p>逻辑时钟可以保证，如果事件 A 先于事件 B，那么事件 A 的时钟一定小于事件 B 的时钟，但是返过来则无法保证，因为返过来没有因果关系。所以，向量时钟解释了因果关系。向量时钟维护了数据更新的一组版本号（版本号其实就是使用逻辑时钟）。</p><p>假如一个数据需要存在三个结点上 A、B、C。那么向量维度就是 3，在初始化的时候，所有结点对于这个数据的向量版本是 [A:0, B:0, C:0]。当有数据更新时，比如从 A 结点更新，那么，数据的向量版本变成 [A:1, B:0, C:0]，然后向其他结点复制这个版本，其在语义上表示为我当前的数据是由 A 结果更新的，而在逻辑上则可以让分布式系统中的数据更新的顺序找到相关的因果关系。</p><p>这其中的逻辑关系，你可以看一下<a href="http://lass.cs.umass.edu/~shenoy/courses/spring05/lectures.html"> 马萨诸塞大学课程 Distributed Operating System </a>中第 10 节<a href="http://lass.cs.umass.edu/~shenoy/courses/spring05/lectures/Lec10.pdf"> Clock Synchronization </a>这篇讲议。关于 Vector Clock，你可以看一下<a href="http://basho.com/posts/technical/why-vector-clocks-are-easy/"> Why Vector Clocks are Easy</a>和<a href="http://basho.com/posts/technical/why-vector-clocks-are-hard/">Why Vector Clocks are Hard</a> 这两篇文章。</p><h4 id="Gossip-协议"><a href="#Gossip-协议" class="headerlink" title="Gossip 协议"></a>Gossip 协议</h4><p>另外，DynamoDB 中使用到了 Gossip 协议来做数据同步，这个协议的原始论文是 <a href="https://www.cs.cornell.edu/home/rvr/papers/flowgossip.pdf">Efficient Reconciliation and Flow Control for Anti-Entropy Protocols</a>。Gossip 算法也是 Cassandra 使用的数据复制协议。这个协议就像八卦和谣言传播一样，可以 “一传十、十传百”传播开来。但是这个协议看似简单，细节上却非常麻烦。</p><p>根据这篇论文，节点之间存在三种通信方式。</p><ul><li>push 方式。A 节点将数据 (key,value,version) 及对应的版本号推送给 B 节点，B 节点更新 A 中比自己新的数据。</li><li>pull 方式。A 仅将数据 key,version 推送给 B，B 将本地比 A 新的数据 (key,value,version) 推送给 A，A 更新本地。</li><li>push&#x2F;pull 方式。与 pull 类似，只是多了一步，A 再将本地比 B 新的数据推送给 B，B 更新本地。</li></ul><p>如果把两个节点数据同步一次定义为一个周期，那么在一个周期内，push 需通信 1 次，pull 需 2 次，push&#x2F;pull 则需 3 次。从效果上来讲，push&#x2F;pull 最好，理论上一个周期内可以使两个节点完全一致。直观感觉上，也是 push&#x2F;pull 的收敛速度最快。</p><p>另外，每个节点上的又需要一个协调机制，也就是如何交换数据能达到最快的一致性——消除节点的不一致性。上面所讲的 push、pull 等是通信方式，协调是在通信方式下的数据交换机制。</p><p>关于 Gossip 的一些图示化的东西，可以看一下动画<a href="https://rrmoelker.github.io/gossip-visualization/">gossip visualization</a>。</p><h4 id="分布式数据库方面"><a href="#分布式数据库方面" class="headerlink" title="分布式数据库方面"></a>分布式数据库方面</h4><p>数据库方面的一些论文。</p><p>一篇是 AWS Aurora 的论文 <a href="http://www.allthingsdistributed.com/files/p1041-verbitski.pdf">Amazon Aurora: Design Considerations for High Throughput Cloud –Native Relation Databases</a>。</p><p>Aurora 是 AWS 将 MySQL 的计算和存储分离后，计算节点 scale up，存储节点 scale out。并把其 redo log 独立设计成一个存储服务，把分布式的数据方面的东西全部甩给了底层存储系统。从而提高了整体的吞吐量和水平的扩展能力。</p><p>Aurora 要写 6 份拷贝，但是其只需要把一个 Quorum 中的日志写成功就可以了。如下所示。可以看到，将存储服务做成一个跨数据中心的服务，提高数据库容灾，降低性能影响。</p><p><img src="https://s2.loli.net/2022/08/28/G2eINobOhBxMFXu.png" alt="image-20220814125522918.png"></p><p>对于存储服务的设计，核心的原理就是 latency 一定要低，毕竟写 6 个 copy 是一件开销很大的事。所以，基本上来说，Aurora 用的是异步模型，然后拼命地做并行处理，其中用到的也是 Gossip 协议。如下所示。</p><p><img src="https://s2.loli.net/2022/08/28/8jlGWnqfyZHVTFx.png" alt="image-20220814125553446.png"></p><p>在上面这个图中，我们可以看到，完成前两步，就可以 ACK 回调用方。也就是说，只要数据在本地落地了，就可以返回成功了。然后，对于六个副本，这个 log 会同时发送到 6 个存储结点，只需要有大于 4 个成功 ACK，就算写成功了。第 4 步我们可以看到用的是 Gossip 协议。然后，第 5 步产生 cache 页，便于查询。第 6 步在 S3 做 Snapshot，类似于 Checkpoint。</p><h3 id="经典资料"><a href="#经典资料" class="headerlink" title="经典资料"></a>经典资料</h3><ul><li>Distributed systems theory for the distributed systems engineer</li><li>FLP Impossibility Result</li><li>An introduction to distributed systems</li><li>Distributed Systems for fun and profit</li><li>Distributed Systems: Principles and Paradigms</li><li>Scalable Web Architecture and Distributed Systems</li><li>Principles of Distributed Systems</li><li>Making reliable distributed systems in the presence of software errors</li><li>Designing Data Intensive Applications</li></ul>]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>分布式</tag>
      
      <tag>左耳听风</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RPC实战笔记</title>
    <link href="/2022/08/28/RPC%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/08/28/RPC%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="RPC总结"><a href="#RPC总结" class="headerlink" title="RPC总结"></a>RPC总结</h1><h2 id="RPC-的作用"><a href="#RPC-的作用" class="headerlink" title="RPC 的作用"></a>RPC 的作用</h2><ol><li><p>屏蔽远程调用跟本地调用的区别，让我们感觉就是调用项目内的方法；</p></li><li><p>隐藏底层网络通信的复杂性，让我们更专注于业务逻辑。</p></li></ol><h2 id="一个完整的-RPC-会涉及到哪些步骤？"><a href="#一个完整的-RPC-会涉及到哪些步骤？" class="headerlink" title="一个完整的 RPC 会涉及到哪些步骤？"></a>一个完整的 RPC 会涉及到哪些步骤？</h2><p><img src="https://static001.geekbang.org/resource/image/ac/fa/acf53138659f4982bbef02acdd30f1fa.jpg?wh=3846*1377" alt="img"></p><h2 id="RPC架构"><a href="#RPC架构" class="headerlink" title="RPC架构"></a>RPC架构</h2><p><img src="https://static001.geekbang.org/resource/image/30/fb/30f52b433aa5f103114a8420c6f829fb.jpg?wh=2951*2181" alt="img"></p><p>​                                                                                                                      <strong>核心功能体系</strong> </p><p><img src="https://static001.geekbang.org/resource/image/a3/a6/a3688580dccd3053fac8c0178cef4ba6.jpg?wh=3084*2183" alt="img"></p><p>​                                                                                                                  <strong>插件化体系架构</strong> </p><p><strong>插件化体系</strong>整个架构就变成了一个微内核架构，我们将每个功能点抽象成一个接口，将这个接口作为插件的契约，然后把这个功能的接口与功能的实现分离并提供接口的默认实现。这样的架构相比之前的架构，有很多优势。首先它的可扩展性很好，实现了开闭原则，用户可以非常方便地通过插件扩展实现自己的功能，而且不需要修改核心功能的本身；其次就是保持了核心包的精简，依赖外部包少，这样可以有效减少开发人员引入 RPC 导致的包版本冲突问题。</p><h2 id="RPC应用场景"><a href="#RPC应用场景" class="headerlink" title="RPC应用场景"></a>RPC应用场景</h2><p><img src="https://static001.geekbang.org/resource/image/50/be/506e902e06e91663334672c29bfbc2be.jpg?wh=3205*1778" alt="img"></p><h2 id="RPC注意点"><a href="#RPC注意点" class="headerlink" title="RPC注意点"></a>RPC注意点</h2><p><img src="https://s2.loli.net/2022/08/28/c19JKY4lWEjf3y7.png" alt="image-20220716134042279.png"></p><h2 id="RPC协议与HTTP的设计区别"><a href="#RPC协议与HTTP的设计区别" class="headerlink" title="RPC协议与HTTP的设计区别"></a>RPC协议与HTTP的设计区别</h2><p>相对于 HTTP 的用处，RPC 更多的是负责应用间的通信，所以性能要求相对更高。但 HTTP 协议的数据包大小相对请求数据本身要大很多，又需要加入很多无用的内容，比如换行符号、回车符等；还有一个更重要的原因是，HTTP 协议属于无状态协议，客户端无法对请求和响应进行关联，每次请求都需要重新建立连接，响应完成后再关闭连接。因此，对于要求高性能的 RPC 来说，HTTP 协议基本很难满足需求，所以 RPC 会选择设计更紧凑的私有协议。</p><h2 id="对象如何在网络中传输"><a href="#对象如何在网络中传输" class="headerlink" title="对象如何在网络中传输"></a>对象如何在网络中传输</h2><h3 id="序列化与反序列化"><a href="#序列化与反序列化" class="headerlink" title="序列化与反序列化"></a>序列化与反序列化</h3><p>网络传输的数据必须是二进制数据，但调用方请求的出入参数都是对象。对象是不能直接在网络中传输的，所以我们需要提前把它转成可传输的二进制，并且要求转换算法是可逆的，这个过程叫做“序列化”。这时，服务提供方就可以正确地从二进制数据中分割出不同的请求，同时根据请求类型和序列化类型，把二进制的消息体逆向还原成请求对象，这个过程称之为“反序列化”。</p><p><img src="https://static001.geekbang.org/resource/image/d2/04/d215d279ef8bfbe84286e81174b4e704.jpg" alt="img"></p><h3 id="RPC通信流程"><a href="#RPC通信流程" class="headerlink" title="RPC通信流程"></a>RPC通信流程</h3><p><img src="https://static001.geekbang.org/resource/image/82/59/826a6da653c4093f3dc3f0a833915259.jpg" alt="img"></p><h3 id="常见的序列化方式"><a href="#常见的序列化方式" class="headerlink" title="常见的序列化方式"></a>常见的序列化方式</h3><p><strong>JDK 原生序列化</strong></p><p>JDK序列化过程就是在读取对象数据的时候，不断加入一些特殊分隔符，这些特殊分隔符用于在反序列化过程中截断用。</p><p><img src="https://static001.geekbang.org/resource/image/7e/9f/7e2616937e3bc5323faf3ba4c09d739f.jpg" alt="img"></p><ul><li>头部数据用来声明序列化协议、序列化版本，用于高低版本向后兼容</li><li>对象数据主要包括类名、签名、属性名、属性类型及属性值，当然还有开头结尾等数据，除了属性值属于真正的对象值，其他都是为了反序列化用的元数据</li><li>存在对象引用、继承的情况下，就是递归遍历“写对象”逻辑</li></ul><p><strong>JSON序列化</strong></p><p><strong>缺点：</strong></p><p>JSON 进行序列化的额外空间开销比较大，对于大数据量服务这意味着需要巨大的内存和磁盘开销；JSON 没有类型，但像 Java 这种强类型语言，需要通过反射统一解决，所以性能不会太好。</p><p>JSON 没有类型，但像 Java 这种强类型语言，需要通过反射统一解决，所以性能不会太好。</p><p>所以如果 RPC 框架选用 JSON 序列化，服务提供者与服务调用者之间传输的数据量要相对较小，否则将严重影响性能。</p><p><strong>Hessian序列化</strong></p><p>Hessian 是动态类型、二进制、紧凑的，并且可跨语言移植的一种序列化框架。Hessian 协议要比 JDK、JSON 更加紧凑，性能上要比 JDK、JSON 序列化高效很多，而且生成的字节数也更小。</p><p><strong>Protobuf序列化</strong></p><p>Protobuf 是 Google 公司内部的混合语言数据标准，是一种轻便、高效的结构化数据存储格式，可以用于结构化数据序列化，支持 Java、Python、C++、Go 等语言。Protobuf 使用的时候需要定义 IDL（Interface description language），然后使用不同语言的 IDL 编译器，生成序列化工具类；</p><p><strong>优点：</strong></p><ul><li>序列化后体积相比 JSON、Hessian 小很多；</li><li>IDL 能清晰地描述语义，所以足以帮助并保证应用程序之间的类型不会丢失，无需类似 XML 解析器；</li><li>序列化反序列化速度很快，不需要通过反射获取类型；</li><li>消息格式升级和兼容性不错，可以做到向后兼容。</li></ul><p><strong>缺点：</strong>对于具有反射和动态能力的语言来说，用起来很费劲</p><h4 id="如何选择哪种框架"><a href="#如何选择哪种框架" class="headerlink" title="如何选择哪种框架"></a>如何选择哪种框架</h4><p><img src="https://static001.geekbang.org/resource/image/b4/a5/b42e44968c3fdcdfe2acf96377f5b2a5.jpg" alt="img"></p><h2 id="RPC-框架在使用时要注意哪些问题？"><a href="#RPC-框架在使用时要注意哪些问题？" class="headerlink" title="RPC 框架在使用时要注意哪些问题？"></a>RPC 框架在使用时要注意哪些问题？</h2><p><strong>对象构造得过于复杂：</strong>属性很多，并且存在多层的嵌套，比如 A 对象关联 B 对象，B 对象又聚合 C 对象，C 对象又关联聚合很多其他对象，对象依赖关系过于复杂。序列化框架在序列化与反序列化对象时，对象越复杂就越浪费性能，消耗 CPU，这会严重影响 RPC 框架整体的性能；另外，对象越复杂，在序列化与反序列化的过程中，出现问题的概率就越高。</p><p><strong>对象过于庞大：</strong>我经常遇到业务过来咨询，为啥他们的 RPC 请求经常超时，排查后发现他们的入参对象非常得大，比如为一个大 List 或者大 Map，序列化之后字节长度达到了上兆字节。这种情况同样会严重地浪费了性能、CPU，并且序列化一个如此大的对象是很耗费时间的，这肯定会直接影响到请求的耗时。</p><p><strong>使用序列化框架不支持的类作为入参类：</strong>比如 Hessian 框架，不支持 LinkedHashMap、LinkedHashSet 等，而且大多数情况下最好不要使用第三方集合类，如 Guava 中的集合类，很多开源的序列化框架都是优先支持编程语言原生的对象。因此如果入参是集合类，应尽量选用原生的、最为常用的集合类，如 HashMap、ArrayList。</p><p><strong>对象有复杂的继承关系：</strong>大多数序列化框架在序列化对象时都会将对象的属性一一进行序列化，当有继承关系时，会不停地寻找父类，遍历属性。就像问题 1 一样，对象关系越复杂，就越浪费性能，同时又很容易出现序列化上的问题。</p><p><img src="C:\Users\longp\AppData\Roaming\Typora\typora-user-images\image-20220717151659788.png" alt="image-20220717151659788"></p><h2 id="RPC主要实现功能"><a href="#RPC主要实现功能" class="headerlink" title="RPC主要实现功能"></a>RPC主要实现功能</h2><h3 id="服务发现"><a href="#服务发现" class="headerlink" title="服务发现"></a>服务发现</h3><p><strong>一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）要AP还是CP</strong></p><p><img src="https://static001.geekbang.org/resource/image/51/5d/514dc04df2b8b2f3130b7d44776a825d.jpg?wh=2746*1445" alt="img"></p><p>​                                                                                                                                          <strong>服务发现原理</strong></p><p><strong>服务注册：</strong>在服务提供方启动的时候，将对外暴露的接口注册到注册中心之中，注册中心将这个服务节点的 IP 和接口保存下来。</p><p><strong>服务订阅：</strong>在服务调用方启动的时候，去注册中心查找并订阅服务提供方的 IP，然后缓存到本地，并用于后续的远程调用。</p><h4 id="为什么不使用-DNS？"><a href="#为什么不使用-DNS？" class="headerlink" title="为什么不使用 DNS？"></a>为什么不使用 DNS？</h4><p><img src="https://static001.geekbang.org/resource/image/3b/18/3b6a23f392b9b8d6fcf31803a5b4ef18.jpg?wh=5273*1884" alt="img"></p><p>​                                                                                                             <strong>DNS查询流程</strong></p><p><strong>使用DNS存在的问题：</strong></p><ul><li>如果这个 IP 端口下线了，服务调用者不能及时摘除服务节点；</li><li>如果在之前已经上线了一部分服务节点，这时我突然对这个服务进行扩容，那么新上线的服务节点不能及时接收到流量；</li></ul><h4 id="基于-ZooKeeper-的服务发现"><a href="#基于-ZooKeeper-的服务发现" class="headerlink" title="基于 ZooKeeper 的服务发现"></a>基于 ZooKeeper 的服务发现</h4><p><img src="https://static001.geekbang.org/resource/image/50/75/503fabeeae226a722f83e9fb6c0d4075.jpg?wh=4214*1803" alt="img"></p><p><img src="https://s2.loli.net/2022/08/28/9rWbXJjnkFMGdgL.png" alt="实践.png"></p><h4 id="基于消息总线的最终一致性的注册中心"><a href="#基于消息总线的最终一致性的注册中心" class="headerlink" title="基于消息总线的最终一致性的注册中心"></a>基于消息总线的最终一致性的注册中心</h4><p>ZooKeeper 的一大特点就是强一致性，ZooKeeper 集群的每个节点的数据每次发生更新操作，都会通知其它 ZooKeeper 节点同时执行更新。它要求保证每个节点的数据能够实时的完全一致，这也就直接导致了 ZooKeeper 集群性能上的下降。这就好比几个人在玩传递东西的游戏，必须这一轮每个人都拿到东西之后，所有的人才能开始下一轮，而不是说我只要获得到东西之后，就可以直接进行下一轮了。</p><p>而 RPC 框架的服务发现，在服务节点刚上线时，服务调用方是可以容忍在一段时间之后（比如几秒钟之后）发现这个新上线的节点的。毕竟服务节点刚上线之后的几秒内，甚至更长的一段时间内没有接收到请求流量，对整个服务集群是没有什么影响的，<strong>所以我们可以牺牲掉 CP（强制一致性），而选择 AP（最终一致），来换取整个注册中心集群的性能和稳定性。</strong></p><p>是否有一种简单、高效，并且最终一致的更新机制，能代替 ZooKeeper 那种数据强一致的数据更新机制呢？</p><p>因为要求最终一致性，我们可以考虑采用消息总线机制。注册数据可以全量缓存在每个注册中心内存中，通过消息总线来同步数据。当有一个注册中心节点接收到服务节点注册时，会产生一个消息推送给消息总线，再通过消息总线通知给其它注册中心节点更新数据并进行服务下发，从而达到注册中心间数据最终一致性，具体流程如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/73/ff/73b59c7949ebed2903ede474856062ff.jpg?wh=4256*2276" alt="img"></p><ul><li>当有服务上线，注册中心节点收到注册请求，服务列表数据发生变化，会生成一个消息，推送给消息总线，每个消息都有整体递增的版本。</li><li>消息总线会主动推送消息到各个注册中心，同时注册中心也会定时拉取消息。对于获取到消息的在消息回放模块里面回放，只接受大于本地版本号的消息，小于本地版本号的消息直接丢弃，从而实现最终一致性。</li><li>消费者订阅可以从注册中心内存拿到指定接口的全部服务实例，并缓存到消费者的内存里面。</li><li>采用推拉模式，消费者可以及时地拿到服务实例增量变化情况，并和内存中的缓存数据进行合并。</li></ul><p>为了性能，采用两级缓存，注册中心和消费者的内存缓存，通过异步推拉模式来确保最终一致性。</p><p><img src="https://s2.loli.net/2022/08/28/OfNVTmrCbYEkohq.png" alt="image-20220720220716266.png"><br><img src="https://s2.loli.net/2022/08/28/g4dzHEyWfrpRhDK.png" alt="image-20220720220817173.png"></p><h3 id="健康检测"><a href="#健康检测" class="headerlink" title="健康检测"></a>健康检测</h3><p><strong>Script Check、HTTP Check、TCP Check、TTL Check等</strong></p><h4 id="consul做法"><a href="#consul做法" class="headerlink" title="consul做法"></a>consul做法</h4><p><strong>TTL&#x2F;TCP？</strong></p><h4 id="etcd做法？"><a href="#etcd做法？" class="headerlink" title="etcd做法？"></a>etcd做法？</h4><p><strong>基于lease租约机制，对注册的服务设置key TTL，定时保持服务的心跳以达到监控健康状态的效果。</strong></p><h3 id="路由策略"><a href="#路由策略" class="headerlink" title="路由策略"></a>路由策略</h3><p><img src="https://static001.geekbang.org/resource/image/b7/68/b78964a2db3adc8080364e9cfc79ca68.jpg?wh=3900*879" alt="img"></p><p>​                                                                                                                              <strong>调用流程</strong></p><p><img src="https://static001.geekbang.org/resource/image/23/f7/23f24c545d33ec4d6d72fc10e94a0ff7.jpg?wh=2513*1991" alt="img"></p><p>​                                                                                                                              <strong>IP路由调用拓扑</strong></p><h4 id="参数路由："><a href="#参数路由：" class="headerlink" title="参数路由："></a>参数路由：</h4><p><img src="https://static001.geekbang.org/resource/image/78/39/7868289c87ca9de144fe32fac98f8339.jpg?wh=2506*1964" alt="img"></p><p>​                                                                                                                 <strong>参数路由调用拓扑</strong></p><p>相比 IP 路由，参数路由支持的灰度粒度更小，他为服务提供方应用提供了另外一个服务治理的手段。灰度发布功能是 RPC 路由功能的一个典型应用场景，通过 RPC 路由策略的组合使用可以让服务提供方更加灵活地管理、调用自己的流量，进一步降低上线可能导致的风险。</p><h3 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h3><p><strong>需求：</strong></p><p><img src="https://s2.loli.net/2022/08/28/vAXShkqrxu8e2pK.png" alt="需求.png"></p><h4 id="什么是负载均衡？"><a href="#什么是负载均衡？" class="headerlink" title="什么是负载均衡？"></a>什么是负载均衡？</h4><p>当我们的一个服务节点无法支撑现有的访问量时，我们会部署多个节点，组成一个集群，然后通过负载均衡，将请求分发给这个集群下的每个服务节点，从而达到多个服务节点共同分担请求压力的目的。</p><p><img src="https://static001.geekbang.org/resource/image/f4/b8/f48704443b33df17fc490778c00c71b8.jpg?wh=3345*1443" alt="img"></p><p>​                                                                                                                    <strong>负载均衡示意图</strong></p><p>负载均衡主要分为软负载和硬负载，软负载就是在一台或多台服务器上安装负载均衡的软件，如 LVS、Nginx 等，硬负载就是通过硬件设备来实现的负载均衡，如 F5 服务器等。负载均衡的算法主要有随机法、轮询法、最小连接法等。</p><p>刚才介绍的负载均衡主要还是应用在 Web 服务上，Web 服务的域名绑定负载均衡的地址，通过负载均衡将用户的请求分发到一个个后端服务上。</p><h4 id="RPC-框架中的负载均衡"><a href="#RPC-框架中的负载均衡" class="headerlink" title="RPC 框架中的负载均衡"></a>RPC 框架中的负载均衡</h4><p><strong>RPC使用传统的负载均衡存在的问题？</strong></p><ol><li>搭建负载均衡设备或 TCP&#x2F;IP 四层代理，需要额外成本；</li><li>请求流量都经过负载均衡设备，多经过一次网络传输，会额外浪费一些性能；</li><li>负载均衡添加节点和摘除节点，一般都要手动添加，当大批量扩容和下线时，会有大量的人工操作，“服务发现”在操作上是个问题；</li><li>我们在服务治理的时候，针对不同接口服务、服务的不同分组，我们的负载均衡策略是需要可配的，如果大家都经过这一个负载均衡设备，就不容易根据不同的场景来配置不同的负载均衡策略了。</li></ol><p>RPC 的负载均衡完全由 RPC 框架自身实现，RPC 的服务调用者会与“注册中心”下发的所有服务节点建立长连接，在每次发起 RPC 调用时，服务调用者都会通过配置的负载均衡插件，自主选择一个服务节点，发起 RPC 调用请求。</p><p><img src="https://static001.geekbang.org/resource/image/5e/1c/5e294378a3d86e7d279507f62fe5ee1c.jpg?wh=4175*1969" alt="img"></p><p>​                                                                                                                      RPC框架负载均衡示意图</p><p>RPC 负载均衡策略一般包括随机权重、Hash、轮询。当然，这还是主要看 RPC 框架自身的实现。其中的随机权重策略应该是我们最常用的一种了，通过随机算法，我们基本可以保证每个节点接收到的请求流量是均匀的；同时我们还可以通过控制节点权重的方式，来进行流量控制。比如我们默认每个节点的权重都是 100，但当我们把其中的一个节点的权重设置成 50 时，它接收到的流量就是其他节点的 1&#x2F;2。</p><h4 id="如何设计自适应的负载均衡？"><a href="#如何设计自适应的负载均衡？" class="headerlink" title="如何设计自适应的负载均衡？"></a>如何设计自适应的负载均衡？</h4><p>RPC 的负载均衡完全由 RPC 框架自身实现，服务调用者发起请求时，会通过配置的负载均衡插件，自主地选择服务节点。那是不是只要调用者知道每个服务节点处理请求的能力，再根据服务处理节点处理请求的能力来判断要打给它多少流量就可以了？当一个服务节点负载过高或响应过慢时，就少给它发送请求，反之则多给它发送请求。这就有点像日常工作中的分配任务，要多考虑实际情况。当一位下属身体欠佳，就少给他些工作；若刚好另一位下属状态很好，手头工作又不是很多，就多分给他一点。</p><h5 id="服务调用者节点该如何判定一个服务节点的处理能力呢？"><a href="#服务调用者节点该如何判定一个服务节点的处理能力呢？" class="headerlink" title="服务调用者节点该如何判定一个服务节点的处理能力呢？"></a>服务调用者节点该如何判定一个服务节点的处理能力呢？</h5><p>采用一种打分的策略，服务调用者收集与之建立长连接的每个服务节点的指标数据，如服务节点的负载指标、CPU 核数、内存大小、请求处理的耗时指标（如请求平均耗时、TP99、TP999）、服务节点的状态指标（如正常、亚健康）。通过这些指标，计算出一个分数，比如总分 10 分，如果 CPU 负载达到 70%，就减它 3 分，当然了，减 3 分只是个类比，需要减多少分是需要一个计算策略的。</p><h5 id="该如果根据这些指标来打分呢？"><a href="#该如果根据这些指标来打分呢？" class="headerlink" title="该如果根据这些指标来打分呢？"></a>该如果根据这些指标来打分呢？</h5><p>这就有点像公司对员工进行年终考核。假设我是老板，我要考核专业能力、沟通能力和工作态度，这三项的占比分别是 30%、30%、40%，我给一个员工的评分是 10、8、8，那他的综合分数就是这样计算的：10<em>30%+8</em>30%+8*40%&#x3D;8.6 分。给服务节点打分也一样，我们可以为每个指标都设置一个指标权重占比，然后再根据这些指标数据，计算分数。</p><h5 id="服务调用者给每个服务节点都打完分之后，会发送请求，那这时候我们又该如何根据分数去控制给每个服务节点发送多少流量呢？"><a href="#服务调用者给每个服务节点都打完分之后，会发送请求，那这时候我们又该如何根据分数去控制给每个服务节点发送多少流量呢？" class="headerlink" title="服务调用者给每个服务节点都打完分之后，会发送请求，那这时候我们又该如何根据分数去控制给每个服务节点发送多少流量呢？"></a>服务调用者给每个服务节点都打完分之后，会发送请求，那这时候我们又该如何根据分数去控制给每个服务节点发送多少流量呢？</h5><p>我们可以配合随机权重的负载均衡策略去控制，通过最终的指标分数修改服务节点最终的权重。例如给一个服务节点综合打分是 8 分（满分 10 分），服务节点的权重是 100，那么计算后最终权重就是 80（100*80%）。服务调用者发送请求时，会通过随机权重的策略来选择服务节点，那么这个节点接收到的流量就是其他正常节点的 80%（这里假设其他节点默认权重都是 100，且指标正常，打分为 10 分的情况）。</p><p>整体的设计方案如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/00/af/00065674063f30c98caaa58bb4cd7baf.jpg?wh=4085*2365" alt="img"></p><p>​                                                                                                                      <strong>RPC自适应负载均衡示意图</strong></p><p><strong>关键步骤：</strong></p><ol><li>添加服务指标收集器，并将其作为插件，默认有运行时状态指标收集器、请求耗时指标收集器。</li><li>运行时状态指标收集器收集服务节点 CPU 核数、CPU 负载以及内存等指标，在服务调用者与服务提供者的心跳数据中获取。</li><li>请求耗时指标收集器收集请求耗时数据，如平均耗时、TP99、TP999 等。</li><li>可以配置开启哪些指标收集器，并设置这些参考指标的指标权重，再根据指标数据和指标权重来综合打分。</li><li>通过服务节点的综合打分与节点的权重，最终计算出节点的最终权重，之后服务调用者会根据随机权重的策略，来选择服务节点。</li></ol><p><strong>RPC 框架的负载均衡与 Web 服务的负载均衡的不同之处在于：</strong></p><p>RPC 框架并不是依赖一个负载均衡设备或者负载均衡服务器来实现负载均衡的，而是由 RPC 框架本身实现的，服务调用者可以自主选择服务节点，发起服务调用。这样的好处是，RPC 框架不再需要依赖专门的负载均衡设备，可以节约成本；还减少了与负载均衡设备间额外的网络传输，提升了传输效率；并且均衡策略可配，便于服务治理。</p><h3 id="异常重试与熔断限流"><a href="#异常重试与熔断限流" class="headerlink" title="异常重试与熔断限流"></a>异常重试与熔断限流</h3><h4 id="RPC重试机制"><a href="#RPC重试机制" class="headerlink" title="RPC重试机制"></a>RPC重试机制</h4><p><img src="https://static001.geekbang.org/resource/image/32/81/32441dc643e64a022acfcbe0b4c77e81.jpg?wh=5154*1923" alt="img"></p><p>​                                                                                                                       <strong>RPC异常重试流程</strong></p><p>调用端发起的请求失败时，RPC 框架自身可以进行重试，再重新发送请求，用户可以自行设置是否开启重试以及重试的次数。</p><p>调用端在发起 RPC 调用时，会经过负载均衡，选择一个节点，之后它会向这个节点发送请求信息。当消息发送失败或收到异常消息时，我们就可以捕获异常，根据异常触发重试，重新通过负载均衡选择一个节点发送请求消息，并且记录请求的重试次数，当重试次数达到用户配置的重试次数的时候，就返回给调用端动态代理一个失败异常，否则就一直重试下去。</p><p>RPC 框架的重试机制就是调用端发现请求失败时捕获异常，之后触发重试，那是不是所有的异常都要触发重试呢？当然不是了，因为这个异常可能是服务提供方抛回来的业务异常，它是应该正常返回给动态代理的，所以我们要在触发重试之前对捕获的异常进行判定，只有符合重试条件的异常才能触发重试，比如网络超时异常、网络连接异常等等。</p><p><strong>异常重试需要注意的问题：</strong></p><p>当网络突然抖动了一下导致请求超时了，但这个时候调用方的请求信息可能已经发送到服务提供方的节点上，也可能已经发送到服务提供方的服务节点上，那如果请求信息成功地发送到了服务节点上，那这个节点是不是就要执行业务逻辑了呢？是的。</p><p>如果该业务不是幂等，比如插入数据操作，那触发重试的话会不会引发问题呢？会的。</p><h4 id="如何在约定时间内安全可靠地重试？"><a href="#如何在约定时间内安全可靠地重试？" class="headerlink" title="如何在约定时间内安全可靠地重试？"></a>如何在约定时间内安全可靠地重试？</h4><p>RPC 框架是不会知道哪些业务异常能够去进行异常重试的，我们可以加个重试异常的白名单，用户可以将允许重试的异常加入到这个白名单中。当调用端发起调用，并且配置了异常重试策略，捕获到异常之后，我们就可以采用这样的异常处理策略。如果这个异常是 RPC 框架允许重试的异常，或者这个异常类型存在于可重试异常的白名单中，我们就允许对这个请求进行重试。</p><p><img src="https://static001.geekbang.org/resource/image/5e/81/5e5706e6fc02ef0caaee565ea358f281.jpg?wh=5129*2058" alt="img"></p><p>​                                                                                                                 <strong>可靠的异常重试机制</strong></p><h4 id="为什么需要自我保护"><a href="#为什么需要自我保护" class="headerlink" title="为什么需要自我保护"></a>为什么需要自我保护</h4><p>RPC 是解决分布式系统通信问题的一大利器，而分布式系统的一大特点就是高并发，所以说 RPC 也会面临高并发的场景。在这样的情况下，我们提供服务的每个服务节点就都可能由于访问量过大而引起一系列的问题，比如业务处理耗时过长、CPU 飘高、频繁 Full GC 以及服务进程直接宕机等等。但是在生产环境中，我们要保证服务的稳定性和高可用性，这时我们就需要业务进行自我保护，从而保证在高访问量、高并发的场景下，应用系统依然稳定，服务依然高可用。</p><h5 id="那么在使用-RPC-时，业务又如何实现自我保护呢？"><a href="#那么在使用-RPC-时，业务又如何实现自我保护呢？" class="headerlink" title="那么在使用 RPC 时，业务又如何实现自我保护呢？"></a>那么在使用 RPC 时，业务又如何实现自我保护呢？</h5><p>最常见的方式就是限流了，简单有效，但 RPC 框架的自我保护方式可不只有限流，并且 RPC 框架的限流方式可以是多种多样的。我们可以将 RPC 框架拆开来分析，RPC 调用包括服务端和调用端，调用端向服务端发起调用。下面分享一下服务端与调用端分别是如何进行自我保护的。</p><h4 id="服务端的自我保护"><a href="#服务端的自我保护" class="headerlink" title="服务端的自我保护"></a>服务端的自我保护</h4><p>举个例子，假如我们要发布一个 RPC 服务，作为服务端接收调用端发送过来的请求，这时服务端的某个节点负载压力过高了，我们该如何保护这个节点？</p><p><img src="https://static001.geekbang.org/resource/image/9b/17/9bae10ba8a5b96b03102fb9ef4f30e17.jpg?wh=2560*1315" alt="img"></p><p>那么就是限流吧？是的，<strong>在 RPC 调用中服务端的自我保护策略就是限流</strong>，那你有没有想过我们是如何实现限流的呢？是在服务端的业务逻辑中做限流吗？有没有更优雅的方式？</p><p>限流是一个比较通用的功能，我们可以在 RPC 框架中集成限流的功能，让使用方自己去配置限流阈值；我们还可以在服务端添加限流逻辑，当调用端发送请求过来时，服务端在执行业务逻辑之前先执行限流逻辑，如果发现访问量过大并且超出了限流的阈值，就让服务端直接抛回给调用端一个限流异常，否则就执行正常的业务逻辑。                                                </p><p><img src="https://static001.geekbang.org/resource/image/f8/ad/f8e8a4dd16f2fd2af366f810404057ad.jpg?wh=2563*1313" alt="img">                 </p><h5 id="服务端的限流逻辑该如何实现呢？"><a href="#服务端的限流逻辑该如何实现呢？" class="headerlink" title="服务端的限流逻辑该如何实现呢？"></a>服务端的限流逻辑该如何实现呢？</h5><p>计数器，平滑限流的滑动窗口、漏斗算法以及令牌桶算法等等</p><h4 id="调用端的自我保护"><a href="#调用端的自我保护" class="headerlink" title="调用端的自我保护"></a>调用端的自我保护</h4><p>举个例子，假如发布一个服务 B，而服务 B 又依赖服务 C，当一个服务 A 来调用服务 B 时，服务 B 的业务逻辑调用服务 C，而这时服务 C 响应超时了，由于服务 B 依赖服务 C，C 超时直接导致 B 的业务逻辑一直等待，而这个时候服务 A 在频繁地调用服务 B，服务 B 就可能会因为堆积大量的请求而导致服务宕机。</p><p><img src="https://static001.geekbang.org/resource/image/dc/31/dc2a18f1e2c495380cc4053b92ed3131.jpg?wh=2171*1472" alt="img"></p><p>由此可见，服务 B 调用服务 C，服务 C 执行业务逻辑出现异常时，会影响到服务 B，甚至可能会引起服务 B 宕机。这还只是 A-&gt;B-&gt;C 的情况，试想一下 A-&gt;B-&gt;C-&gt;D-&gt;……呢？在整个调用链中，只要中间有一个服务出现问题，都可能会引起上游的所有服务出现一系列的问题，甚至会引起整个调用链的服务都宕机，这是非常恐怖的。</p><p>所以说，在一个服务作为调用端调用另外一个服务时，为了防止被调用的服务出现问题而影响到作为调用端的这个服务，这个服务也需要进行自我保护。<strong>而最有效的自我保护方式就是熔断。</strong></p><p><strong>熔断机制:</strong></p><p><img src="https://static001.geekbang.org/resource/image/90/64/903fa4374beb753c1db8f1f8b82ff464.jpg?wh=2642*1990" alt="img"></p><p><strong>熔断器的工作机制主要是关闭、打开和半打开这三个状态之间的切换</strong>。</p><ol><li>在正常情况下，熔断器是关闭的；</li><li>当调用端调用下游服务出现异常时，熔断器会收集异常指标信息进行计算，当达到熔断条件时熔断器打开，这时调用端再发起请求是会直接被熔断器拦截，并快速地执行失败逻辑；</li><li>当熔断器打开一段时间后，会转为半打开状态，这时熔断器允许调用端发送一个请求给服务端，如果这次请求能够正常地得到服务端的响应，则将状态置为关闭状态，否则设置为打开。</li></ol><h5 id="在-RPC-框架中，该如何整合熔断器呢？"><a href="#在-RPC-框架中，该如何整合熔断器呢？" class="headerlink" title="在 RPC 框架中，该如何整合熔断器呢？"></a>在 RPC 框架中，该如何整合熔断器呢？</h5><p>熔断机制主要是保护调用端，调用端在发出请求的时候会先经过熔断器。我们可以回想下 RPC 的调用流程：</p><p><img src="https://static001.geekbang.org/resource/image/59/87/59b7479220a415ef034fb6edb589ec87.jpg?wh=3788*1350" alt="img"></p><p><strong>哪个步骤整合熔断器会比较合适呢？</strong></p><p>动态代理，因为在 RPC 调用的流程中，动态代理是 RPC 调用的第一个关口。在发出请求时先经过熔断器，如果状态是闭合则正常发出请求，如果状态是打开则执行熔断器的失败策略。</p><h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h4><h5 id="RPC-框架是如何实现业务的自我保护？"><a href="#RPC-框架是如何实现业务的自我保护？" class="headerlink" title="RPC 框架是如何实现业务的自我保护？"></a>RPC 框架是如何实现业务的自我保护？</h5><p><strong>服务端主要是通过限流来进行自我保护</strong>，我们在实现限流时要考虑到应用和 IP 级别，方便我们在服务治理的时候，对部分访问量特别大的应用进行合理的限流；服务端的限流阈值配置都是作用于单机的，而在有些场景下，例如对整个服务设置限流阈值，服务进行扩容时，限流的配置并不方便，我们可以在注册中心或配置中心下发限流阈值配置的时候，将总服务节点数也下发给服务节点，让 RPC 框架自己去计算限流阈值；我们还可以让 RPC 框架的限流模块依赖一个专门的限流服务，对服务设置限流阈值进行精准地控制，但是这种方式依赖了限流服务，相比单机的限流方式，在性能和耗时上有劣势。</p><p><strong>调用端可以通过熔断机制进行自我保护</strong>，防止调用下游服务出现异常，或者耗时过长影响调用端的业务逻辑，RPC 框架可以在动态代理的逻辑中去整合熔断器，实现 RPC 框架的熔断功能。</p><h5 id="服务保护一般就是限流、熔断、降级。"><a href="#服务保护一般就是限流、熔断、降级。" class="headerlink" title="服务保护一般就是限流、熔断、降级。"></a>服务保护一般就是限流、熔断、降级。</h5><p> 限流的落地方式有：Guava RateLimiter、lua+Redis、Sentinel等； 熔断：Hystrix、Resilience4j； 降级：服务降级，就是对不怎么重要的服务进行低优先级的处理。说白了，就是尽可能的把系统资源让给优先级高的服务。资源有限，而请求是无限的。</p><h4 id="业务分组"><a href="#业务分组" class="headerlink" title="业务分组"></a>业务分组</h4><p>通过分组的方式人为地给不同的调用方划分出不同的小集群，从而实现调用方流量隔离的效果，保障我们的核心业务不受非核心业务的干扰。但我们在考虑问题的时候，不能顾此失彼，不能因为新加一个的功能而影响到原有系统的稳定性。</p><h4 id="实践案例："><a href="#实践案例：" class="headerlink" title="实践案例："></a>实践案例：</h4><p><img src="https://s2.loli.net/2022/08/28/KPjgxwin6sIVuby.png" alt="实现.png"></p><h3 id="RPC服务重启的关闭与开启"><a href="#RPC服务重启的关闭与开启" class="headerlink" title="RPC服务重启的关闭与开启"></a>RPC服务重启的关闭与开启</h3><h4 id="在重启服务的过程中，RPC-怎么做到让调用方系统不出问题呢？"><a href="#在重启服务的过程中，RPC-怎么做到让调用方系统不出问题呢？" class="headerlink" title="在重启服务的过程中，RPC 怎么做到让调用方系统不出问题呢？"></a>在重启服务的过程中，RPC 怎么做到让调用方系统不出问题呢？</h4><p>简述下上线的大概流程：当服务提供方要上线的时候，一般是通过部署系统完成实例重启。在这个过程中，服务提供方的团队并不会事先告诉调用方他们需要操作哪些机器，从而让调用方去事先切走流量。而对调用方来说，它也无法预测到服务提供方要对哪些机器重启上线，因此负载均衡就有可能把要正在重启的机器选出来，这样就会导致把请求发送到正在重启中的机器里面，从而导致调用方不能拿到正确的响应结果。</p><p><img src="https://static001.geekbang.org/resource/image/c8/67/c899c36097fd5e3f70bf031f4b2c2167.jpg?wh=3596*1810" alt="img"></p><p><strong>在服务重启的时候，对于调用方来说，这时候可能会存在以下几种情况：</strong></p><ul><li>调用方发请求前，目标服务已经下线。对于调用方来说，跟目标节点的连接会断开，这时候调用方可以立马感知到，并且在其健康列表里面会把这个节点挪掉，自然也就不会被负载均衡选中。</li><li>调用方发请求的时候，目标服务正在关闭，但调用方并不知道它正在关闭，而且两者之间的连接也没断开，所以这个节点还会存在健康列表里面，因此该节点就有一定概率会被负载均衡选中。</li></ul><h4 id="关闭流程"><a href="#关闭流程" class="headerlink" title="关闭流程"></a>关闭流程</h4><p><strong>通常的关闭流程：</strong></p><p><img src="https://static001.geekbang.org/resource/image/a1/50/a15be58b32195422bd5a18dba0e68050.jpg?wh=3195*1277" alt="img"></p><p>如上图所示，整个关闭过程中依赖了两次 RPC 调用，一次是服务提供方通知注册中心下线操作，一次是注册中心通知服务调用方下线节点操作。注册中心通知服务调用方都是异步的，我们在“服务发现”一讲中讲过在大规模集群里面，服务发现只保证最终一致性，并不保证实时性，所以注册中心在收到服务提供方下线的时候，并不能成功保证把这次要下线的节点推送到所有的调用方。</p><p>所以这么来看，通过服务发现并不能做到应用无损关闭。不能强依赖“服务发现”来通知调用方要下线的机器，那服务提供方自己来通知行不行？因为在 RPC 里面调用方跟服务提供方之间是长连接，我们可以在提供方应用内存里面维护一份调用方连接集合，当服务要关闭的时候，挨个去通知调用方去下线这台机器。这样整个调用链路就变短了，对于每个调用方来说就一次 RPC，可以确保调用的成功率很高。大部分场景下，这么做确实没有问题，我们之前也是这么实现的，但是我们发现线上还是会偶尔会出现，因为服务提供方上线而导致调用失败的问题。</p><h4 id="优雅关闭"><a href="#优雅关闭" class="headerlink" title="优雅关闭"></a>优雅关闭</h4><p>因为服务提供方已经开始进入关闭流程，那么很多对象就可能已经被销毁了，关闭后再收到的请求按照正常业务请求来处理，肯定是没法保证能处理的。所以我们可以在关闭的时候，设置一个请求“挡板”，挡板的作用就是告诉调用方，我已经开始进入关闭流程了，我不能再处理你这个请求了。</p><p><strong>举例：</strong>如果大家经常去银行办理业务，就会很熟悉这个流程。在交接班或者有其他要事情处理的时候，银行柜台工作人员会拿出一个纸板，放在窗口前，上面写到“该窗口已关闭”。在该窗口排队的人虽然有一万个不愿意，也只能换到其它窗口办理业务，因为柜台工作人员会把当前正在办理的业务处理完后正式关闭窗口。</p><p>基于这个思路，我们可以这么处理：当服务提供方正在关闭，如果这之后还收到了新的业务请求，服务提供方直接返回一个特定的异常给调用方（比如 ShutdownException）。这个异常就是告诉调用方“我已经收到这个请求了，但是我正在关闭，并没有处理这个请求”，然后调用方收到这个异常响应后，RPC 框架把这个节点从健康列表挪出，并把请求自动重试到其他节点，因为这个请求是没有被服务提供方处理过，所以可以安全地重试到其他节点，这样就可以实现对业务无损。</p><p>但如果只是靠等待被动调用，就会让这个关闭过程整体有点漫长。因为有的调用方那个时刻没有业务请求，就不能及时地通知调用方了，所以我们可以加上主动通知流程，这样既可以保证实时性，也可以避免通知失败的情况。</p><p><strong>怎么捕获到关闭事件呢？</strong></p><p>通过捕获操作系统的进程信号来获取，在 Java 语言里面，对应的是 Runtime.addShutdownHook 方法，可以注册关闭的钩子。在 RPC 启动的时候，我们提前注册关闭钩子，并在里面添加了两个处理程序，一个负责开启关闭标识，一个负责安全关闭服务对象，服务对象在关闭的时候会通知调用方下线节点。同时需要在我们调用链里面加上挡板处理器，当新的请求来的时候，会判断关闭标识，如果正在关闭，则抛出特定异常。</p><p><strong>关闭过程中已经在处理的请求会不会受到影响呢？</strong></p><p>如果进程结束过快会造成这些请求还没有来得及应答，同时调用方会也会抛出异常。为了尽可能地完成正在处理的请求，首先我们要把这些请求识别出来。</p><p>这就好比日常生活中，我们经常看见停车场指示牌上提示还有多少剩余车位，这个是如何做到的呢？如果仔细观察一下，你就会发现它是每进入一辆车，剩余车位就减一，每出来一辆车，剩余车位就加一。我们也可以利用这个原理在服务对象加上引用计数器，每开始处理请求之前加一，完成请求处理减一，通过该计数器我们就可以快速判断是否有正在处理的请求。</p><p>服务对象在关闭过程中，会拒绝新的请求，同时根据引用计数器等待正在处理的请求全部结束之后才会真正关闭。但考虑到有些业务请求可能处理时间长，或者存在被挂住的情况，为了避免一直等待造成应用无法正常退出，我们可以在整个 ShutdownHook 里面，加上超时时间控制，当超过了指定时间没有结束，则强制退出应用。超时时间我建议可以设定成 10s，基本可以确保请求都处理完了。整个流程如下图所示。</p><p><img src="https://static001.geekbang.org/resource/image/77/cc/7752081ec658f1d56ac4219f1c07fbcc.jpg?wh=3131*2891" alt="img"></p><h4 id="关闭总结"><a href="#关闭总结" class="headerlink" title="关闭总结"></a>关闭总结</h4><p>在 RPC 里面，关闭虽然看似不属于 RPC 主流程，但如果我们不能处理得很好的话，可能就会导致调用方业务异常，从而需要我们加入很多额外的运维工作。一个好的关闭流程，可以确保使用我们框架的业务实现平滑的上下线，而不用担心重启导致的问题。</p><p>“优雅关闭”这个概念除了在 RPC 里面有，在很多框架里面也都挺常见的，比如像我们经常用的应用容器框架 Tomcat。Tomcat 关闭的时候也是先从外层到里层逐层进行关闭，先保证不接收新请求，然后再处理关闭前收到的请求。</p><h5 id="相关解释："><a href="#相关解释：" class="headerlink" title="相关解释："></a>相关解释：</h5><p><img src="https://s2.loli.net/2022/08/28/qUCtZw2Ps7EdYDu.png" alt="image-20220723212313198.png"></p><h4 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h4><h5 id="启动预热"><a href="#启动预热" class="headerlink" title="启动预热"></a>启动预热</h5><p>让刚启动的服务提供方应用不承担全部的流量，而是让它被调用的次数随着时间的移动慢慢增加，最终让流量缓和地增加到跟已经运行一段时间后的水平一样。</p><p><strong>实现：</strong>我们可以先简单地回顾下调用方发起的 RPC 调用流程是怎样的，调用方应用通过服务发现能够获取到服务提供方的 IP 地址，然后每次发送请求前，都需要通过负载均衡算法从连接池中选择一个可用连接。那这样的话，我们是不是就可以让负载均衡在选择连接的时候，区分一下是否是刚启动不久的应用？对于刚启动的应用，我们可以让它被选择到的概率特别低，但这个概率会随着时间的推移慢慢变大，从而实现一个动态增加流量的过程。</p><p>当服务提供方运行时长小于预热时间时，对服务提供方进行降权，减少被负载均衡选择的概率，避免让应用在启动之初就处于高负载状态，从而实现服务提供方在启动后有一个预热的过程。</p><p><img src="https://static001.geekbang.org/resource/image/e7/d4/e796da8cf26f056479a59fd97b43d0d4.jpg?wh=2558*2523" alt="img"></p><p>​                                                                                                                   <strong>预热过程图</strong></p><p>启动预热更多是从调用方的角度出发，去解决服务提供方应用冷启动的问题，让调用方的请求量通过一个时间窗口过渡，慢慢达到一个正常水平，从而实现平滑上线。但对于服务提供方本身来说，有没有相关方案可以实现这种效果呢？</p><h5 id="延迟暴露"><a href="#延迟暴露" class="headerlink" title="延迟暴露"></a>延迟暴露</h5><p>举例：spring应用启动的时候都是通过 main 入口，然后顺序加载各种相关依赖的类。以 Spring 应用启动为例，在加载的过程中，Spring 容器会顺序加载 Spring Bean，如果某个 Bean 是 RPC 服务的话，我们不光要把它注册到 Spring-BeanFactory 里面去，还要把这个 Bean 对应的接口注册到注册中心。注册中心在收到新上线的服务提供方地址的时候，会把这个地址推送到调用方应用内存中；当调用方收到这个服务提供方地址的时候，就会去建立连接发请求。</p><p>但这时候可能存在服务提供方并没有启动完成的情况？因为服务提供方应用可能还在加载其它的 Bean。对于调用方来说，只要获取到了服务提供方的 IP，就有可能发起 RPC 调用，但如果这时候服务提供方没有启动完成的话，就会导致调用失败，从而使业务受损。</p><p>解决方案：</p><ul><li><p>在应用启动加载、解析 Bean 的时候，如果遇到了 RPC 服务的 Bean，只先把这个 Bean 注册到 Spring-BeanFactory 里面去，而并不把这个 Bean 对应的接口注册到注册中心，只有等应用启动完成后，才把接口注册到注册中心用于服务发现，从而实现让服务调用方延迟获取到服务提供方地址。这样是可以保证应用在启动完后才开始接入流量的，但其实这样做，我们还是没有实现最开始的目标。因为这时候应用虽然启动完成了，但并没有执行相关的业务代码，所以 JVM 内存里面还是冷的。如果这时候大量请求过来，还是会导致整个应用在高负载模式下运行，从而导致不能及时地返回请求结果。而且在实际业务中，一个服务的内部业务逻辑一般会依赖其它资源的，比如缓存数据。如果我们能在服务正式提供服务前，先完成缓存的初始化操作，而不是等请求来了之后才去加载，我们就可以降低重启后第一次请求出错的概率。</p></li><li><p>利用服务提供方把接口注册到注册中心的那段时间。我们可以在服务提供方应用启动后，接口注册到注册中心前，预留一个 Hook 过程，让用户可以实现可扩展的 Hook 逻辑。用户可以在 Hook 里面模拟调用逻辑，从而使 JVM 指令能够预热起来，并且用户也可以在 Hook 里面事先预加载一些资源，只有等所有的资源都加载完成后，最后才把接口注册到注册中心。整个应用启动过程如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/3c/bd/3c84f9cf6745f2d50e34bd8431c84abd.jpg?wh=3374*893" alt="img"></p></li></ul><p>​                                                                                                                <strong>启动顺序图</strong></p><h5 id="相关解释：-1"><a href="#相关解释：-1" class="headerlink" title="相关解释："></a>相关解释：</h5><p><img src="https://s2.loli.net/2022/08/28/HhWuQ9j2KxJ3Ayt.png" alt="image-20220723225147106.png"></p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p><img src="https://s2.loli.net/2022/08/28/FZ1Tlig6GQbLCvE.png" alt="image-20220724150651258.png"></p><p><img src="https://s2.loli.net/2022/08/28/tT46nkNsdzK9Vmw.png" alt="image-20220724150622961.png"></p><h4 id="分布式场景中如何做到快速定位RPC相关问题？"><a href="#分布式场景中如何做到快速定位RPC相关问题？" class="headerlink" title="分布式场景中如何做到快速定位RPC相关问题？"></a>分布式场景中如何做到快速定位RPC相关问题？</h4><p>在分布式的生产环境中，比如下面这个场景：我们搭建了一个分布式的应用系统，在这个应用系统中，我启动了 4 个子服务，分别是服务 A、服务 B、服务 C 与服务 D，而这 4 个服务的依赖关系是 A-&gt;B-&gt;C-&gt;D，而这些服务又都部署在不同的机器上。在 RPC 调用中，如果服务端的业务逻辑出现了异常，就会把异常抛回给调用端，那么如果现在这个调用链中有一个服务出现了异常，我们该如何定位问题呢？</p><h5 id="方法-1：借助合理封装的异常信息"><a href="#方法-1：借助合理封装的异常信息" class="headerlink" title="方法 1：借助合理封装的异常信息"></a>方法 1：借助合理封装的异常信息</h5><p><img src="https://static001.geekbang.org/resource/image/b8/1b/b8fee37688d39ae7913429f6cbc06f1b.jpg?wh=4498*1228" alt="img"></p><h5 id="方法-2：借助分布式链路跟踪"><a href="#方法-2：借助分布式链路跟踪" class="headerlink" title="方法 2：借助分布式链路跟踪"></a>方法 2：借助分布式链路跟踪</h5><p><img src="https://s2.loli.net/2022/08/28/ldO1BCDz3xqs7ay.png" alt="image-20220724163935463.png"></p><h4 id="流量回放"><a href="#流量回放" class="headerlink" title="流量回放"></a>流量回放</h4><p><img src="https://s2.loli.net/2022/08/28/Cq24cT78wnWaYRz.png" alt="image-20220724170611177.png"></p>]]></content>
    
    
    <categories>
      
      <category>RPC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>RPC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Golang内存分配和垃圾回收</title>
    <link href="/2022/08/27/Golang%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%92%8C%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"/>
    <url>/2022/08/27/Golang%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%92%8C%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="Golang内存管理和垃圾回收"><a href="#Golang内存管理和垃圾回收" class="headerlink" title="Golang内存管理和垃圾回收"></a>Golang内存管理和垃圾回收</h1><p>现代高级编程语言管理内存的方式分自动和手动两种。手动管理内存的典型代表是C和C++，编写代码过程中需要主动申请或者释放内存；而PHP、Java 和Go等语言使用自动的内存管理系统，由内存分配器和垃圾收集器来代为分配和回收内存，其中垃圾收集器就是我们常说的GC。今天腾讯后台开发工程师汪汇向大家分享 Golang 垃圾回收算法。（当然，Rust 是另一种）</p><p>从Go v1.12版本开始，Go使用了<strong>非分代的、并发的、基于三色标记清除的垃圾回收器</strong>。相关标记清除算法可以参考C&#x2F;C++，而Go是一种静态类型的编译型语言。因此，Go不需要VM，Go应用程序二进制文件中嵌入了一个小型运行时(Go runtime)，可以处理诸如垃圾收集(GC)、调度和并发之类的语言功能。首先让我们看一下Go内部的内存管理是什么样子的。</p><h2 id="一、-Golang内存管理"><a href="#一、-Golang内存管理" class="headerlink" title="一、 Golang内存管理"></a><strong>一、 Golang内存管理</strong></h2><p>这里先简单介绍一下 Golang 运行调度。在 Golang 里面有三个基本的概念：G, M, P。</p><ul><li>G: Goroutine 执行的上下文环境。</li><li>M: 操作系统线程。</li><li>P: Processer。进程调度的关键，调度器，也可以认为约等于CPU。</li></ul><p>一个 Goroutine 的运行需要G+P+M三部分结合起来。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/VY8SELNGe94Cjxng5VbT4M7FkUgyAfhpQu2e0dr5z6Za2b2aIw9peb8icIQyc29bC7VNuYfPh81ibaUdoSJg6ibicw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p> 图源：《Golang—内存管理(内存分配)》</p><p>(<a href="http://t.zoukankan.com/zpcoding-p-13259943.html">http://t.zoukankan.com/zpcoding-p-13259943.html</a>)</p><h3 id="（一）TCMalloc"><a href="#（一）TCMalloc" class="headerlink" title="（一）TCMalloc"></a><strong>（一）TCMalloc</strong></h3><p>Go将内存划分和分组为页（Page），这和Java的内存结构完全不同，没有分代内存，这样的原因是Go的内存分配器采用了TCMalloc的<strong>设计思想</strong>：</p><h4 id="1-Page"><a href="#1-Page" class="headerlink" title="1.Page"></a><strong>1.Page</strong></h4><p>与TCMalloc中的Page相同，x64下1个Page的大小是8KB。上图的最下方，1个浅蓝色的长方形代表1个Page。</p><h4 id="2-Span"><a href="#2-Span" class="headerlink" title="2.Span"></a><strong>2.Span</strong></h4><p>与TCMalloc中的Span相同，Span是内存管理的基本单位，代码中为mspan，一组连续的Page组成1个Span，所以上图一组连续的浅蓝色长方形代表的是一组Page组成的1个Span，另外，1个淡紫色长方形为1个Span。</p><h4 id="3-mcache"><a href="#3-mcache" class="headerlink" title="3.mcache"></a><strong>3.mcache</strong></h4><p>mcache是提供给P（逻辑处理器）的高速缓存，用于存储小对象（对象大小&lt;&#x3D; 32Kb）。尽管这类似于线程堆栈，但它是堆的一部分，用于动态数据。所有类大小的mcache包含scan和noscan类型mspan。Goroutine可以从mcache没有任何锁的情况下获取内存，因为一次P只能有一个锁G。因此，这更有效。mcache从mcentral需要时请求新的span。</p><h4 id="4-mcentral"><a href="#4-mcentral" class="headerlink" title="4.mcentral"></a><strong>4.mcentral</strong></h4><p>mcentral与TCMalloc中的CentralCache类似，是所有线程共享的缓存，需要加锁访问，它按Span class对Span分类，串联成链表，当mcache的某个级别Span的内存被分配光时，它会向mcentral申请1个当前级别的Span。每个mcentral包含两个mspanList：</p><ul><li>empty：双向span链表，包括没有空闲对象的span或缓存mcache中的span。当此处的span被释放时，它将被移至non-empty span链表。</li><li>non-empty：有空闲对象的span双向链表。当从mcentral请求新的span，mcentral将从该链表中获取span并将其移入empty span链表。</li></ul><h4 id="5-mheap"><a href="#5-mheap" class="headerlink" title="5.mheap"></a><strong>5.mheap</strong></h4><p>mheap与TCMalloc中的PageHeap类似，它是堆内存的抽象，也是垃圾回收的重点区域，把从OS申请出的内存页组织成Span，并保存起来。当mcentral的Span不够用时会向mheap申请，mheap的Span不够用时会向OS申请，向OS的内存申请是按页来的，然后把申请来的内存页生成Span组织起来，同样也是需要加锁访问的。</p><h4 id="6-栈"><a href="#6-栈" class="headerlink" title="6.栈"></a><strong>6.栈</strong></h4><p>这是栈存储区，每个Goroutine（G）有一个栈。在这里存储了静态数据，包括函数栈帧，静态结构，原生类型值和指向动态结构的指针。这与分配给每个P的mcache不是一回事。</p><h3 id="7-TCMalloc为什么快："><a href="#7-TCMalloc为什么快：" class="headerlink" title="7.TCMalloc为什么快："></a>7.TCMalloc为什么快：</h3><p>1.使用了thread cache（线程cache），小块的内存分配都可以从cache中分配，这样再多线程分配内存的情况下，可以减少锁竞争。</p><p>2.tcmalloc会为每个线程分配本地缓存，小对象请求可以直接从本地缓存获取，如果没有空闲内存，则从central heap中一次性获取一连串小对象。大对象是直接使用页级分配器（page-level allocator）从Central page Heap中进行分配，即一个大对象总是按页对齐的。tcmalloc对于小内存，按8的整数次倍分配，对于大内存，按4K的整数次倍分配。</p><p>3.当某个线程缓存中所有对象的总大小超过2MB的时候，会进行垃圾收集。垃圾收集阈值会自动根据线程数量的增加而减少，这样就不会因为程序有大量线程而过度浪费内存。</p><p>4.tcmalloc为每个线程分配一个thread-local cache，小对象的分配直接从thread-local cache中分配。根据需要将对象从CentralHeap中移动到thread-local cache，同时定期的用垃圾回收器把内存从thread-local cache回收到Central free list中。</p><h3 id="8-总结"><a href="#8-总结" class="headerlink" title="8.总结"></a>8.总结</h3><p>ThreadCache（用于小对象分配）：线程本地缓存，每个线程独立维护一个该对象，多线程在并发申请内存时不会产生锁竞争。</p><p>CentralCache（Central free list，用于小对象分配）：全局cache，所有线程共享。当thread cache空闲链表为空时，会批量从CentralCache中申请内存；当thread cache总内存超过阈值，会进行内存垃圾回收，将空闲内存返还给CentralCache。</p><p>Page Heap（小&#x2F;大对象）：全局页堆，所有线程共享。对于小对象，当centralcache为空时，会从page heap中申请一个span；当一个span完全空闲时，会将该span返还给page heap。对于大对象，直接从page heap中分配，用完直接返还给page heap。系统内存：当page cache内存用光后，会通过sbrk、mmap等系统调用向OS申请内存。</p><h3 id="（二）内存分配"><a href="#（二）内存分配" class="headerlink" title="（二）内存分配"></a><strong>（二）内存分配</strong></h3><p>Go 中的内存分类并不像TCMalloc那样分成小、中、大对象，但是它的小对象里又细分了一个Tiny对象，Tiny对象指大小在1Byte到16Byte之间并且不包含指针的对象。小对象和大对象只用大小划定，无其他区分。</p><p><strong>核心思想</strong>：把内存分为多级管理，降低锁的粒度(只是去mcentral和mheap会申请锁), 以及多种对象大小类型，减少分配产生的内存碎片。</p><ul><li>*<em>*微小对象(Tiny)（size&lt;16B*</em>*<em>）*</em>*</li></ul><p>使用mcache的微小分配器分配小于16个字节的对象，并且在单个16字节块上可完成多个微小分配。</p><ul><li><em><strong>*小对象（尺寸16B〜32KB）*</strong></em></li></ul><p>大小在16个字节和32k字节之间的对象被分配在G运行所在的P的mcache的对应的mspan size class上。</p><ul><li><em><strong>*大对象（大小&gt;32KB）*</strong></em></li></ul><p>大于32 KB的对象直接分配在mheap的相应大小类上(size class)。</p><ul><li>如果mheap为空或没有足够大的页面满足分配请求，则它将从操作系统中分配一组新的页（至少1MB）。</li><li>如果对应的大小规格在mcache中没有可用的块，则向mcentral申请。</li><li>如果mcentral中没有可用的块，则向mheap申请，并根据BestFit 算法找到最合适的mspan。如果申请到的mspan超出申请大小，将会根据需求进行切分，以返回用户所需的页数。剩余的页构成一个新的mspan放回mheap的空闲列表。</li><li>如果mheap中没有可用span，则向操作系统申请一系列新的页（最小 1MB）。Go 会在操作系统分配超大的页（称作arena）。分配一大批页会减少和操作系统通信的成本。</li></ul><h3 id="（三）内存回收"><a href="#（三）内存回收" class="headerlink" title="（三）内存回收"></a><strong>（三）内存回收</strong></h3><p>go内存会分成堆区（Heap）和栈区（Stack）两个部分，程序在运行期间可以主动从堆区申请内存空间，这些内存由内存分配器分配并由垃圾收集器负责回收。栈区的内存由编译器自动进行分配和释放，栈区中存储着函数的参数以及局部变量，它们会随着函数的创建而创建，函数的返回而销毁。如果只申请和分配内存，内存终将枯竭。Go使用垃圾回收收集不再使用的span，把span释放交给mheap，mheap对span进行span的合并，把合并后的span加入scav树中，等待再分配内存时，由mheap进行内存再分配。<strong>因此，Go堆是Go垃圾收集器管理的主要区域</strong>。</p><h2 id="二、常见的GC算法"><a href="#二、常见的GC算法" class="headerlink" title="二、常见的GC算法"></a>二、常见的GC算法</h2><h3 id="引用计数法"><a href="#引用计数法" class="headerlink" title="引用计数法"></a>引用计数法</h3><p>根据对象自身的引用计数来回收，当引用计数归零时进行回收，但是计数频繁更新会带来更多开销，且无法解决循环引用的问题。</p><ul><li>优点：简单直接，回收速度快</li><li>缺点：需要额外的空间存放计数，无法处理循环引用的情况；</li></ul><h3 id="可达性分析"><a href="#可达性分析" class="headerlink" title="可达性分析"></a>可达性分析</h3><p>对象引用链：通过一系列的称为”GCRoots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链(Reference Chain) ，如果一个对象到GCRoots没有任何引用链相连，或者用图论的话来说，就是，从GCRoots到这个对象不可达时，则证明此对象是不可用的。</p><p>根对象在垃圾回收的术语中又叫做根集合，它是垃圾回收器在标记过程时最先检查的对象，包括：</p><p>1.全局变量：程序在编译期就能确定的那些存在于程序整个生命周期的变量。</p><p>2.执行栈：每个 goroutine 都包含自己的执行栈，这些执行栈上包含栈上的变量及指向分配的堆内存区块的指针。</p><p>3.寄存器：寄存器的值可能表示一个指针，参与计算的这些指针可能指向某些赋值器分配的堆内存区块。<br><img src="https://img-blog.csdnimg.cn/20200801163955410.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyMDA5MjYy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="标记清除法"><a href="#标记清除法" class="headerlink" title="标记清除法"></a>标记清除法</h3><p>标记出所有不需要回收的对象，在标记完成后统一回收掉所有未被标记的对象。<br><img src="https://liangyaopei.github.io/2021/01/02/golang-gc-intro/mark_clean.png" alt="mark_clean"></p><ul><li>优点：简单直接，速度快，适合可回收对象不多的场景</li><li>缺点：会造成不连续的内存空间（内存碎片），导致有大的对象创建的时候，明明内存中总内存是够的，但是空间不是连续的造成对象无法分配；</li></ul><h3 id="复制法"><a href="#复制法" class="headerlink" title="复制法"></a>复制法</h3><p>复制法将内存分为大小相同的两块，每次使用其中的一块，当这一块的内存使用完后，将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉<br><img src="https://liangyaopei.github.io/2021/01/02/golang-gc-intro/copy_method.png" alt="copy_method"></p><ul><li>优点：解决了内存碎片的问题，每次清除针对的都是整块内存，但是因为移动对象需要耗费时间，效率低于标记清除法；</li><li>缺点：有部分内存总是利用不到，资源浪费，移动存活对象比较耗时，并且如果存活对象较多的时候，需要担保机制确保复制区有足够的空间可完成复制；</li></ul><h3 id="标记整理"><a href="#标记整理" class="headerlink" title="标记整理"></a>标记整理</h3><p>标记过程同标记清除法，结束后将存活对象压缩至一端，然后清除边界外的内容<br><img src="https://liangyaopei.github.io/2021/01/02/golang-gc-intro/mark_tidy.png" alt="mark_tidy"></p><ul><li>优点：解决了内存碎片的问题，也不像标记复制法那样需要担保机制，存活对象较多的场景也使适用；</li><li>缺点：性能低，因为在移动对象的时候不仅需要移动对象还要维护对象的引用地址，可能需要对内存经过几次扫描才能完成；</li></ul><h3 id="分代式"><a href="#分代式" class="headerlink" title="分代式"></a>分代式</h3><p>将对象根据存活时间的长短进行分类，存活时间小于某个值的为年轻代，存活时间大于某个值的为老年代，永远不会参与回收的对象为永久代。并根据分代假设（如果一个对象存活时间不长则倾向于被回收，如果一个对象已经存活很长时间则倾向于存活更长时间）对对象进行回收。</p><h3 id="Golang的垃圾回收（GC）算法"><a href="#Golang的垃圾回收（GC）算法" class="headerlink" title="Golang的垃圾回收（GC）算法"></a>Golang的垃圾回收（GC）算法</h3><p>Golang的垃圾回收（GC）算法使用的是无分代（对象没有代际之分）、不整理（回收过程中不对对象进行移动与整理）、并发（与用户代码并发执行）的三色标记清扫算法。原因在于：</p><ul><li>对象整理的优势是解决内存碎片问题以及“允许”使用顺序内存分配器。但 Go 运行时的分配算法基于<code>tcmalloc</code>，基本上没有碎片问题。 并且顺序内存分配器在多线程的场景下并不适用。Go 使用的是基于<code>tcmalloc</code>的现代内存分配算法，对对象进行整理不会带来实质性的性能提升。</li><li>分代<code>GC</code>依赖分代假设，即<code>GC</code>将主要的回收目标放在新创建的对象上（存活时间短，更倾向于被回收），而非频繁检查所有对象。</li><li>Go 的编译器会通过逃逸分析将大部分新生对象存储在栈上（栈直接被回收），只有那些需要长期存在的对象才会被分配到需要进行垃圾回收的堆中。也就是说，分代<code>GC</code>回收的那些存活时间短的对象在 Go 中是直接被分配到栈上，当<code>goroutine</code>死亡后栈也会被直接回收，不需要<code>GC</code>的参与，进而分代假设并没有带来直接优势。</li><li>Go 的垃圾回收器与用户代码并发执行，使得 STW 的时间与对象的代际、对象的 size 没有关系。Go 团队更关注于如何更好地让 GC 与用户代码并发执行（使用适当的 CPU 来执行垃圾回收），而非减少停顿时间这一单一目标上。</li></ul><h2 id="三、Go的垃圾回收"><a href="#三、Go的垃圾回收" class="headerlink" title="三、Go的垃圾回收"></a>三、Go的垃圾回收</h2><p>垃圾回收(Garbage Collection，简称GC)是编程语言中提供的自动的内存管理机制，自动释放不需要的内存对象，让出存储器资源。GC过程中无需程序员手动执行。GC机制在现代很多编程语言都支持，GC能力的性能与优劣也是不同语言之间对比度指标之一。</p><p>Golang在GC的演进过程中也经历了很多次变革，Go V1.3之前的标记-清除(mark and sweep)算法，Go V1.3之前的标记-清扫(mark and sweep)的缺点</p><ul><li>Go V1.5的三色并发标记法</li><li>Go V1.5的三色标记为什么需要STW</li><li>Go V1.5的三色标记为什么需要屏障机制(“强-弱” 三色不变式、插入屏障、删除屏障 )</li><li>Go V1.8混合写屏障机制</li><li>Go V1.8混合写屏障机制的全场景分析</li></ul><h3 id="（一）、Go-V1-3之前的标记-清除-mark-and-sweep-算法"><a href="#（一）、Go-V1-3之前的标记-清除-mark-and-sweep-算法" class="headerlink" title="（一）、Go V1.3之前的标记-清除(mark and sweep)算法"></a>（一）、Go V1.3之前的标记-清除(mark and sweep)算法</h3><p>在Golang1.3之前的时候主要用的普通的标记-清除算法，此算法主要有两个主要的步骤：</p><ul><li>标记(Mark phase)</li><li>清除(Sweep phase)</li></ul><h4 id="1-标记清除算法的具体步骤"><a href="#1-标记清除算法的具体步骤" class="headerlink" title="1 标记清除算法的具体步骤"></a>1 标记清除算法的具体步骤</h4><p><strong>第一步</strong>，暂停程序业务逻辑, 分类出可达和不可达的对象，然后做上标记。</p><p><img src="https://cdn.nlark.com/yuque/0/2022/png/26269664/1650787873045-d038fe47-4898-4b07-9e16-007bebb6fb9c.png?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_43,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><p>图中表示是程序与对象的可达关系，目前程序的可达对象有对象1-2-3，对象4-7等五个对象。</p><p><strong>第二步</strong>, 开始标记，程序找出它所有可达的对象，并做上标记。如下图所示：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/26269664/1650787891194-883ec541-5f13-4934-9274-080e5f44cf5e.png?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_44,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><p>所以对象1-2-3、对象4-7等五个对象被做上标记。</p><p><strong>第三步</strong>,  标记完了之后，然后开始清除未标记的对象. 结果如下。</p><p><img src="https://cdn.nlark.com/yuque/0/2022/png/26269664/1650787913616-ecf21ee2-c247-4401-9d3e-5e2fa278726f.png?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_38,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><p>操作非常简单，但是有一点需要额外注意：mark and sweep算法在执行的时候，需要程序暂停！即 <code>STW(stop the world)</code>，STW的过程中，CPU不执行用户代码，全部用于垃圾回收，这个过程的影响很大，所以STW也是一些回收机制最大的难题和希望优化的点。所以在执行第三步的这段时间，程序会暂定停止任何工作，卡在那等待回收执行完毕。</p><p><strong>第四步</strong>, 停止暂停，让程序继续跑。然后循环重复这个过程，直到process程序生命周期结束。</p><p>以上便是标记-清除（mark and sweep）回收的算法。</p><h4 id="2-标记-清除-mark-and-sweep-的缺点"><a href="#2-标记-清除-mark-and-sweep-的缺点" class="headerlink" title="2 标记-清除(mark and sweep)的缺点"></a>2 标记-清除(mark and sweep)的缺点</h4><p>标记清除算法明了，过程鲜明干脆，但是也有非常严重的问题。</p><ul><li>STW，stop the world；让程序暂停，程序出现卡顿 **(重要问题)**；</li><li>标记需要扫描整个heap；</li><li>清除数据会产生heap碎片。</li></ul><p>Go V1.3版本之前就是以上来实施的,  在执行GC的基本流程就是首先启动STW暂停，然后执行标记，再执行数据回收，最后停止STW，如图所示。</p><p><img src="https://cdn.nlark.com/yuque/0/2022/png/26269664/1650787936233-9002040d-220b-4af6-8e51-75d7887569b4.png?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_69,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><p>从上图来看，全部的GC时间都是包裹在STW范围之内的，这样貌似程序暂停的时间过长，影响程序的运行性能。所以Go V1.3 做了简单的优化,将STW的步骤提前, 减少STW暂停的时间范围.如下所示</p><p><img src="https://cdn.nlark.com/yuque/0/2022/png/26269664/1650788071197-26a29703-0fb5-43f4-afc5-87a35fc78a4b.png?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_69,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"><br>上图主要是将STW的步骤提前了一步，因为在Sweep清除的时候，可以不需要STW停止，因为这些对象已经是不可达对象了，不会出现回收写冲突等问题。</p><p>但是无论怎么优化，Go V1.3都面临这个一个重要问题，就是<strong>mark-and-sweep 算法会暂停整个程序</strong> 。</p><p>Go是如何面对并这个问题的呢？接下来G V1.5版本 就用<strong>三色并发标记法</strong>来优化这个问题.</p><h3 id="（二）、Go-V1-5的三色并发标记法"><a href="#（二）、Go-V1-5的三色并发标记法" class="headerlink" title="（二）、Go V1.5的三色并发标记法"></a>（二）、Go V1.5的三色并发标记法</h3><p>为了解决标记清除算法带来的STW问题，Go和Java都会实现三色可达性分析标记算法的变种以缩短STW的时间。三色可达性分析标记算法按“是否被访问过”将程序中的对象分成白色、黑色和灰色：</p><ul><li><strong>白色对象 — 对象尚未被垃圾收集器访问过，在可达性分析刚开始的阶段，所有的对象都是白色的，若在分析结束阶段，仍然是白色的对象，即代表不可达。</strong></li><li><strong>黑色对象 — 表示对象已经被垃圾收集器访问过，且这个对象的所有引用都已经被扫描过，黑色的对象代表已经被扫描过而且是安全存活的，如果有其他对象只想黑色对象无需再扫描一遍，黑色对象不可能直接（不经过灰色对象）指向某个白色对象。</strong></li><li><strong>灰色对象 — 表示对象已经被垃圾收集器访问过，但是这个对象上至少存在一个引用还没有被扫描过，因为存在指向白色对象的外部指针，垃圾收集器会扫描这些对象的子对象。</strong></li></ul><p>三色可达性分析算法大致的流程是（初始状态所有对象都是白色）：</p><p><strong>1.从GC Roots开始枚举，它们所有的直接引用变为灰色（移入灰色集合），GC Roots变为黑色。</strong></p><p><strong>2.从灰色集合中取出一个灰色对象进行分析：</strong></p><ul><li><strong>将这个对象所有的直接引用变为灰色，放入灰色集合中；</strong></li><li><strong>将这个对象变为黑色。</strong></li></ul><p><strong>3.重复步骤2，一直重复直到灰色集合为空。</strong></p><p><strong>4.分析完成，仍然是白色的对象就是GC Roots不可达的对象，可以作为垃圾被清理。</strong></p><p>Golang中的垃圾回收主要应用三色标记法，GC过程和其他用户goroutine可并发运行，但需要一定时间的<strong>STW(stop the world)<strong>，所谓</strong>三色标记法</strong>实际上就是通过三个阶段的标记来确定清楚的对象都有哪些？我们来看一下具体的过程。</p><p><strong>第一步</strong> , 每次新创建的对象，默认的颜色都是标记为“白色”，如图所示。</p><p><img src="https://cdn.nlark.com/yuque/0/2022/png/26269664/1651035738281-051f7a89-e07f-418c-ad0e-7cb94ef1a3b8.png?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_61,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><p>上图所示，我们的程序可抵达的内存对象关系如左图所示，右边的标记表，是用来记录目前每个对象的标记颜色分类。这里面需要注意的是，所谓“程序”，则是一些对象的根节点集合。所以我们如果将“程序”展开，会得到类似如下的表现形式，如图所示。</p><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651035821416-b0ad644e-ef8e-440a-bbf4-b9e24a7e0257.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><p><strong>第二步</strong>, 每次GC回收开始, 会从根节点开始遍历所有对象，把遍历到的对象从白色集合放入“灰色”集合如图所示。</p><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651035842467-7341846f-6dee-4f8b-ad37-dc9723aa6407.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"><br>这里 要注意的是，本次遍历是一次遍历，非递归形式，是从程序抽次可抵达的对象遍历一层，如上图所示，当前可抵达的对象是对象1和对象4，那么自然本轮遍历结束，对象1和对象4就会被标记为灰色，灰色标记表就会多出这两个对象。</p><p><strong>第三步</strong>, 遍历灰色集合，将灰色对象引用的对象从白色集合放入灰色集合，之后将此灰色对象放入黑色集合，如图所示。</p><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651035859950-96053775-24f7-4bdc-a1fb-295747055b3e.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"><br>这一次遍历是只扫描灰色对象，将灰色对象的第一层遍历可抵达的对象由白色变为灰色，如：对象2、对象7. 而之前的灰色对象1和对象4则会被标记为黑色，同时由灰色标记表移动到黑色标记表中。</p><p><strong>第四步</strong>, 重复<strong>第三步</strong>, 直到灰色中无任何对象，如图所示。<br><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651035907012-927d6cbc-686b-4f81-a1de-097ac7598a8e.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"><br><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651035916208-9c293dc0-8988-4180-a9b7-412e2599af0e.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><p>当我们全部的可达对象都遍历完后，灰色标记表将不再存在灰色对象，目前全部内存的数据只有两种颜色，黑色和白色。那么黑色对象就是我们程序逻辑可达（需要的）对象，这些数据是目前支撑程序正常业务运行的，是合法的有用数据，不可删除，白色的对象是全部不可达对象，目前程序逻辑并不依赖他们，那么白色对象就是内存中目前的垃圾数据，需要被清除。</p><p><strong>第五步</strong>: 回收所有的白色标记表的对象. 也就是回收垃圾，如图所示。</p><p>以上我们将全部的白色对象进行删除回收，<img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651035960263-e50436a6-4a3c-48f9-82cb-bb5729d71116.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img">剩下的就是全部依赖的黑色对象。</p><p>三色标记清除算法本身是不可以并发或者增量执行的，<strong>它需要STW</strong>，<strong>而如果并发执行，用户程序可能在标记执行的过程中修改对象的指针。</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/VY8SELNGe94Cjxng5VbT4M7FkUgyAfhpDGBI8liaibcvyXxOjP7kowzG1TnVmgAJefhegPo2IJiabXQ6IxnRdVqPQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><h3 id="没有STW的异常情况一般会有2种："><a href="#没有STW的异常情况一般会有2种：" class="headerlink" title="没有STW的异常情况一般会有2种："></a><strong>没有STW的异常情况一般会有2种：</strong></h3><p>1.一种是把原本应该垃圾回收的死亡对象错误的标记为存活。虽然这不好，但是不会导致严重后果，只不过产生了一点逃过本次回收的浮动垃圾而已，下次清理就可以，比如上图所示的三色标记过程中，用户程序取消了从B对象到E对象的引用，但是因为B到E已经被标记完成不会继续执行步骤2，所以E对象最终会被错误的标记成黑色，不会被回收，这个D就是<strong>浮动垃圾</strong>，会在下次垃圾收集中清理。</p><p>2.一种是把原本存活的对象错误的标记为已死亡，导致“对象消失”，这在内存管理中是非常严重的错误。比如上图所示的三色标记过程中，用户程序建立了从B对象到H对象的引用(例如<strong>B.next &#x3D;H</strong>)，接着执行<strong>D.next&#x3D;nil</strong>，但是因为B到H中不存在灰色对象，因此在这之间不会继续执行三色并发标记中的步骤2，D到H之间的链接被断开，所以H对象最终会被标记成白色，会被垃圾收集器错误地回收。我们将这种错误称为<strong>悬挂指针</strong>，即指针没有指向特定类型的合法对象，影响了内存的安全性。</p><h3 id="没有STW的三色标记法情况下-—-悬挂指针的具体介绍"><a href="#没有STW的三色标记法情况下-—-悬挂指针的具体介绍" class="headerlink" title="没有STW的三色标记法情况下  — 悬挂指针的具体介绍"></a>没有STW的三色标记法情况下  — 悬挂指针的具体介绍</h3><p>先抛砖引玉，我们加入如果没有STW，那么也就不会再存在性能上的问题，那么接下来我们假设如果三色标记法不加入STW会发生什么事情？<br>我们还是基于上述的三色并发标记法来说, 他是一定要依赖STW的. 因为如果不暂停程序, 程序的逻辑改变对象引用关系, 这种动作如果在标记阶段做了修改，会影响标记结果的正确性，我们来看看一个场景，如果三色标记法, 标记过程不使用STW将会发生什么事情?</p><p>1.我们把初始状态设置为已经经历了第一轮扫描，目前黑色的有对象1和对象4， 灰色的有对象2和对象7，其他的为白色对象，且对象2是通过指针p指向对象3的，如图所示。<br><img src="https://liangyaopei.github.io/2021/01/02/golang-gc-intro/no_STW_1.png" alt="no_STW_1"></p><p>2.现在如何三色标记过程不启动STW，那么在GC扫描过程中，任意的对象均可能发生读写操作，如图所示，在还没有扫描到对象2的时候，已经标记为黑色的对象4，此时创建指针q，并且指向白色的对象3。</p><p><img src="https://liangyaopei.github.io/2021/01/02/golang-gc-intro/no_STW_2.png" alt="no_STW_2"></p><ol start="3"><li>与此同时灰色的对象2将指针p移除，那么白色的对象3实则就是被挂在了已经扫描完成的黑色的对象4下，如图所示。</li></ol><p><img src="https://liangyaopei.github.io/2021/01/02/golang-gc-intro/no_STW_3.png" alt="no_STW_3"></p><p>4.然后我们正常指向三色标记的算法逻辑，将所有灰色的对象标记为黑色，那么对象2和对象7就被标记成了黑色，如图所示。<img src="https://liangyaopei.github.io/2021/01/02/golang-gc-intro/no_STW_4.png" alt="no_STW_4"></p><p>5.那么就执行了三色标记的最后一步，将所有白色对象当做垃圾进行回收，如图所示。</p><p><img src="https://liangyaopei.github.io/2021/01/02/golang-gc-intro/no_STW_5.png" alt="no_STW_5"><br>但是最后我们才发现，本来是对象4合法引用的对象3，却被GC给“误杀”回收掉了。</p><p><strong>可以看出，有两种情况，在三色标记法中，是不希望被发生的。</strong></p><ul><li>条件1: 一个白色对象被黑色对象引用**(白色被挂在黑色下)**</li><li>条件2: 灰色对象与它之间的可达关系的白色对象遭到破坏**(灰色同时丢了该白色)**<br>如果当以上两个条件同时满足时，就会出现对象丢失现象!</li></ul><p>并且，如图所示的场景中，如果示例中的白色对象3还有很多下游对象的话, 也会一并都清理掉。</p><p>为了防止这种现象的发生，最简单的方式就是STW，直接禁止掉其他用户程序对对象引用关系的干扰，但是<strong>STW的过程有明显的资源浪费，对所有的用户程序都有很大影响</strong>。那么是否可以在保证对象不丢失的情况下合理的尽可能的提高GC效率，减少STW时间呢？答案是可以的，我们只要使用一种机制，尝试去破坏上面的两个必要条件就可以了。</p><h2 id="四、屏障技术"><a href="#四、屏障技术" class="headerlink" title="四、屏障技术"></a><strong>四、屏障技术</strong></h2><p>为了解决上述的“对象消失”的现象，Wilson于1994年在理论上证明了，当且仅当以下两个条件同时满足时，会产生“对象消失”的问题，即原本应该是黑色的对象被误标为白色：</p><ul><li>赋值器插入了一条或多条从黑色对象到白色对象的新引用；</li><li>赋值器删除了全部从灰色对象到该白色对象的直接或间接引用。</li></ul><p>因此为了我们要解决并发扫描时的对象消失问题，保证垃圾收集算法的正确性，只需破坏这两个条件的任意一个即可，<strong>屏障技术</strong>就是在并发或者增量标记过程中保证<strong>三色不变性</strong>的重要技术。</p><p>内存屏障技术是一种屏障指令，它可以让CPU或者编译器在执行内存相关操作时遵循特定的约束，目前多数的现代处理器都会乱序执行指令以最大化性能，但是该技术能够保证内存操作的顺序性，在内存屏障前执行的操作一定会先于内存屏障后执行的操作。垃圾收集中的屏障技术更像是一个<strong>钩子方法</strong>，它是在用户程序读取对象、创建新对象以及更新对象指针时执行的一段代码，根据操作类型的不同，我们可以将它们分成<strong>读屏障（Read barrier）</strong>和写屏障（Write barrier）两种，<strong>因为读屏障需要在读操作中加入代码片段，对用户程序的性能影响很大，所以编程语言往往都会采用写屏障保证三色不变性。</strong></p><p><strong>重点</strong>：</p><ol><li><strong>写屏障的代码在编译期间生成好，之后不会再变化；</strong></li><li>堆上对象赋值才会生成写屏障；</li><li>哪些对象分配在栈上，哪些分配在堆上？也是编译期间由编译器决定，这个过程叫做“逃逸分析”；</li></ol><h4 id="原理分析"><a href="#原理分析" class="headerlink" title="原理分析"></a>原理分析</h4><p>下面的例子使用的是 go1.13.3。</p><h5 id="示例分析代码"><a href="#示例分析代码" class="headerlink" title="示例分析代码"></a>示例分析代码</h5><p>写屏障是编译器生成的，先形象看下代码样子：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs go"> <span class="hljs-number">1</span> <span class="hljs-keyword">package</span> main<br> <span class="hljs-number">2</span> <br> <span class="hljs-number">3</span> <span class="hljs-keyword">type</span> BaseStruct <span class="hljs-keyword">struct</span> &#123;<br> <span class="hljs-number">4</span>     name <span class="hljs-type">string</span><br> <span class="hljs-number">5</span>     age  <span class="hljs-type">int</span><br> <span class="hljs-number">6</span> &#125;<br> <span class="hljs-number">7</span> <br> <span class="hljs-number">8</span> <span class="hljs-keyword">type</span> Tstruct <span class="hljs-keyword">struct</span> &#123;<br> <span class="hljs-number">9</span>     base   *BaseStruct<br><span class="hljs-number">10</span>     field0 <span class="hljs-type">int</span><br><span class="hljs-number">11</span> &#125;<br><span class="hljs-number">12</span> <br><span class="hljs-number">13</span> <span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">funcAlloc0</span> <span class="hljs-params">(a *Tstruct)</span></span> &#123;<br><span class="hljs-number">14</span>     a.base = <span class="hljs-built_in">new</span>(BaseStruct)    <span class="hljs-comment">// new 一个BaseStruct结构体，赋值给 a.base 字段</span><br><span class="hljs-number">15</span> &#125;<br><span class="hljs-number">16</span> <br><span class="hljs-number">17</span> <span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">funcAlloc1</span> <span class="hljs-params">(b *Tstruct)</span></span> &#123;<br><span class="hljs-number">18</span>     <span class="hljs-keyword">var</span> b0 Tstruct<br><span class="hljs-number">19</span>     b0.base = <span class="hljs-built_in">new</span>(BaseStruct)  <span class="hljs-comment">// new 一个BaseStruct结构体，赋值给 b0.base 字段</span><br><span class="hljs-number">20</span> &#125;<br><span class="hljs-number">21</span> <br><span class="hljs-number">22</span> <span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<br><span class="hljs-number">23</span>     a := <span class="hljs-built_in">new</span>(Tstruct)    <span class="hljs-comment">// new 一个Tstruct 结构体</span><br><span class="hljs-number">24</span>     b := <span class="hljs-built_in">new</span>(Tstruct)   <span class="hljs-comment">// new 一个Tstruct 结构体</span><br><span class="hljs-number">25</span>     <br><span class="hljs-number">26</span>     <span class="hljs-keyword">go</span> funcAlloc0(a)<br><span class="hljs-number">27</span>     <span class="hljs-keyword">go</span> funcAlloc1(b)<br><span class="hljs-number">28</span> &#125;<br></code></pre></td></tr></table></figure><p>这里例子，可以用来观察两个东西：</p><ol><li>逃逸分析</li><li>编译器插入内存屏障的时机</li></ol><h5 id="逃逸分析"><a href="#逃逸分析" class="headerlink" title="逃逸分析"></a>逃逸分析</h5><p>只有堆上对象的写才会可能有写屏障，因为如果对栈上的写做拦截，那么流程代码会非常复杂，并且性能下降会非常大，得不偿失。根据局部性的原理来说，其实我们程序跑起来，大部分的其实都是操作在栈上，函数参数啊、函数调用导致的压栈出栈啊、局部变量啊，协程栈，这些如果也弄起写屏障，那么可想而知了，根本就不现实，复杂度和性能就是越不过去的坎。</p><p>继续看逃逸什么意思？就是内存分配到堆上。golang 可以在编译的时候使用 <code>-m</code> 参数支持把这个可视化出来：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs vim">$ <span class="hljs-keyword">go</span> build -gcflags <span class="hljs-string">&quot;-N -l -m&quot;</span> ./test_writebarrier0.<span class="hljs-keyword">go</span> <br># <span class="hljs-keyword">command</span>-<span class="hljs-built_in">line</span>-arguments<br>./test_writebarrier0.<span class="hljs-keyword">go</span>:<span class="hljs-number">13</span>:<span class="hljs-number">18</span>: funcAlloc0 <span class="hljs-keyword">a</span> does not <span class="hljs-built_in">escape</span><br>./test_writebarrier0.<span class="hljs-keyword">go</span>:<span class="hljs-number">14</span>:<span class="hljs-number">17</span>: <span class="hljs-keyword">new</span>(BaseStruct) escapes <span class="hljs-keyword">to</span> heap<br>./test_writebarrier0.<span class="hljs-keyword">go</span>:<span class="hljs-number">17</span>:<span class="hljs-number">18</span>: funcAlloc1 <span class="hljs-keyword">b</span> does not <span class="hljs-built_in">escape</span><br>./test_writebarrier0.<span class="hljs-keyword">go</span>:<span class="hljs-number">19</span>:<span class="hljs-number">18</span>: funcAlloc1 <span class="hljs-keyword">new</span>(BaseStruct) does not <span class="hljs-built_in">escape</span><br>./test_writebarrier0.<span class="hljs-keyword">go</span>:<span class="hljs-number">23</span>:<span class="hljs-number">13</span>: <span class="hljs-keyword">new</span>(Tstruct) escapes <span class="hljs-keyword">to</span> heap<br>./test_writebarrier0.<span class="hljs-keyword">go</span>:<span class="hljs-number">24</span>:<span class="hljs-number">13</span>: <span class="hljs-keyword">new</span>(Tstruct) escapes <span class="hljs-keyword">to</span> heap<br></code></pre></td></tr></table></figure><p><strong>先说逃逸分析两点原则</strong>：</p><ol><li>在保证程序正确性的前提下，尽可能的把对象分配到栈上，这样性能最好；<ol><li>栈上的对象生命周期就跟随 goroutine ，协程终结了，它就没了</li></ol></li><li>明确一定要分配到堆上对象，或者不确定是否要分配在堆上的对象，那么就全都分配到堆上；<ol><li>这种对象的生命周期始于业务程序的创建，终于垃圾回收器的回收</li></ol></li></ol><p>我们看到源代码，有四次 new 对象的操作，经过编译器的“逃逸分析”之后，实际分配到堆上的是三次：</p><ol><li><p>14 行 —— 触发逃逸（分配到堆上）</p><ol><li>这个必须得分配到堆上，因为除了这个 goroutine 还要存活呢</li></ol></li><li><p>19 行 —— 无 （分配到栈上）</p><ol><li>这个虽然也是 new，单就分配到栈上就行，因为 b0 这个对象就是一个纯粹的栈对象</li></ol></li><li><p>23 行 —— 触发逃逸 （分配到堆上）</p><ol><li>这个需要分配到堆上，因为分配出来的对象需要传递到其他协程使用</li></ol></li><li><p>24 行 —— 触发逃逸 （分配到堆上）</p><p>  1.这次必须注意下，其实站在我们上帝视角，这次的分配其实也可以分配到栈上。这种情况编译器就简单处理了，直接给分配到堆上。这种就属于编译器它摸不准的，那么分配到堆上就对了，反正也就性能有点影响，功能不会有问题，不然的话你真分配到栈上了，一旦栈被回收就出问题了</p></li></ol><h5 id="写屏障真实的样子"><a href="#写屏障真实的样子" class="headerlink" title="写屏障真实的样子"></a>写屏障真实的样子</h5><p>再看下编译器汇编的代码：</p><p>![汇编后]<img src="https://cdn.jsdelivr.net/gh/longpi1/blog-img/20220919223939.png"></p><p>从这个地方我们需要知道一个事情，go 的关键字语法呀，其实在编译的时候，都会对应到一个特定的函数，比如 new 这个关键字就对应了 <code>newobject</code> 函数，go 这个关键字对应的是 <code>newproc</code> 函数。贴一张比较完整的图：</p><p><img src="https://cdn.jsdelivr.net/gh/longpi1/blog-img/20220919224028.png"></p><p>从这个汇编代码我们也确认了，23，24行的对象分配确实是在堆上。我们再看下函数 <code>funcAlloc0</code> 和 <code>funcAlloc1</code> 这两个。</p><p><strong><code>main.funcAlloc0</code></strong></p><figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs basic"><span class="hljs-symbol">13 </span>func funcAlloc0 (a *Tstruct) &#123;<br><span class="hljs-symbol">14 </span>    a.<span class="hljs-keyword">base</span> = <span class="hljs-keyword">new</span>(BaseStruct)    // <span class="hljs-keyword">new</span> 一个BaseStruct结构体，赋值给 a.<span class="hljs-keyword">base</span> 字段<br><span class="hljs-symbol">15 </span>&#125;<br></code></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/liqingqiya/liqingqiya.github.io/images/posts/2020-07-11-gc2/23DEE999-886F-4298-BCAE-EDB4F7A0B454.png"></p><p>简单的注释解析：</p><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs x86asm">(gdb) disassemble <br>Dump of assembler code for function main<span class="hljs-number">.</span>funcAlloc0:<br>   <span class="hljs-number">0x0000000000456b10</span> &lt;+<span class="hljs-number">0</span>&gt;:     <span class="hljs-keyword">mov</span>    %fs:<span class="hljs-number">0xfffffffffffffff8</span>,%rcx<br>   <span class="hljs-number">0x0000000000456b19</span> &lt;+<span class="hljs-number">9</span>&gt;:     <span class="hljs-keyword">cmp</span>    <span class="hljs-number">0x10</span>(%rcx),%rsp<br>   <span class="hljs-number">0x0000000000456b1d</span> &lt;+<span class="hljs-number">13</span>&gt;:    <span class="hljs-keyword">jbe</span>    <span class="hljs-number">0x456b6f</span> &lt;main<span class="hljs-number">.</span>funcAlloc0+<span class="hljs-number">95</span>&gt;<br>   <span class="hljs-number">0x0000000000456b1f</span> &lt;+<span class="hljs-number">15</span>&gt;:    <span class="hljs-keyword">sub</span>    <span class="hljs-number">$0</span>x20,%rsp<br>   <span class="hljs-number">0x0000000000456b23</span> &lt;+<span class="hljs-number">19</span>&gt;:    <span class="hljs-keyword">mov</span>    %rbp,<span class="hljs-number">0x18</span>(%rsp)<br>   <span class="hljs-number">0x0000000000456b28</span> &lt;+<span class="hljs-number">24</span>&gt;:    <span class="hljs-keyword">lea</span>    <span class="hljs-number">0x18</span>(%rsp),%rbp<br>   <span class="hljs-number">0x0000000000456b2d</span> &lt;+<span class="hljs-number">29</span>&gt;:    <span class="hljs-keyword">lea</span>    <span class="hljs-number">0x1430c</span>(%rip),%rax        # <span class="hljs-number">0x46ae40</span><br>   <span class="hljs-number">0x0000000000456b34</span> &lt;+<span class="hljs-number">36</span>&gt;:    <span class="hljs-keyword">mov</span>    %rax,(%rsp)<br>   <span class="hljs-number">0x0000000000456b38</span> &lt;+<span class="hljs-number">40</span>&gt;:    callq  <span class="hljs-number">0x40b060</span> &lt;runtime<span class="hljs-number">.</span>newobject&gt;<br>   # newobject的返回值在 <span class="hljs-number">0x8</span>(%rsp) 里，golang 的参数和返回值都是通过栈传递的。这个跟 c 程序不同，c 程序是溢出才会用到栈，这里先把返回值放到寄存器 <span class="hljs-built_in">rax</span><br>   <span class="hljs-number">0x0000000000456b3d</span> &lt;+<span class="hljs-number">45</span>&gt;:    <span class="hljs-keyword">mov</span>    <span class="hljs-number">0x8</span>(%rsp),%rax           <br>   <span class="hljs-number">0x0000000000456b42</span> &lt;+<span class="hljs-number">50</span>&gt;:    <span class="hljs-keyword">mov</span>    %rax,<span class="hljs-number">0x10</span>(%rsp)<br>   # <span class="hljs-number">0x28</span>(%rsp) 就是 a 的地址：<span class="hljs-number">0xc0000840b0</span><br>=&gt; <span class="hljs-number">0x0000000000456b47</span> &lt;+<span class="hljs-number">55</span>&gt;:    <span class="hljs-keyword">mov</span>    <span class="hljs-number">0x28</span>(%rsp),%rdi         <br>   <span class="hljs-number">0x0000000000456b4c</span> &lt;+<span class="hljs-number">60</span>&gt;:    <span class="hljs-keyword">test</span>   %al,(%rdi)<br>   # 这里判断是否开启了屏障（垃圾回收的扫描并发过程，才会把这个标记打开，没有打开的情况，对于堆上的赋值只是多走一次判断开销）<br>   <span class="hljs-number">0x0000000000456b4e</span> &lt;+<span class="hljs-number">62</span>&gt;:    cmpl   <span class="hljs-number">$0</span>x0,<span class="hljs-number">0x960fb</span>(%rip)        # <span class="hljs-number">0x4ecc50</span> &lt;runtime<span class="hljs-number">.</span>writeBarrier&gt;<br>   <span class="hljs-number">0x0000000000456b55</span> &lt;+<span class="hljs-number">69</span>&gt;:    <span class="hljs-keyword">je</span>     <span class="hljs-number">0x456b59</span> &lt;main<span class="hljs-number">.</span>funcAlloc0+<span class="hljs-number">73</span>&gt;<br>   <span class="hljs-number">0x0000000000456b57</span> &lt;+<span class="hljs-number">71</span>&gt;:    <span class="hljs-keyword">jmp</span>    <span class="hljs-number">0x456b68</span> &lt;main<span class="hljs-number">.</span>funcAlloc0+<span class="hljs-number">88</span>&gt;<br>   # 赋值 a<span class="hljs-number">.</span>base = xxxx<br>   <span class="hljs-number">0x0000000000456b59</span> &lt;+<span class="hljs-number">73</span>&gt;:    <span class="hljs-keyword">mov</span>    %rax,(%rdi)<br>   <span class="hljs-number">0x0000000000456b5c</span> &lt;+<span class="hljs-number">76</span>&gt;:    <span class="hljs-keyword">jmp</span>    <span class="hljs-number">0x456b5e</span> &lt;main<span class="hljs-number">.</span>funcAlloc0+<span class="hljs-number">78</span>&gt;<br>   <span class="hljs-number">0x0000000000456b5e</span> &lt;+<span class="hljs-number">78</span>&gt;:    <span class="hljs-keyword">mov</span>    <span class="hljs-number">0x18</span>(%rsp),%rbp<br>   <span class="hljs-number">0x0000000000456b63</span> &lt;+<span class="hljs-number">83</span>&gt;:    <span class="hljs-keyword">add</span>    <span class="hljs-number">$0</span>x20,%rsp<br>   <span class="hljs-number">0x0000000000456b67</span> &lt;+<span class="hljs-number">87</span>&gt;:    retq   <br>   # 如果是开启了屏障，那么完成 a<span class="hljs-number">.</span>base = xxx 的赋值就是在 gcWriteBarrier 函数里面了<br>   <span class="hljs-number">0x0000000000456b68</span> &lt;+<span class="hljs-number">88</span>&gt;:    callq  <span class="hljs-number">0x44d170</span> &lt;runtime<span class="hljs-number">.</span>gcWriteBarrier&gt;<br>   <span class="hljs-number">0x0000000000456b6d</span> &lt;+<span class="hljs-number">93</span>&gt;:    <span class="hljs-keyword">jmp</span>    <span class="hljs-number">0x456b5e</span> &lt;main<span class="hljs-number">.</span>funcAlloc0+<span class="hljs-number">78</span>&gt;<br>   <span class="hljs-number">0x0000000000456b6f</span> &lt;+<span class="hljs-number">95</span>&gt;:    callq  <span class="hljs-number">0x44b370</span> &lt;runtime<span class="hljs-number">.</span>morestack_noctxt&gt;<br>   <span class="hljs-number">0x0000000000456b74</span> &lt;+<span class="hljs-number">100</span>&gt;:   <span class="hljs-keyword">jmp</span>    <span class="hljs-number">0x456b10</span> &lt;main<span class="hljs-number">.</span>funcAlloc0&gt;<br>End of assembler dump.<br></code></pre></td></tr></table></figure><p><strong>所以，从上面简单的汇编代码，我们印证得出几个小知识点</strong>：</p><ol><li>golang 传参和返回参数都是通过栈来传递的（可以思考下优略点，有点是逻辑简单了，也能很好的支持多返回值的实现，缺点是比寄存器的方式略慢，但是这种损耗在程序的运行下可以忽略）；</li><li>写屏障是一段编译器插入的特殊代码，在编译期间插入，代码函数名字叫做 <code>gcWriteBarrier</code> ；</li><li>屏障代码并不是直接运行，也是要条件判断的，并不是只要是堆上内存赋值就会运行gcWriteBarrier 代码，而是要有一个条件判断。这里提前透露下，这个条件判断是垃圾回收器扫描开始前，stw 程序给设置上去的；<ol><li>所以平时对于堆上内存的赋值，多了一次写操作；</li></ol></li></ol><p>伪代码如下：</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-keyword">if</span> runtime.writeBarrier.enabled &#123;<br>    runtime.gc<span class="hljs-constructor">WriteBarrier(<span class="hljs-params">ptr</span>, <span class="hljs-params">val</span>)</span><br>&#125; <span class="hljs-keyword">else</span> &#123;<br>    *ptr = <span class="hljs-keyword">val</span><br>&#125;<br></code></pre></td></tr></table></figure><p><code>runtime·gcWriteBarrier</code> 函数干啥的，这个函数是用纯汇编写的，举一个特定cpu集合的例子，在 asm_amd64.s 里的实现。这个函数只干两件事：</p><ol><li>执行写请求</li><li>处理 GC 相关的逻辑</li></ol><p>下面简单理解下 <code>runtime·gcWriteBarrier</code> 这个函数：</p><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs x86asm">TEXT runtime·gcWriteBarrier(SB),<span class="hljs-built_in">NOSPLIT</span>,<span class="hljs-number">$120</span><br><br>        get_tls(<span class="hljs-built_in">R13</span>)<br>        <span class="hljs-keyword">MOVQ</span>    g(<span class="hljs-built_in">R13</span>), <span class="hljs-built_in">R13</span><br>        <span class="hljs-keyword">MOVQ</span>    g_m(<span class="hljs-built_in">R13</span>), <span class="hljs-built_in">R13</span><br>        <span class="hljs-keyword">MOVQ</span>    m_p(<span class="hljs-built_in">R13</span>), <span class="hljs-built_in">R13</span><br>        <span class="hljs-keyword">MOVQ</span>    (p_wbBuf+wbBuf_next)(<span class="hljs-built_in">R13</span>), <span class="hljs-built_in">R14</span><br><br>        LEAQ    <span class="hljs-number">16</span>(<span class="hljs-built_in">R14</span>), <span class="hljs-built_in">R14</span><br>        <span class="hljs-keyword">MOVQ</span>    <span class="hljs-built_in">R14</span>, (p_wbBuf+wbBuf_next)(<span class="hljs-built_in">R13</span>)<br>    // 检查 buffer 队列是否满？<br>        CMPQ    <span class="hljs-built_in">R14</span>, (p_wbBuf+wbBuf_end)(<span class="hljs-built_in">R13</span>)<br><br>    // 赋值的前后两个值都会被入队<br><br>        // 把 value 存到指定 buffer 位置<br>        <span class="hljs-keyword">MOVQ</span>    <span class="hljs-built_in">AX</span>, -<span class="hljs-number">16</span>(<span class="hljs-built_in">R14</span>)   // Record value<br><br>    // 把 *slot 存到指定 buffer 位置<br>        <span class="hljs-keyword">MOVQ</span>    (<span class="hljs-built_in">DI</span>), <span class="hljs-built_in">R13</span><br>        <span class="hljs-keyword">MOVQ</span>    <span class="hljs-built_in">R13</span>, -<span class="hljs-number">8</span>(<span class="hljs-built_in">R14</span>)<br><br>    // 如果 wbBuffer 队列满了，那么就下刷处理，比如置灰，置黑等操作<br>        JEQ     flush<br><span class="hljs-symbol">ret:</span><br>    // 赋值：*slot = val <br>        <span class="hljs-keyword">MOVQ</span>    <span class="hljs-number">104</span>(<span class="hljs-built_in">SP</span>), <span class="hljs-built_in">R14</span><br>        <span class="hljs-keyword">MOVQ</span>    <span class="hljs-number">112</span>(<span class="hljs-built_in">SP</span>), <span class="hljs-built_in">R13</span><br>        <span class="hljs-keyword">MOVQ</span>    <span class="hljs-built_in">AX</span>, (<span class="hljs-built_in">DI</span>)<br>        <span class="hljs-keyword">RET</span><br><span class="hljs-symbol"></span><br><span class="hljs-symbol">flush:</span><br>    。。。<br><br>        //  队列满了，统一处理，这个其实是一个批量优化手段<br>        <span class="hljs-keyword">CALL</span>    runtime·wbBufFlush(SB)<br><br>    。。。<br><br>        <span class="hljs-keyword">JMP</span>     <span class="hljs-keyword">ret</span><br></code></pre></td></tr></table></figure><p><strong>思考下：不是说把 <code>\*slot = value</code> 直接置灰色，置黑色，就完了嘛，这里搞得这么复杂？</strong></p><p>最开始还真不是这样的，这个也是一个优化的过程，这里是利用批量的一个思想做的一个优化。我们再理解下最本质的东西，触发了写屏障之后，我们的核心目的是为了能够把赋值的前后两个值记录下来，以便 GC 垃圾回收器能得到通知，从而避免错误的回收。记录下来是最本质的，但是并不是要立马处理，所以这里做的优化就是，攒满一个 buffer ，然后批量处理，这样效率会非常高的。</p><p>wbBuf 结构如下： |————————————-| | 8 | 8 | 8 * 512 | 4 | |————————————-|</p><p>每个 P 都有这么个 wbBuf 队列。</p><p>我们看到 <code>CALL runtime·wbBufFlush(SB)</code> ，这个函数 wbBufFlush 是 golang 实现的，本质上是调用 <code>wbBufFlush1</code> 。这个函数才是 hook 写操作想要做的事情，精简了下代码如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs stylus">func <span class="hljs-built_in">wbBufFlush1</span>(_p_ *p) &#123;<br>        start := <span class="hljs-built_in">uintptr</span>(unsafe<span class="hljs-selector-class">.Pointer</span>(&amp;_p_<span class="hljs-selector-class">.wbBuf</span><span class="hljs-selector-class">.buf</span><span class="hljs-selector-attr">[0]</span>))<br>        n := (_p_<span class="hljs-selector-class">.wbBuf</span><span class="hljs-selector-class">.next</span> - start) / unsafe<span class="hljs-selector-class">.Sizeof</span>(_p_<span class="hljs-selector-class">.wbBuf</span><span class="hljs-selector-class">.buf</span><span class="hljs-selector-attr">[0]</span>)<br>        ptrs := _p_<span class="hljs-selector-class">.wbBuf</span><span class="hljs-selector-class">.buf</span><span class="hljs-selector-attr">[:n]</span><br><br>        _p_<span class="hljs-selector-class">.wbBuf</span><span class="hljs-selector-class">.next</span> = <span class="hljs-number">0</span><br><br>        gcw := &amp;_p_<span class="hljs-selector-class">.gcw</span><br>        pos := <span class="hljs-number">0</span><br>    <span class="hljs-comment">// 循环批量处理队列里的值，这个就是之前在 gcWriteBarrier 赋值的</span><br>        <span class="hljs-keyword">for</span> _, ptr := range ptrs &#123;<br>                <span class="hljs-keyword">if</span> ptr &lt; minLegalPointer &#123;<br>                        continue<br>                &#125;<br>                obj, <span class="hljs-selector-tag">span</span>, objIndex := <span class="hljs-built_in">findObject</span>(ptr, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>)<br>                <span class="hljs-keyword">if</span> obj == <span class="hljs-number">0</span> &#123;<br>                        continue<br>                &#125;<br><br>                mbits := <span class="hljs-selector-tag">span</span><span class="hljs-selector-class">.markBitsForIndex</span>(objIndex)<br>                <span class="hljs-keyword">if</span> mbits<span class="hljs-selector-class">.isMarked</span>() &#123;<br>                        continue<br>                &#125;<br>                mbits<span class="hljs-selector-class">.setMarked</span>()<br>                <span class="hljs-keyword">if</span> <span class="hljs-selector-tag">span</span><span class="hljs-selector-class">.spanclass</span><span class="hljs-selector-class">.noscan</span>() &#123;<br>                        gcw<span class="hljs-selector-class">.bytesMarked</span> += <span class="hljs-built_in">uint64</span>(<span class="hljs-selector-tag">span</span>.elemsize)<br>                        continue<br>                &#125;<br>                ptrs<span class="hljs-selector-attr">[pos]</span> = obj<br>                pos++<br>        &#125;<br><br>        <span class="hljs-comment">// 置灰色（投入灰色的队列），这就是我们的目的，对象在这里面我们就不怕了，我们要扫描的就是这个队列；</span><br>        gcw<span class="hljs-selector-class">.putBatch</span>(ptrs<span class="hljs-selector-attr">[:pos]</span>)<br><br>        _p_<span class="hljs-selector-class">.wbBuf</span><span class="hljs-selector-class">.reset</span>()<br>&#125;<br></code></pre></td></tr></table></figure><p>所以我们总结下，写屏障到底做了什么：</p><ol><li>hook 写操作</li><li>hook 住了写操作之后，把赋值语句的前后两个值都记录下来，投入 buffer 队列</li><li>buffer 攒满之后，批量刷到扫描队列（置灰）（这是 GO 1.10 左右引入的优化）</li></ol><p><strong><code>main.funcAlloc1</code></strong></p><figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs basic"><span class="hljs-symbol">17 </span>func funcAlloc1 (b *Tstruct) &#123;<br><span class="hljs-symbol">18 </span>    var b0 Tstruct<br><span class="hljs-symbol">19 </span>    b0.<span class="hljs-keyword">base</span> = <span class="hljs-keyword">new</span>(BaseStruct)  // <span class="hljs-keyword">new</span> 一个BaseStruct结构体，赋值给 b0.<span class="hljs-keyword">base</span> 字段<br><span class="hljs-symbol">20 </span>&#125;<br></code></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/liqingqiya/liqingqiya.github.io/images/posts/2020-07-11-gc2/FB0B964D-0A0F-4650-8700-45C4278E704E.png"></p><p>最后，再回顾看下 <code>main.funcAlloc1</code> 函数，这个函数是只有栈操作，非常简单。</p><h4 id="“强-弱”-三色不变式"><a href="#“强-弱”-三色不变式" class="headerlink" title="“强-弱” 三色不变式"></a>“强-弱” 三色不变式</h4><p>我们让GC回收器，满足下面两种情况之一时，即可保对象不丢失。  这两种方式就是<strong>“强三色不变式”和“ 弱三色不变式”</strong>。</p><ul><li>强三色不变式</li></ul><p>不存在黑色对象引用到白色对象的指针。</p><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036383192-cb6b9fe9-4946-47da-bb9a-643f0c38a654.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><p>弱三色不变色实际上是强制性的不允许黑色对象引用白色对象，这样就不会出现有白色对象被误删的情况。</p><ul><li>弱三色不变式</li></ul><p>所有被黑色对象引用的白色对象都处于灰色保护状态（允许黑色对象指向白色对象，但必须保证一个前提，这个白色对象必须处于灰色对象的保护下）。</p><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036404003-e0ea569e-7a8a-4d9f-a08f-4bb9ed5c64ed.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><p>弱三色不变式强调，黑色对象可以引用白色对象，但是这个白色对象必须存在其他灰色对象对它的引用，或者可达它的链路上游存在灰色对象。 这样实则是黑色对象引用白色对象，白色对象处于一个危险被删除的状态，但是上游灰色对象的引用，可以保护该白色对象，使其安全。</p><p>为了遵循上述的两个方式，GC算法演进到两种屏障方式，他们<strong>“插入写屏障”, “删除写屏障”</strong>。</p><p><strong>插入写屏障：</strong></p><p><code>具体操作</code>: 在A对象引用B对象的时候，B对象被标记为灰色。(将B挂在A下游，B必须被标记为灰色)</p><p><code>满足</code>: <strong>强三色不变式</strong>. (不存在黑色对象引用白色对象的情况了， 因为白色会强制变成灰色)</p><p><strong>删除写屏障：</strong></p><p><code>具体操作</code>: 被删除的对象，如果自身为灰色或者白色，那么被标记为灰色。</p><p><code>满足</code>: <strong>弱三色不变式</strong>. (保护灰色对象到白色对象的路径不会断)</p><h3 id="（一）插入写屏障"><a href="#（一）插入写屏障" class="headerlink" title="（一）插入写屏障"></a><strong>（一）插入写屏障</strong></h3><p>Dijkstra在1978年提出了插入写屏障，也被叫做增量更新，通过如下所示的写屏障，破坏上述第一个条件（赋值器插入了一条或多条从黑色对象到白色对象的新引用）：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-keyword">func</span> DijkstraWritePointer(slot *unsafe.Pointer, ptr unsafe.Pointer) <br>     shade(ptr)  <span class="hljs-regexp">//</span>先将新下游对象 ptr 标记为灰色<br>     *slot = ptr<br>&#125;<br><br><span class="hljs-regexp">//</span>说明：<br>添加下游对象(当前下游对象slot, 新下游对象ptr) &#123; <br> <span class="hljs-regexp">//</span>step <span class="hljs-number">1</span><br> 标记灰色(新下游对象ptr) <br> <br> <span class="hljs-regexp">//</span>step <span class="hljs-number">2</span><br> 当前下游对象slot = 新下游对象ptr <br>&#125;<br><br><span class="hljs-regexp">//</span>场景：<br>A.添加下游对象(nil, B) <span class="hljs-regexp">//</span>A 之前没有下游， 新添加一个下游对象B， B被标记为灰色<br>A.添加下游对象(C, B) <span class="hljs-regexp">//</span>A 将下游对象C 更换为B， B被标记为灰色<br></code></pre></td></tr></table></figure><p>上述伪代码非常好理解，当黑色对象（slot）插入新的指向白色对象（ptr）的引用关系时，就尝试使用shade函数将这个新插入的引用（ptr）标记为灰色。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/VY8SELNGe94Cjxng5VbT4M7FkUgyAfhpj99S1E3KkyG9kbgAWz9mcJeJthjrVDZZ47DHBs3IgiaicSxjVvhlKUsw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>假设我们上图的例子并发可达性分析中使用插入写屏障：</p><p>1.GC 将根对象Root2指向的B对象标记成黑色并将B对象指向的对象D标记成灰色；</p><p>2.用户程序修改指针，<strong>B.next&#x3D;H</strong>这时触发写屏障将H对象标记成灰色；</p><p>3.用户程序修改指针<strong>D.next&#x3D;null</strong>；</p><p>4.GC依次遍历程序中的H和D将它们分别标记成黑色。</p><h3 id="关于栈没有写屏障的原因"><a href="#关于栈没有写屏障的原因" class="headerlink" title="关于栈没有写屏障的原因"></a>关于栈没有写屏障的原因</h3><p> 黑色对象的内存槽有两种位置, <code>栈</code>和<code>堆</code>. 栈空间的特点是<strong>容量小</strong>,但是<strong>要求响应速度快,因为函数调用弹出频繁使用</strong>, 所以“插入屏障”机制,在<strong>栈空间的对象操作中不使用</strong>. 而仅仅使用在堆空间对象的操作中.</p><p><strong>由于栈上的对象在垃圾回收中被认为是根对象，并没有写屏障，那么导致黑色的栈可能指向白色的堆对象。为了保障内存安全，Dijkstra必须为栈上的对象增加写屏障或者在标记阶段完成重新对栈上的对象进行扫描，这两种方法各有各的缺点，前者会大幅度增加写入指针的额外开销，后者重新扫描栈对象时需要暂停程序，垃圾收集算法的设计者需要在这两者之前做出权衡。</strong></p><p>​接下来，我们用几张图，来模拟整个一个详细的过程， 希望您能够更可观的看清晰整体流程。</p><hr><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036442131-91f36e55-5c94-4931-a140-58ff5627c681.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><hr><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036449149-2fb53d7c-d351-4305-84a8-7a1b51806ce4.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><hr><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036456806-6b1aeb27-831d-43d9-a79e-4dad49fea07d.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><hr><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036465710-e260440e-b53d-4f76-a826-842e28666efe.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><hr><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036474130-755abe1f-d070-47e6-93cf-7aa129489206.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><hr><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036481384-c4e44929-09e4-4a05-81bb-b5e9ed195982.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><p>​但是如果栈不添加,当全部三色标记扫描之后,栈上有可能依然存在白色对象被引用的情况(如上图的对象9).  所以要对栈重新进行三色标记扫描, 但这次为了对象不丢失, 要对本次标记扫描启动STW暂停. 直到栈空间的三色标记结束.</p><hr><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036522462-5e0c1ea9-e136-45c8-9648-bf691b270431.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><hr><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036531031-d37d4239-9b13-4d0e-a9cc-d7bc230d56a8.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><hr><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036538543-d84895c0-451d-4c49-9c67-f77dcf5a3ae9.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><hr><p>​最后将栈和堆空间 扫描剩余的全部 白色节点清除.  这次STW大约的时间在10~100ms间.</p><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036559017-4564c417-9059-415c-aa81-d9504ac4e00b.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><h3 id="（二）删除写屏障"><a href="#（二）删除写屏障" class="headerlink" title="（二）删除写屏障"></a><strong>（二）删除写屏障</strong></h3><p>Yuasa在1990年的论文Real-time garbage collection on general-purpose machines 中提出了删除写屏障，因为一旦该写屏障开始工作，它会保证开启写屏障时堆上所有对象的可达。起始时STW扫描所有的goroutine栈，保证所有堆上在用的对象都处于灰色保护下，所以也被称作<strong>快照垃圾收集或者原始快照</strong>（Snapshot GC），这是破坏了“对象消失”的第二个条件（赋值器删除了全部从灰色对象到该白色对象的直接或间接引用）</p><p>原始快照(Snapshot At The Beginning，SATB)。当某个时刻 的 GC Roots 确定后，当时的对象图就已经确定了。当赋值器（业务线程）从灰色或者白色对象中删除白色指针时候，写屏障会捕捉这一行为，将这一行为通知给回收器。这样，基于起始快照的解决方案保守地将其目标对象当作存活的对象，这样就绝对不会有被误回收的对象，但是有扫描工作量浮动放大的风险。术语叫做追踪波面的回退。这个操作在「修改操作前」进行，JVM中 的 G1 垃圾回收器用的也是这个思路。</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">//</span> 黑色赋值器 Yuasa 屏障<br><span class="hljs-keyword">func</span> YuasaWritePointer(slot *unsafe.Pointer, ptr unsafe.Pointer) &#123;<br>    shade(*slot) 先将*slot标记为灰色<br>    *slot = ptr<br>&#125;<br><br><span class="hljs-regexp">//</span>说明：<br>添加下游对象(当前下游对象slot， 新下游对象ptr) &#123;<br>  <span class="hljs-regexp">//</span>step <span class="hljs-number">1</span><br>  <span class="hljs-keyword">if</span> (当前下游对象slot是灰色 || 当前下游对象slot是白色) &#123;<br>          标记灰色(当前下游对象slot)     <span class="hljs-regexp">//</span>slot为被删除对象， 标记为灰色<br>  &#125;  <br>  <span class="hljs-regexp">//</span>step <span class="hljs-number">2</span><br>  当前下游对象slot = 新下游对象ptr<br>&#125;<br><br><span class="hljs-regexp">//</span>场景<br>A.添加下游对象(B, nil)   <span class="hljs-regexp">//</span>A对象，删除B对象的引用。B被A删除，被标记为灰(如果B之前为白)<br>A.添加下游对象(B, C)     <span class="hljs-regexp">//</span>A对象，更换下游B变成C。B被A删除，被标记为灰(如果B之前为白)<br></code></pre></td></tr></table></figure><p>上述代码会在老对象的引用被删除时，将白色的老对象涂成灰色，这样删除写屏障就可以保证弱三色不变性，老对象引用的下游对象一定可以被灰色对象引用。</p><p>但是这样也会导致一个问题，由于会将<strong>有存活可能的对象都标记成灰色</strong>，因此最后可能会导致应该回收的对象未被回收，这个对象只有在下一个循环才会被回收，比如下图的D对象。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/VY8SELNGe94Cjxng5VbT4M7FkUgyAfhpTuHVXfE7fSIbu8yNpJt877FyhAQuBB96eYr2wH7QcxKBIVxNrssIyQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p><strong>由于原始快照的原因，起始也是执行STW，删除写屏障不适用于栈特别大的场景，栈越大，STW扫描时间越长。</strong></p><p>接下来，我们用几张图，来模拟整个一个详细的过程， 希望您能够更可观的看清晰整体流程。</p><p><img src="https://liangyaopei.github.io/2021/01/02/golang-gc-intro/delete_barrier_1.png" alt="delete_barrier_1"></p><p><img src="https://liangyaopei.github.io/2021/01/02/golang-gc-intro/delete_barrier_2.png" alt="delete_barrier_2"></p><p><img src="https://liangyaopei.github.io/2021/01/02/golang-gc-intro/delete_barrier_3.png" alt="delete_barrier_3"></p><p><img src="https://liangyaopei.github.io/2021/01/02/golang-gc-intro/delete_barrier_4.png" alt="delete_barrier_4"></p><p><img src="https://liangyaopei.github.io/2021/01/02/golang-gc-intro/delete_barrier_5.png" alt="delete_barrier_5"></p><p><img src="https://liangyaopei.github.io/2021/01/02/golang-gc-intro/delete_barrier_6.png" alt="delete_barrier_6"></p><p><img src="https://liangyaopei.github.io/2021/01/02/golang-gc-intro/delete_barrier_7.png" alt="delete_barrier_7"></p><p>这种方式的回收精度低，一个对象即使被删除了最后一个指向它的指针也依旧可以活过这一轮，在下一轮GC中被清理掉。</p><p><img src="C:\Users\longp\AppData\Roaming\Typora\typora-user-images\image-20220821132611515.png" alt="image-20220821132611515"></p><p><strong>插入写屏障和删除写屏障的短板：</strong></p><ul><li>插入写屏障：结束时需要STW来重新扫描栈，标记栈上引用的白色对象的存活； </li><li>删除写屏障：回收精度低，GC开始时STW扫描堆栈来记录初始快照，这个过程会保护开始时刻的所有存活对象。</li></ul><h3 id="（三）混合写屏障"><a href="#（三）混合写屏障" class="headerlink" title="（三）混合写屏障"></a><strong>（三）混合写屏障</strong></h3><p>在 Go 语言 v1.7版本之前，运行时会使用Dijkstra插入写屏障保证强三色不变性，但是运行时并没有在所有的垃圾收集根对象上开启插入写屏障。因为应用程序可能包含成百上千的Goroutine，而垃圾收集的根对象一般包括全局变量和栈对象，如果运行时需要在几百个Goroutine的栈上都开启写屏障，会带来巨大的额外开销，所以 Go 团队在v1.8结合上述2种写屏障构成了混合写屏障，实现上选择了在标记阶段完成时暂停程序、将所有栈对象标记为灰色并重新扫描。</p><p>Go 语言在v1.8组合Dijkstra插入写屏障和Yuasa删除写屏障构成了如下所示的混合写屏障，该写屏障会将被覆盖的对象标记成灰色并在当前栈没有扫描时将新对象也标记成灰色：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">writePointer</span><span class="hljs-params">(slot, ptr)</span></span>:<br>    <span class="hljs-built_in">shade</span>(*slot)<br>    <span class="hljs-keyword">if</span> current stack is grey:<br>        <span class="hljs-built_in">shade</span>(ptr)<br>    *slot = ptr<br></code></pre></td></tr></table></figure><p>为了移除栈的重扫描过程，除了引入混合写屏障之外，在垃圾收集的标记阶段，我们还需要将创建的所有新对象都标记成黑色，防止新分配的栈内存和堆内存中的对象被错误地回收，因为栈内存在标记阶段最终都会变为黑色，所以不再需要重新扫描栈空间。总结来说主要有这几点：</p><ul><li><p>GC开始将栈上的对象全部扫描并标记为黑色；</p></li><li><p>GC期间，任何在栈上创建的新对象，均为黑色；</p></li><li><p>被删除的堆对象标记为灰色；</p></li><li><p>被添加的堆对象标记为灰色。</p></li></ul><p>Go V1.8版本引入了混合写屏障机制（hybrid write barrier），避免了对栈re-scan的过程，极大的减少了STW的时间。结合了两者的优点。</p><p>最本质的区别就是：<strong>内存屏障其实就是编译器帮你生成的一段 hook 代码</strong>，这三个屏障的本质区别就是 hook 的时机不同而已。</p><hr><h4 id="1-混合写屏障规则"><a href="#1-混合写屏障规则" class="headerlink" title="(1) 混合写屏障规则"></a>(1) 混合写屏障规则</h4><p><code>具体操作</code>:</p><p>1、GC开始将栈上的对象全部扫描并标记为黑色(之后不再进行第二次重复扫描，无需STW)，</p><p>2、GC期间，任何在栈上创建的新对象，均为黑色。</p><p>3、被删除的对象标记为灰色。</p><p>4、被添加的对象标记为灰色。</p><p><code>满足</code>: 变形的<strong>弱三色不变式</strong>.</p><p>伪代码：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs go">添加下游对象(当前下游对象slot, 新下游对象ptr) &#123;<br>  <span class="hljs-comment">//1 </span><br>标记灰色(当前下游对象slot)    <span class="hljs-comment">//只要当前下游对象被移走，就标记灰色</span><br>  <br>  <span class="hljs-comment">//2 </span><br>  标记灰色(新下游对象ptr)<br>  <br>  <span class="hljs-comment">//3</span><br>  当前下游对象slot = 新下游对象ptr<br>&#125;<br></code></pre></td></tr></table></figure><p>这里我们注意， 屏障技术是不在栈上应用的，因为要保证栈的运行效率。</p><h4 id="2-混合写屏障的具体场景分析"><a href="#2-混合写屏障的具体场景分析" class="headerlink" title="(2) 混合写屏障的具体场景分析"></a>(2) 混合写屏障的具体场景分析</h4><p>接下来，我们用几张图，来模拟整个一个详细的过程， 希望您能够更可观的看清晰整体流程。</p><p>注意混合写屏障是Gc的一种屏障机制，所以只是当程序执行GC的时候，才会触发这种机制。</p><h5 id="GC开始：扫描栈区，将可达对象全部标记为黑"><a href="#GC开始：扫描栈区，将可达对象全部标记为黑" class="headerlink" title="GC开始：扫描栈区，将可达对象全部标记为黑"></a>GC开始：扫描栈区，将可达对象全部标记为黑</h5><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036708530-f7c50de5-6a63-45dc-baef-f53b1b42eb62.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036716310-65729a9c-d8df-40ce-9c2b-d35228278791.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><hr><h5 id="场景一：-对象被一个堆对象删除引用，成为栈对象的下游"><a href="#场景一：-对象被一个堆对象删除引用，成为栈对象的下游" class="headerlink" title="场景一： 对象被一个堆对象删除引用，成为栈对象的下游"></a>场景一： 对象被一个堆对象删除引用，成为栈对象的下游</h5><p>伪代码</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">//前提：堆对象4-&gt;对象7 = 对象7；  //对象7 被 对象4引用</span><br>栈对象<span class="hljs-number">1</span>-&gt;对象<span class="hljs-number">7</span> = 堆对象<span class="hljs-number">7</span>；  <span class="hljs-comment">//将堆对象7 挂在 栈对象1 下游</span><br>堆对象<span class="hljs-number">4</span>-&gt;对象<span class="hljs-number">7</span> = null；    <span class="hljs-comment">//对象4 删除引用 对象7</span><br></code></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036737874-a2f71441-c4f9-4f74-8c8a-c5a53bd35d4c.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036745104-24b7bf17-27b9-4531-97b7-48c5b7e64fac.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><h5 id="场景二：-对象被一个栈对象删除引用，成为另一个栈对象的下游"><a href="#场景二：-对象被一个栈对象删除引用，成为另一个栈对象的下游" class="headerlink" title="场景二： 对象被一个栈对象删除引用，成为另一个栈对象的下游"></a>场景二： 对象被一个栈对象删除引用，成为另一个栈对象的下游</h5><p>伪代码</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-built_in">new</span> 栈对象<span class="hljs-number">9</span>；<br>对象<span class="hljs-number">8</span>-&gt;对象<span class="hljs-number">3</span> = 对象<span class="hljs-number">3</span>；      <span class="hljs-comment">//将栈对象3 挂在 栈对象9 下游</span><br>对象<span class="hljs-number">2</span>-&gt;对象<span class="hljs-number">3</span> = null；      <span class="hljs-comment">//对象2 删除引用 对象3</span><br></code></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036778055-bda31c21-45dc-4602-9241-11a33b6393a6.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036785024-0edb665e-7b4b-46e3-b8cf-1d4ff02e73cd.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036791814-78eed337-a9ac-42d9-bcd8-99a21c01111c.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><h5 id="场景三：对象被一个堆对象删除引用，成为另一个堆对象的下游"><a href="#场景三：对象被一个堆对象删除引用，成为另一个堆对象的下游" class="headerlink" title="场景三：对象被一个堆对象删除引用，成为另一个堆对象的下游"></a>场景三：对象被一个堆对象删除引用，成为另一个堆对象的下游</h5><p>伪代码</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs go">堆对象<span class="hljs-number">10</span>-&gt;对象<span class="hljs-number">7</span> = 堆对象<span class="hljs-number">7</span>；       <span class="hljs-comment">//将堆对象7 挂在 堆对象10 下游</span><br>堆对象<span class="hljs-number">4</span>-&gt;对象<span class="hljs-number">7</span> = null；         <span class="hljs-comment">//对象4 删除引用 对象7</span><br></code></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036826144-893174fb-0111-4838-9f7d-38fe2f89648a.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036833484-a18064d9-1329-42d7-8687-8a029542e85e.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036840569-f50df9db-5219-48fe-83ff-c3545ed4dec4.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><h5 id="场景四：对象从一个栈对象删除引用，成为另一个堆对象的下游"><a href="#场景四：对象从一个栈对象删除引用，成为另一个堆对象的下游" class="headerlink" title="场景四：对象从一个栈对象删除引用，成为另一个堆对象的下游"></a>场景四：对象从一个栈对象删除引用，成为另一个堆对象的下游</h5><p>伪代码</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs go">堆对象<span class="hljs-number">10</span>-&gt;对象<span class="hljs-number">7</span> = 堆对象<span class="hljs-number">7</span>；       <span class="hljs-comment">//将堆对象7 挂在 堆对象10 下游</span><br>堆对象<span class="hljs-number">4</span>-&gt;对象<span class="hljs-number">7</span> = null；         <span class="hljs-comment">//对象4 删除引用 对象7</span><br></code></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036859560-21a75ea4-ee66-46ae-81bc-ce4e697c3814.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036864959-929ec428-e8d8-48a9-aaeb-e2589723ec62.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><p><img src="https://cdn.nlark.com/yuque/0/2022/jpeg/26269664/1651036876957-976a0ac6-6c82-4eca-88f3-10180782281c.jpeg?x-oss-process=image/watermark,type_d3F5LW1pY3JvaGVp,size_55,text_5YiY5Li55YawQWNlbGQ=,color_FFFFFF,shadow_50,t_80,g_se,x_10,y_10" alt="img"></p><p>​Golang中的混合写屏障满足<code>弱三色不变式</code>，结合了删除写屏障和插入写屏障的优点，只需要在开始时并发扫描各个goroutine的栈，使其变黑并一直保持，这个过程不需要STW，而标记结束后，因为栈在扫描后始终是黑色的，也无需再进行re-scan操作了，减少了STW的时间。</p><h2 id="五、GC演进过程"><a href="#五、GC演进过程" class="headerlink" title="五、GC演进过程"></a><strong>五、GC演进过程</strong></h2><p>v1.0 — 完全串行的标记和清除过程，需要暂停整个程序；</p><p>v1.1 — 在多核主机并行执行垃圾收集的标记和清除阶段；</p><p>v1.3 — 运行时<strong>基于只有指针类型的值包含指针</strong>的假设增加了对栈内存的精确扫描支持，实现了真正精确的垃圾收集；将unsafe.Pointer类型转换成整数类型的值认定为不合法的，可能会造成悬挂指针等严重问题；</p><p>v1.5 — 实现了基于<strong>三色标记清扫的并发</strong>垃圾收集器（插入写屏障）：</p><ul><li>大幅度降低垃圾收集的延迟从几百 ms 降低至 10ms 以下；</li><li>计算垃圾收集启动的合适时间并通过并发加速垃圾收集的过程；</li></ul><p>v1.6 — 实现了去中心化的垃圾收集协调器：</p><ul><li>基于显式的状态机使得任意Goroutine都能触发垃圾收集的状态迁移；</li><li>使用密集的位图替代空闲链表表示的堆内存，降低清除阶段的CPU占用;</li></ul><p>v1.7 — 通过<strong>并行栈收缩</strong>将垃圾收集的时间缩短至2ms以内；</p><p>v1.8 — 使用<strong>混合写屏障</strong>将垃圾收集的时间缩短至0.5ms以内；</p><p>v1.9 — 彻底移除暂停程序的重新扫描栈的过程；</p><p>v1.10 — 更新了垃圾收集调频器（Pacer）的实现，分离软硬堆大小的目标；</p><p>v1.12 — 使用<strong>新的标记终止算法</strong>简化垃圾收集器的几个阶段；</p><p>v1.13 — 通过新的 Scavenger 解决瞬时内存占用过高的应用程序向操作系统归还内存的问题；</p><p>v1.14 — 使用全新的页分配器<strong>优化内存分配的速度</strong>；</p><p>v1.15 — 改进编译器和运行时内部的CL 226367，它使编译器可以将更多的x86寄存器用于垃圾收集器的写屏障调用；</p><p>v1.16 — Go runtime默认使用MADV_DONTNEED更积极的将不用的内存释放给OS。</p><h2 id="六、GC过程"><a href="#六、GC过程" class="headerlink" title="六、GC过程"></a><strong>六、GC过程</strong></h2><p>Golang GC 相关的代码在<strong>runtime&#x2F;mgc.go</strong>文件下，可以看见GC总共分为4个阶段(翻译自Golang v1.16版本源码)：</p><p><strong>1.sweep termination（清理终止）</strong></p><ul><li><p>暂停程序，触发STW。所有的P（处理器）都会进入safe-point（安全点）；</p></li><li><p>清理未被清理的 span 。如果当前垃圾收集是强制触发的，需要处理还未被清理的内存管理单元；</p></li></ul><p><strong>2.the mark phase（标记阶段）</strong></p><ul><li>将<strong>GC状态gcphase从_GCoff改成_GCmark</strong>、开启写屏障、启用协助线程（mutator assists）、将根对象入队；</li><li>恢复程序执行，标记进程（mark workers）和协助程序会开始并发标记内存中的对象，写屏障会覆盖的重写指针和新指针（标记成灰色），而所有新创建的对象都会被直接标记成黑色；</li><li>GC执行根节点的标记，这包括扫描所有的栈、全局对象以及不在堆中的运行时数据结构。扫描goroutine栈会导致goroutine停止，并对栈上找到的所有指针加置灰，然后继续执行goroutine；</li><li>GC遍历灰色对象队列，会将灰色对象变成黑色，并将该指针指向的对象置灰；</li><li>由于GC工作分布在本地缓存中，GC会使用分布式终止算法（distributed termination algorithm）来检测何时不再有根标记作业或灰色对象，如果没有了GC会转为mark termination（标记终止）。</li></ul><p><strong>3. mark termination（标记终止）</strong></p><ul><li>STW；</li><li>将GC状态gcphase切换至_GCmarktermination，关闭gc工作线程和协助程序；</li><li>执行housekeeping，例如刷新mcaches。</li></ul><p><strong>4. the sweep phase（清理阶段）</strong></p><ul><li>将GC状态gcphase切换至_GCoff来准备清理阶段，初始化清理阶段并关闭写屏障；</li><li>恢复用户程序，从现在开始，所有新创建的对象会标记成白色；如果有必要，在使用前分配清理spans；</li><li>后台并发清理所有的内存管理类单元。</li></ul><p><strong>GC过程代码示例</strong></p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">gcfinished</span><span class="hljs-params">()</span></span> *<span class="hljs-type">int</span> &#123;<br>  p := <span class="hljs-number">1</span><br>  runtime.SetFinalizer(&amp;p, <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(_ *<span class="hljs-type">int</span>)</span></span> &#123;<br>    <span class="hljs-built_in">println</span>(<span class="hljs-string">&quot;gc finished&quot;</span>)<br>  &#125;)<br>  <span class="hljs-keyword">return</span> &amp;p<br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">allocate</span><span class="hljs-params">()</span></span> &#123;<br>  _ = <span class="hljs-built_in">make</span>([]<span class="hljs-type">byte</span>, <span class="hljs-type">int</span>((<span class="hljs-number">1</span>&lt;&lt;<span class="hljs-number">20</span>)*<span class="hljs-number">0.25</span>))<br>&#125;<br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<br>  f, _ := os.Create(<span class="hljs-string">&quot;trace.out&quot;</span>)<br>  <span class="hljs-keyword">defer</span> f.Close()<br>  trace.Start(f)<br>  <span class="hljs-keyword">defer</span> trace.Stop()<br>  gcfinished()<br>  <span class="hljs-comment">// 当完成 GC 时停止分配</span><br>  <span class="hljs-keyword">for</span> n := <span class="hljs-number">1</span>; n &lt; <span class="hljs-number">50</span>; n++ &#123;<br>    <span class="hljs-built_in">println</span>(<span class="hljs-string">&quot;#allocate: &quot;</span>, n)<br>    allocate()<br>  &#125;<br>  <span class="hljs-built_in">println</span>(<span class="hljs-string">&quot;terminate&quot;</span>)<br>&#125;<br></code></pre></td></tr></table></figure><p>运行程序</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">hewittwang@HEWITTWANG-MB0 rtx % <span class="hljs-attr">GODEBUG=</span><span class="hljs-attr">gctrace=</span><span class="hljs-number">1</span> go run new1.go  <br>gc <span class="hljs-number">1</span> @<span class="hljs-number">0.015s</span> <span class="hljs-number">0</span>%: <span class="hljs-number">0.015</span>+<span class="hljs-number">0.36</span>+<span class="hljs-number">0.043</span> <span class="hljs-keyword">ms</span> <span class="hljs-title">clock</span>, <span class="hljs-number">0.18</span>+<span class="hljs-number">0.55</span>/<span class="hljs-number">0.64</span>/<span class="hljs-number">0.13</span>+<span class="hljs-number">0.52</span> <span class="hljs-keyword">ms</span> <span class="hljs-title">cpu</span>, <span class="hljs-number">4</span>-&gt;<span class="hljs-number">4</span>-&gt;<span class="hljs-number">0</span> MB, <span class="hljs-number">5</span> MB goal, <span class="hljs-number">12</span> P<br>gc <span class="hljs-number">2</span> @<span class="hljs-number">0.024s</span> <span class="hljs-number">1</span>%: <span class="hljs-number">0.045</span>+<span class="hljs-number">0.19</span>+<span class="hljs-number">0.018</span> <span class="hljs-keyword">ms</span> <span class="hljs-title">clock</span>, <span class="hljs-number">0.54</span>+<span class="hljs-number">0.37</span>/<span class="hljs-number">0.31</span>/<span class="hljs-number">0.041</span>+<span class="hljs-number">0.22</span> <span class="hljs-keyword">ms</span> <span class="hljs-title">cpu</span>, <span class="hljs-number">4</span>-&gt;<span class="hljs-number">4</span>-&gt;<span class="hljs-number">0</span> MB, <span class="hljs-number">5</span> MB goal, <span class="hljs-number">12</span> P<br>....<br></code></pre></td></tr></table></figure><p>栈分析</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">gc <span class="hljs-number">2</span>      : 第一个GC周期<br>@<span class="hljs-number">0.024s</span>   : 从程序开始运行到第一次GC时间为<span class="hljs-number">0.024</span> 秒<br><span class="hljs-number">1</span>%        : 此次GC过程中CPU 占用率<br><br>wall clock<br><span class="hljs-number">0.045</span>+<span class="hljs-number">0.19</span>+<span class="hljs-number">0.018</span> <span class="hljs-keyword">ms</span> <span class="hljs-title">clock</span><br><span class="hljs-number">0.045</span> <span class="hljs-keyword">ms</span>  <span class="hljs-title">: STW</span>，Marking <span class="hljs-literal">Start</span>, 开启写屏障<br><span class="hljs-number">0.19</span> <span class="hljs-keyword">ms</span>   <span class="hljs-title">: Marking</span>阶段<br><span class="hljs-number">0.018</span> <span class="hljs-keyword">ms</span>  <span class="hljs-title">: STW</span>，Marking终止，关闭写屏障<br><br>CPU time<br><span class="hljs-number">0.54</span>+<span class="hljs-number">0.37</span>/<span class="hljs-number">0.31</span>/<span class="hljs-number">0.041</span>+<span class="hljs-number">0.22</span> <span class="hljs-keyword">ms</span> <span class="hljs-title">cpu</span><br><span class="hljs-number">0.54</span> <span class="hljs-keyword">ms</span>   <span class="hljs-title">: STW</span>，Marking <span class="hljs-literal">Start</span><br><span class="hljs-number">0.37</span> <span class="hljs-keyword">ms</span>  <span class="hljs-title">: 辅助标记时间</span><br><span class="hljs-title">0</span>.<span class="hljs-number">31</span> <span class="hljs-keyword">ms</span>  <span class="hljs-title">: 并发标记时间</span><br><span class="hljs-title">0</span>.<span class="hljs-number">041</span> <span class="hljs-keyword">ms</span>   <span class="hljs-title">: GC</span> 空闲时间<br><span class="hljs-number">0.22</span> <span class="hljs-keyword">ms</span>   <span class="hljs-title">: Mark</span> 终止时间<br><br><span class="hljs-number">4</span>-&gt;<span class="hljs-number">4</span>-&gt;<span class="hljs-number">0</span> MB， <span class="hljs-number">5</span> MB goal<br><span class="hljs-number">4</span> MB      ：标记开始时，堆大小实际值<br><span class="hljs-number">4</span> MB      ：标记结束时，堆大小实际值<br><span class="hljs-number">0</span> MB      ：标记结束时，标记为存活对象大小<br><span class="hljs-number">5</span> MB      ：标记结束时，堆大小预测值<br><br><span class="hljs-number">12</span> P      ：本次GC过程中使用的goroutine 数量<br></code></pre></td></tr></table></figure><h2 id="七、GC触发条件"><a href="#七、GC触发条件" class="headerlink" title="七、GC触发条件"></a><strong>七、GC触发条件</strong></h2><p>运行时会通过runtime.gcTrigger.test方法决定是否需要触发垃圾收集，当满足触发垃圾收集的基本条件（即满足_GCoff阶段的退出条件）时——允许垃圾收集、程序没有崩溃并且没有处于垃圾收集循环，该方法会根据三种不同方式触发进行不同的检查：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">//mgc.go 文件 runtime.gcTrigger.test</span><br> <span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(t gcTrigger)</span></span> test() <span class="hljs-type">bool</span> &#123;<br>    <span class="hljs-comment">//测试是否满足触发垃圾手机的基本条件</span><br>    <span class="hljs-keyword">if</span> !memstats.enablegc || panicking != <span class="hljs-number">0</span> || gcphase != _GCoff &#123;<br>       <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span><br>    &#125;<br>    <span class="hljs-keyword">switch</span> t.kind &#123;<br>      <span class="hljs-keyword">case</span> gcTriggerHeap:    <span class="hljs-comment">//堆内存的分配达到达控制器计算的触发堆大小</span><br>         <span class="hljs-comment">// Non-atomic access to gcController.heapLive for performance. If</span><br>         <span class="hljs-comment">// we are going to trigger on this, this thread just</span><br>         <span class="hljs-comment">// atomically wrote gcController.heapLive anyway and we&#x27;ll see our</span><br>         <span class="hljs-comment">// own write.</span><br>         <span class="hljs-keyword">return</span> gcController.heapLive &gt;= gcController.trigger<br>      <span class="hljs-keyword">case</span> gcTriggerTime:      <span class="hljs-comment">//如果一定时间内没有触发，就会触发新的循环，该出发条件由 `runtime.forcegcperiod`变量控制，默认为 2 分钟；</span><br>         <span class="hljs-keyword">if</span> gcController.gcPercent &lt; <span class="hljs-number">0</span> &#123;<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span><br>        &#125;<br>         lastgc := <span class="hljs-type">int64</span>(atomic.Load64(&amp;memstats.last_gc_nanotime))<br>         <span class="hljs-keyword">return</span> lastgc != <span class="hljs-number">0</span> &amp;&amp; t.now-lastgc &gt; forcegcperiod<br>      <span class="hljs-keyword">case</span> gcTriggerCycle:      <span class="hljs-comment">//如果当前没有开启垃圾收集，则触发新的循环；</span><br>         <span class="hljs-comment">// t.n &gt; work.cycles, but accounting for wraparound.</span><br>         <span class="hljs-keyword">return</span> <span class="hljs-type">int32</span>(t.n-work.cycles) &gt; <span class="hljs-number">0</span><br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span><br> &#125;<br></code></pre></td></tr></table></figure><p>用于开启垃圾回收的方法为runtime.gcStart，因此所有调用该函数的地方都是触发GC的代码：</p><ul><li>runtime.mallocgc申请内存时根据堆大小触发GC</li><li>runtime.GC用户程序手动触发GC</li><li>runtime.forcegchelper后台运行定时检查触发GC</li></ul><p><strong>（一）申请内存触发runtime.mallocgc</strong></p><p>Go运行时会将堆上的对象按大小分成微对象、小对象和大对象三类，这三类对象的创建都可能会触发新的GC。</p><p>1.当前线程的内存管理单元中不存在空闲空间时，创建微对象(noscan &amp;&amp;size&lt;maxTinySize)和小对象需要调用 runtime.mcache.nextFree从中心缓存或者页堆中获取新的管理单元，这时如果span满了就会导致返回的shouldhelpgc&#x3D;true，就可能触发垃圾收集；</p><p>2.当用户程序申请分配32KB以上的大对象时，一定会构建 runtime.gcTrigger结构体尝试触发垃圾收集。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">mallocgc</span><span class="hljs-params">(size <span class="hljs-type">uintptr</span>, typ *_type, needzero <span class="hljs-type">bool</span>)</span></span> unsafe.Pointer &#123;<br>    省略代码 ...<br>    shouldhelpgc := <span class="hljs-literal">false</span>  <br>  dataSize := size<br>  c := getMCache()       <span class="hljs-comment">//尝试获取mCache。如果没启动或者没有P,返回nil；</span><br> <br>    省略代码 ...<br>    <span class="hljs-keyword">if</span> size &lt;= maxSmallSize &#123;  <br>       <span class="hljs-keyword">if</span> noscan &amp;&amp; size &lt; maxTinySize &#123; <span class="hljs-comment">// 微对象分配</span><br>  省略代码 ...<br>          v := nextFreeFast(span)<br>          <span class="hljs-keyword">if</span> v == <span class="hljs-number">0</span> &#123;<br>             v, span, shouldhelpgc = c.nextFree(tinySpanClass)<br>          &#125;<br>      省略代码 ...<br>      &#125; <span class="hljs-keyword">else</span> &#123;      <span class="hljs-comment">//小对象分配</span><br>         省略代码 ...<br>          <span class="hljs-keyword">if</span> v == <span class="hljs-number">0</span> &#123;<br>             v, span, shouldhelpgc = c.nextFree(spc)<br>          &#125;<br>        省略代码 ...<br>      &#125;<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>       shouldhelpgc = <span class="hljs-literal">true</span><br>       省略代码 ...<br>    &#125;<br>  省略代码 ...<br>    <span class="hljs-keyword">if</span> shouldhelpgc &#123;      <span class="hljs-comment">//是否应该触发gc</span><br>      <span class="hljs-keyword">if</span> t := (gcTrigger&#123;kind: gcTriggerHeap&#125;); t.test() &#123;   <span class="hljs-comment">//如果满足gc触发条件就调用gcStart()</span><br>          gcStart(t)<br>      &#125;<br>    &#125;<br>  省略代码 ...<br>    <span class="hljs-keyword">return</span> x<br> &#125;<br></code></pre></td></tr></table></figure><p>这个时候调用t.test()执行的是gcTriggerHeap情况，只需要判断gcController.heapLive &gt;&#x3D; gcController.trigger的真假就可以了。 heapLive表示垃圾收集中存活对象字节数，trigger表示触发标记的堆内存大小的；当内存中存活的对象字节数大于触发垃圾收集的堆大小时，新一轮的垃圾收集就会开始。</p><p>1.heapLive — 为了减少锁竞争，运行时只会在中心缓存分配或者释放内存管理单元以及在堆上分配大对象时才会更新；</p><p>2.trigger — 在标记终止阶段调用runtime.gcSetTriggerRatio更新触发下一次垃圾收集的堆大小，它能够决定触发垃圾收集的时间以及用户程序和后台处理的标记任务的多少，利用反馈控制的算法根据堆的增长情况和垃圾收集CPU利用率确定触发垃圾收集的时机。</p><p><strong>（二）手动触发runtime.GC</strong></p><p>用户程序会通过runtime.GC函数在程序运行期间主动通知运行时执行，该方法在调用时会阻塞调用方直到当前垃圾收集循环完成，在垃圾收集期间也可能会通过STW暂停整个程序：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-keyword">func</span> GC() &#123;<br>    <span class="hljs-regexp">//</span>在正式开始垃圾收集前，运行时需要通过runtime.gcWaitOnMark等待上一个循环的标记终止、标记和清除终止阶段完成；<br>    n := atomic.Load(&amp;work.cycles)<br>    gcWaitOnMark(n)<br> <br>  <span class="hljs-regexp">//</span>调用 `runtime.gcStart` 触发新一轮的垃圾收集<br>    gcStart(gcTrigger&#123;kind: gcTriggerCycle, n: n + <span class="hljs-number">1</span>&#125;)<br> <br>    <span class="hljs-regexp">//</span>`runtime.gcWaitOnMark` 等待该轮垃圾收集的标记终止阶段正常结束；<br>    gcWaitOnMark(n + <span class="hljs-number">1</span>)<br> <br>    <span class="hljs-regexp">//</span> 持续调用 `runtime.sweepone` 清理全部待处理的内存管理单元并等待所有的清理工作完成<br>    <span class="hljs-keyword">for</span> atomic.Load(&amp;work.cycles) == n+<span class="hljs-number">1</span> &amp;&amp; sweepone() != ^uintptr(<span class="hljs-number">0</span>) &#123;<br>        sweep.nbgsweep++<br>        Gosched()  <span class="hljs-regexp">//</span>等待期间会调用 `runtime.Gosched` 让出处理器<br>    &#125;<br> <br>    <span class="hljs-regexp">//</span><br>    <span class="hljs-keyword">for</span> atomic.Load(&amp;work.cycles) == n+<span class="hljs-number">1</span> &amp;&amp; !isSweepDone() &#123;<br>        Gosched()<br>    &#125;<br> <br>    <span class="hljs-regexp">//</span> 完成本轮垃圾收集的清理工作后，通过 `runtime.mProf_PostSweep` 将该阶段的堆内存状态快照发布出来，我们可以获取这时的内存状态<br>    mp := acquirem()<br>    cycle := atomic.Load(&amp;work.cycles)<br>    <span class="hljs-keyword">if</span> cycle == n+<span class="hljs-number">1</span> || (gcphase == _GCmark &amp;&amp; cycle == n+<span class="hljs-number">2</span>) &#123;   <span class="hljs-regexp">//</span>仅限于没有启动其他标记终止过程<br>        mProf_PostSweep()<br>    &#125;<br>    releasem(mp)<br>&#125;<br></code></pre></td></tr></table></figure><p><strong>（三）后台运行定时检查触发runtime.forcegchelper</strong></p><p>运行时会在应用程序启动时在后台开启一个用于强制触发垃圾收集的Goroutine，该Goroutine调用runtime.gcStart尝试启动新一轮的垃圾收集：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-comment">// start forcegc helper goroutine</span><br>func <span class="hljs-built_in">init</span>() &#123;<br>   go <span class="hljs-built_in">forcegchelper</span>()<br>&#125;<br> <br>func <span class="hljs-built_in">forcegchelper</span>() &#123;<br>   forcegc<span class="hljs-selector-class">.g</span> = <span class="hljs-built_in">getg</span>()<br>   <span class="hljs-built_in">lockInit</span>(&amp;forcegc<span class="hljs-selector-class">.lock</span>, lockRankForcegc)<br>   <span class="hljs-keyword">for</span> &#123;<br>      <span class="hljs-built_in">lock</span>(&amp;forcegc.lock)<br>      <span class="hljs-keyword">if</span> forcegc<span class="hljs-selector-class">.idle</span> != <span class="hljs-number">0</span> &#123;<br>         <span class="hljs-built_in">throw</span>(<span class="hljs-string">&quot;forcegc: phase error&quot;</span>)<br>      &#125;<br>      atomic<span class="hljs-selector-class">.Store</span>(&amp;forcegc<span class="hljs-selector-class">.idle</span>, <span class="hljs-number">1</span>)<br>      <br>     <span class="hljs-comment">//该 Goroutine 会在循环中调用runtime.goparkunlock主动陷入休眠等待其他 Goroutine 的唤醒</span><br>      <span class="hljs-built_in">goparkunlock</span>(&amp;forcegc<span class="hljs-selector-class">.lock</span>, waitReasonForceGCIdle, traceEvGoBlock, <span class="hljs-number">1</span>)<br>       <br>      <span class="hljs-keyword">if</span> debug<span class="hljs-selector-class">.gctrace</span> &gt; <span class="hljs-number">0</span> &#123;<br>         <span class="hljs-built_in">println</span>(<span class="hljs-string">&quot;GC forced&quot;</span>)<br>      &#125;<br>      <span class="hljs-comment">// Time-triggered, fully concurrent.</span><br>      <span class="hljs-built_in">gcStart</span>(gcTrigger&#123;kind: gcTriggerTime, n<br>      ow: <span class="hljs-built_in">nanotime</span>()&#125;)<br>   &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="八、问题思考"><a href="#八、问题思考" class="headerlink" title="八、问题思考"></a>八、问题思考</h2><p>1.为什么删除写屏障的时候要原始快照？</p><p>2.删除写屏障出现已扫描黑色对象新增白色对象的怎么处理？</p><p>3.关于内存管理，gc整体流程，go如何将代码转化为二进制？</p><p><strong>参考文献</strong></p><p>  1.《Go语言设计与实现》</p><p>(<a href="https://draveness.me/golang/docs/part3-runtime/ch07-memory/golang-garbage-collector/">https://draveness.me/golang/docs/part3-runtime/ch07-memory/golang-garbage-collector/</a>)</p><p>  2.《一个专家眼中的Go与Java垃圾回收算法大对比》</p><p>(<a href="https://blog.csdn.net/u011277123/article/details/53991572">https://blog.csdn.net/u011277123/article/details/53991572</a>)</p><p>  3.《Go语言问题集》</p><p>(<a href="https://www.bookstack.cn/read/qcrao-Go-Questions/spilt.19.GC-GC.md">https://www.bookstack.cn/read/qcrao-Go-Questions/spilt.19.GC-GC.md</a>)</p><p>   4.《CMS垃圾收集器》</p><p>(<a href="https://juejin.cn/post/6844903782107578382">https://juejin.cn/post/6844903782107578382</a>)</p><p>  5.《Golang v 1.16版本源码》</p><p>(<a href="https://github.com/golang/go">https://github.com/golang/go</a>)</p><p>  6.《Golang—内存管理(内存分配)》</p><p>(<a href="http://t.zoukankan.com/zpcoding-p-13259943.html">http://t.zoukankan.com/zpcoding-p-13259943.html</a>)</p><p>  7.《深入理解Java虚拟机：JVM高级特性与最佳实践（第3版）》—机械工业出版社</p><p>  8.《腾讯妹子图解Golang内存分配和垃圾回收》](<a href="https://mp.weixin.qq.com/s/iAy9ReQhnmCYUFvwYroGPA">https://mp.weixin.qq.com/s/iAy9ReQhnmCYUFvwYroGPA</a>)</p><p>  9.<a href="https://www.yuque.com/aceld/golang/zhzanb">《Golang修养之路》</a></p><ol start="10"><li><p><a href="https://golang.design/under-the-hood/zh-cn/part2runtime/ch08gc/barrier/">https://golang.design/under-the-hood/zh-cn/part2runtime/ch08gc/barrier/</a></p></li><li><p><a href="https://liqingqiya.github.io/golang/gc/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/2020/06/02/gc2.html">https://liqingqiya.github.io/golang/gc/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/2020/06/02/gc2.html</a></p></li></ol>]]></content>
    
    
    <categories>
      
      <category>golang</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>golang</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes 网络基础</title>
    <link href="/2022/08/27/Kubernetes-%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"/>
    <url>/2022/08/27/Kubernetes-%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h1 id="Kubernetes-网络模型基础"><a href="#Kubernetes-网络模型基础" class="headerlink" title="Kubernetes 网络模型基础"></a>Kubernetes 网络模型基础</h1><p>github对应地址：<a href="https://github.com/longpi1/Reading-notes/blob/main/kuberneters/%E7%BD%91%E7%BB%9C/Kubernetes%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80.md">https://github.com/longpi1/Reading-notes/blob/main/kuberneters/%E7%BD%91%E7%BB%9C/Kubernetes%20%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80.md</a></p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Kubernetes 是为运行分布式集群而建立的，分布式系统的本质使得网络成为 Kubernetes 的核心和必要组成部分，了解 Kubernetes 网络模型可以使你能够正确运行、监控和排查应用程序故障。</p><p><img src="https://s2.loli.net/2022/08/27/OlUZo4yNiTAPgmC.png" alt="网络模型.png"></p><p>网络是非常复杂的，拥有许多概念，对于不熟悉这个领域的用户来说，这可能会有一定的难度，这里面有很多概念需要理解，并且还需要把这些概念整合起来形成一个连贯的整体，比如网络命名空间、虚拟接口、IP 转发、NAT 等概念。</p><p>Kubernetes 中对任何网络实现都规定了以下的一些要求：</p><ul><li>所有 Pod 都可以在不使用 NAT 的情况下与所有其他 Pod 进行通信</li><li>所有节点都可以在没有 NAT 的情况下与所有 Pod 进行通信</li><li>Pod 自己的 IP 与其他 Pod 看到的 IP 是相同的</li></ul><p>鉴于这些限制，我们需要解决几个不同的网络问题：</p><ol><li>容器到容器的网络</li><li>Pod 到 Pod 的网络</li><li>Pod 到 Service 的网络</li><li>互联网到 Service 的网络</li></ol><p>接下来我们将来讨论这些问题及其解决方案。</p><h2 id="容器到容器网络"><a href="#容器到容器网络" class="headerlink" title="容器到容器网络"></a>容器到容器网络</h2><p>通常情况下我们将虚拟机中的网络通信视为直接与以太网设备进行交互，如图1所示。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/z9BgVMEm7YtugibqBH8vj7OvmDx0O2fwgCV9T9zxQ7pfMHEiauW3V6z9TiaUIrm4TfdVibb5hNJMJO7OticAOK1v6Ng/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>​                                                                                                                <strong>图1.网络设备的理想视图</strong></p><p>实际的情况肯定比这要复杂，在 Linux 中，每个正在运行的进程都在一个网络命名空间内进行通信，该命名空间提供了一个具有自己的路由、防火墙规则和网络设备的逻辑网络栈，从本质上讲，网络命名空间为命名空间内的所有进程提供了一个全新的网络堆栈。</p><p>Linux 用户可以使用 <code>ip</code> 命令创建网络命名空间。例如，以下命令将创建一个名为 ns1 的网络命名空间。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">$<span class="hljs-built_in"> ip </span>netns <span class="hljs-built_in">add</span> ns1 <br></code></pre></td></tr></table></figure><p>命名空间创建后，会在 <code>/var/run/netns</code> 下面为其创建一个挂载点，即使没有附加任何进程，命名空间也是可以保留的。</p><p>你可以通过列出 <code>/var/run/netns</code> 下的所有挂载点或使用 <code>ip</code> 命令来列出可用的命名空间。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash"><span class="hljs-built_in">ls</span> /var/run/netns</span><br>ns1<br><span class="hljs-meta prompt_">$ </span><span class="language-bash">ip netns</span><br>ns1<br></code></pre></td></tr></table></figure><p>默认情况下，Linux 将为每个进程分配到 root network namespace，以提供访问外部的能力，如图2所示。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtugibqBH8vj7OvmDx0O2fwgoTjJmceqShbEoZ6ibwMOA1VZOV2yYQmN6z9BovoSiafExusQt9dpyu0A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>​                                                                                                                <strong>图2.root network namespace</strong></p><p>对于 Docker 而言，一个 Pod 会被构建成一组共享网络命名空间的 Docker 容器，Pod 中的容器都有相同的 IP 地址和端口空间，它们都是通过分配给 Pod 的网络命名空间来分配的，并且可以通过 localhost 访问彼此，因为它们位于同一个命名空间中。这是使用 Docker 作为 Pod 容器来实现的，它持有网络命名空间，而应用容器则通过 Docker 的 <code>-net=container:sandbox-container</code> 功能加入到该命名空间中，图3显示了每个 Pod 如何由共享网络命名空间内的多个 Docker 容器（<code>ctr*</code>）组成的。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtugibqBH8vj7OvmDx0O2fwgx14N89bgPKjXwqTDV2ia9FbbLyLP2fGEvBrMUT5U4ibvq87nySmZ1xTQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>​                                                                                                               <strong>图3.每个 Pod 的网络命名空间</strong></p><p>此外 Pod 中的容器还可以访问共享卷，这些卷被定义为 Pod 的一部分，并且可以挂载到每个容器的文件系统中。</p><h2 id="Pod-到-Pod-网络"><a href="#Pod-到-Pod-网络" class="headerlink" title="Pod 到 Pod 网络"></a>Pod 到 Pod 网络</h2><p>在 Kubernetes 中，每个 Pod 都有一个真实的 IP 地址，每个 Pod 都使用该 IP 地址与其他 Pod 进行通信。接下来我们将来了解 Kubernetes 如何使用真实的 IP 来实现 Pod 与 Pod 之间的通信的。我们先来讨论同一节点上的 Pod 通信的方式。</p><p>从 Pod 的角度来看，它存在于自己的网络命名空间中，需要与同一节点上的其他网络命名空间进行通信。值得庆幸的时候，命名空间可以使用 Linux 虚拟以太网设备或由两个虚拟接口组成的 <code>veth</code> 对进行连接，这些虚拟接口可以分布在多个命名空间上。要连接 Pod 命名空间，我们可以将 veth 对的的一侧分配给 root network namespace，将另一侧分配给 Pod 的网络命名空间。每个 veth 对就像一根网线，连接两侧并允许流量在它们之间流动。这种设置可以复制到节点上的任意数量的 Pod。图4显示了连接虚拟机上每个 Pod 的 root network namespace 的 veth 对。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtugibqBH8vj7OvmDx0O2fwgiav7goAhdM2Fg40BpBNia6OmnP1yZJ0O2aD9ajK98r46EfkGxIfMYJzQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>​                                                                                                                  <strong>图4.Pod 的 veth 对</strong></p><p>现在 Pod 都有自己的网络命名空间，这样它们就有自己的网络设备和 IP 地址，并且它们连接到节点的 root 命名空间，现在我们希望 Pod 能够通过 root 命名空间进行通信，那么我们将要使用一个网络 <em>bridge（网桥）</em>来实现。</p><p>Linux bridge 是用纯软件实现的虚拟交换机，有着和物理交换机相同的功能，例如二层交换，MAC 地址学习等。因此我们可以把 veth pair 等设备绑定到网桥上，就像是把设备连接到物理交换机上一样。bridge 的工作方式是通过检查通过它的数据包目的地，并决定是否将数据包传递给连接到网桥的其他网段，从而在源和目的地之间维护一个转发表。bridge 通过查看网络中每个以太网设备的唯一 MAC 地址来决定是桥接数据还是丢弃数据。</p><p>Bridges 实现了 ARP 协议来发现与指定 IP 地址关联的链路层 MAC 地址。当 bridge 接收到数据帧的时候，bridge 将该帧广播给所有连接的设备（原始发送者除外），响应该帧的设备被存储在一个查找表中，未来具有相同 IP 地址的通信使用查找表来发现正确的 MAC 地址来转发数据包。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtugibqBH8vj7OvmDx0O2fwgaC5x2L2NGESEDibAC2J9Y4cSics1zvr3vlQEubR88po8icKdIZnzVDGag/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>​                                                                                                          <strong>图5.使用桥接连接命名空间</strong></p><h3 id="同节点-Pod-通信"><a href="#同节点-Pod-通信" class="headerlink" title="同节点 Pod 通信"></a>同节点 Pod 通信</h3><p>网络命名空间将每个 Pod 隔离到自己的网络堆栈中，虚拟以太网设备将每个命名空间连接到根命名空间，以及一个将命名空间连接在一起的网桥，这样我们就准备好在同一节点上的 Pod 之间发送流量了，如下图6所示。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_gif/z9BgVMEm7YtugibqBH8vj7OvmDx0O2fwgbMZZruJE0xAuacLiaia0y3HtN4ic5QJWsCEEpHgfoWsQMboak31eaeXOg/640?wx_fmt=gif&wxfrom=5&wx_lazy=1" alt="图片"></p><p>​                                                                                                                                   <strong>图6.同节点上的Pod间的数据包移动</strong></p><p>这上图中，pod1 向自己的网络设备 <code>eth0</code> 发送了一个数据包，对于 pod1 来说，<code>eth0</code> 通过虚拟网络设备连接到 root netns 的 <code>veth0(1)</code>，网桥 <code>cbr0</code> 被配置为与 <code>veth0</code> 一端相连，一旦数据包到达网桥，网桥就会使用 ARP 协议将数据包发送到 <code>veth1(3)</code>。当数据包到达虚拟设备 <code>veth1</code> 时，它被直接转发到 pod2 的命名空间内的 <code>eth0(4)</code> 设备。这整个过程中，每个 Pod 仅与 <code>localhost</code> 上的 <code>eth0</code> 进行通信，流量就会被路由到正确的 Pod。</p><p>Kubernetes 的网络模型决定了 Pod 必须可以通过其 IP 地址跨节点访问，也就是说，一个 Pod 的 IP 地址始终对网络中的其他 Pod 是可见的，每个 Pod 看待自己的 IP 地址的方式与其他 Pod 看待它的方式是相同的。接下来我们来看看不同节点上的 Pod 之间的流量路由问题。</p><h3 id="跨节点-Pod-通信"><a href="#跨节点-Pod-通信" class="headerlink" title="跨节点 Pod 通信"></a>跨节点 Pod 通信</h3><p>在研究了如何在同一节点上的 Pod 之间路由数据包之后，接下来我们来看下不同节点上的 Pod 之间的通信。Kubernetes 网络模型要求 Pod 的 IP 是可以通过网络访问的，但它并没有规定必须如何来实现。</p><p>通常集群中的每个节点都分配有一个 <code>CIDR</code>，用来指定该节点上运行的 Pod 可用的 IP 地址。一旦以 <code>CIDR</code> 为目的地的流量到达节点，节点就会将流量转发到正确的 Pod。图7展示了两个节点之间的网络通信，假设网络可以将 <code>CIDR</code> 中的流量转发到正确的节点。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_gif/z9BgVMEm7YtugibqBH8vj7OvmDx0O2fwgvHLovL3sZSEtEia3tKWIDCS43V6PLN4kxIjdLnMugfW32fl4ZfHmwSg/640?wx_fmt=gif&wxfrom=5&wx_lazy=1" alt="图片"></p><p>​                                                                                                                                   <strong>图7.不同节点上的Pod间通信</strong></p><p>上图一样和图6相同的地方开始请求，但是这次目标 Pod（绿色标注）与源 Pod（蓝色标注）位于不同的节点上。数据包首先通过 pod1 的网络设备发送，该设备与 root netns（1）中的虚拟网络设备配对，最终数据包到达 root netns 的网桥（2）上。</p><p>这个时候网桥上的 ARP 会失败，因为与网桥相连的没有正确的数据包 MAC 地址。一旦失败，网桥会将数据包发送到默认路由上 - root netns 的 <code>eth0</code> 设备，此时就会路由离开节点，进入网络（3）。我们现在假设网络可以根据分配给节点的 <code>CIDR</code> 将数据包路由到正确的节点（4）。数据包进入目标节点的 root netns（VM2 上的 eth0），这那里它通过网桥路由到正确的虚拟设备（5）。最后，路由通过位于 pod4 的命名空间（6）中的虚拟设备 <code>eth0</code> 来完成。一般来说，每个节点都知道如何将数据包传递给其内部运行的 Pod，一旦数据包到达目标节点，数据包的流动方式与同一节点上的 Pod 间通信方式一样。</p><p>我们这里没有介绍如何配置网络来将 Pod IPs 的流量路由到负责这些 IP 的正确节点，这和特定的网络有关系，比如 AWS 就维护了一个 Kubernetes 容器网络插件，该插件允许在 AWS 的 VPC 环境中使用 [容器网络接口（<code>CNI</code>）插件]（<a href="https://github.com/aws/amazon-vpc-cni-k8s%EF%BC%89%E6%9D%A5%E8%BF%9B%E8%A1%8C%E8%8A%82%E7%82%B9%E5%88%B0%E8%8A%82%E7%82%B9%E7%9A%84%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E3%80%82">https://github.com/aws/amazon-vpc-cni-k8s）来进行节点到节点的网络通信。</a></p><p>在 EC2 中，每个实例都绑定到一个弹性网络接口 (ENI)，并且所有 ENI 都连接在一个 VPC 内 —— ENI 无需额外操作即可相互访问。默认情况下，每个 EC2 实例部署一个 ENI，但你可以创建多个 ENI 并将它们部署到 EC2 实例上。Kubernetes 的 AWS CNI 插件会为节点上的每个 Pod 创建一个新的 ENI，因为 VPC 中的 ENI 已经连接到了现有 AWS 基础设施中，这使得每个 Pod 的 IP 地址可以在 VPC 内自然寻址。当 CNI 插件被部署到集群时，每个节点（EC2 实例）都会创建多个弹性网络接口，并为这些实例分配 IP 地址，从而为每个节点形成了一个 <code>CIDR</code> 块。当部署 Pod 时，有一个小的二进制文件会作为 DaemonSet 部署到 Kubernetes 集群中，从节点本地的 <code>kubelet</code> 进程接收任何添加 Pod 到网络的请求，这个二进制文件会从节点的可用 ENI 池中挑选一个可用的 IP 地址，并通过在 Linux 内核中连接虚拟网络设备和网桥将其分配给 Pod，和在同一节点内容的 Pod 通信一样，有了这个，Pod 的流量就可以跨集群内的节点进行通信了。</p><h2 id="Pod-到-Service"><a href="#Pod-到-Service" class="headerlink" title="Pod 到 Service"></a>Pod 到 Service</h2><p>上面我们已经介绍了如何在 Pod 和它们相关的 IP 地址之间的通信。但是 Pod 的 IP 地址并不是固定不变的，会随着应用的扩缩容、应用崩溃或节点重启而出现或消失，这些都可能导致 Pod IP 地址发生变化，Kubernetes 中可以通过 <em>Service</em> 对象来解决这个问题。</p><p>Kubernetes Service 管理一组 Pod，允许你跟踪一组随时间动态变化的 Pod IP 地址，Service 作为对 Pod 的抽象，为一组 Pod 分配一个虚拟的 VIP 地址，任何发往 Service VIP 的流量都会被路由到与其关联的一组 Pod。这就允许与 Service 相关的 Pod 集可以随时变更 - 客户端只需要知道 Service VIP 即可。</p><p>创建 Service 时候，会创建一个新的虚拟 IP（也称为 clusterIP），这集群中的任何地方，发往虚拟 IP 的流量都将负载均衡到与 Service 关联的一组 Pod。实际上，Kubernetes 会自动创建并维护一个分布式集群内的负载均衡器，将流量分配到 Service 相关联的健康 Pod 上。接下来让我们仔细看看它是如何工作的。</p><h3 id="netfilter-与-iptables"><a href="#netfilter-与-iptables" class="headerlink" title="netfilter 与 iptables"></a>netfilter 与 iptables</h3><p>为了在集群中执行负载均衡，Kubernetes 会依赖于 Linux 内置的网络框架 - <code>netfilter</code>。Netfilter 是 Linux 提供的一个框架，它允许以自定义处理程序的形式实现各种与网络相关的操作，Netfilter 为数据包过滤、网络地址转换和端口转换提供了各种功能和操作，它们提供了引导数据包通过网络所需的功能，以及提供禁止数据包到达计算机网络中敏感位置的能力。</p><p><code>iptables</code> 是一个用户空间程序，它提供了一个基于 table 的系统，用于定义使用 netfilter 框架操作和转换数据包的规则。在 Kubernetes 中，iptables 规则由 kube-proxy 控制器配置，该控制器会 watch kube-apiserver 的变更，当对 Service 或 Pod 的变化更新了 Service 的虚拟 IP 地址或 Pod 的 IP 地址时，iptables 规则会被自动更新，以便正确地将指向 Service 的流量路由到支持 Pod。iptables 规则会监听发往 Service VIP 的流量，并且在匹配时，从可用 Pod 集中选择一个随机 Pod IP 地址，并且 iptables 规则将数据包的目标 IP 地址从 Service 的 VIP 更改为所选的 Pod IP。当 Pod 启动或关闭时，iptables 规则集也会更新以反映集群的变化状态。换句话说，iptables 已经在节点上做了负载均衡，以将指向 Service VIP 的流量路由到实际的 Pod 的 IP 上。</p><p>在返回路径上，IP 地址来自目标 Pod，在这种情况下，iptables 再次重写 IP 头以将 Pod IP 替换为 Service 的 IP，以便 Pod 认为它一直只与 Service 的 IP 通信。</p><h3 id="IPVS"><a href="#IPVS" class="headerlink" title="IPVS"></a>IPVS</h3><p>Kubernetes 新版本已经提供了另外一个用于集群负载均衡的选项：IPVS， IPVS 也是构建在 netfilter 之上的，并作为 Linux 内核的一部分实现了传输层的负载均衡。IPVS 被合并到了 LVS（Linux 虚拟服务器）中，它在主机上运行并充当真实服务器集群前面的负载均衡器，IPVS 可以将基于 TCP 和 UDP 的服务请求定向到真实服务器，并使真实服务器的服务作为虚拟服务出现在一个 IP 地址上。这使得 IPVS 非常适合 Kubernetes 服务。</p><p>这部署 kube-proxy 时，可以指定使用 iptables 或 IPVS 来实现集群内的负载均衡。IPVS 专为负载均衡而设计，并使用更高效的数据结构（哈希表），与 iptables  相比允许更大的规模。在使用 IPVS 模式的 Service 时，会发生三件事：在 Node 节点上创建一个虚拟 IPVS 接口，将 Service 的 VIP 地址绑定到虚拟 IPVS 接口，并为每个 Service VIP 地址创建 IPVS 服务器。</p><h3 id="Pod-到-Service-通信"><a href="#Pod-到-Service-通信" class="headerlink" title="Pod 到 Service 通信"></a>Pod 到 Service 通信</h3><p><img src="https://mmbiz.qpic.cn/mmbiz_gif/z9BgVMEm7YtugibqBH8vj7OvmDx0O2fwgO80nIibbUc6npiblqjuW8RAqlU6MhtBDUSRCwf4D1K81Wc9jdzwhnr8w/640?wx_fmt=gif&wxfrom=5&wx_lazy=1" alt="图片"></p><p>​                                                                                                                                     <strong>图8. Pod 与 Service 之间通信</strong></p><p>当这 Pod 和 Service 之间路由一个数据包时，流量和以前开始的方式一样，数据包首先通过连接到 Pod 的网络命名空间（1）的 <code>eth0</code> 离开 Pod，。然后它通过虚拟网络设备到达网桥（2）。网桥上运行的 ARP 是不知道 Service 地址的，所以它通过默认路由 <code>eth0</code>（3）将数据包传输出去。到这里会有一些不同的地方了，在 <code>eth0</code> 接收之前，该数据包会被 iptables 过滤，在收到数据包后，iptables 使用 kube-proxy 在节点上安装的规则来响应 Service 或 Pod 事件，将数据包的目的地从 Service VIP 改写为特定的 Pod IP（4）。该数据包现在就要到达 pod4 了，而不是 Service 的 VIP，iptables 利用内核的 <code>conntrack</code> 工具来记录选择的 Pod，以便将来的流量会被路由到相同的 Pod。从本质上讲，iptables 直接从节点上完成了集群内的负载均衡，然后流量流向 Pod，剩下的就和前面的 Pod 到 Pod 通信一样的了（5）。</p><h3 id="Service-到-Pod-通信"><a href="#Service-到-Pod-通信" class="headerlink" title="Service 到 Pod 通信"></a>Service 到 Pod 通信</h3><p><img src="https://mmbiz.qpic.cn/mmbiz_gif/z9BgVMEm7YtugibqBH8vj7OvmDx0O2fwgtV3KndqR2yoKUjoRlicMAwVOAnRzQn1lzibNE7ndyQpNHQ3UoeF0toiag/640?wx_fmt=gif&wxfrom=5&wx_lazy=1" alt="图片"></p><p>​                                                                                                                                              <strong>图9.在 Service 和 Pod 之间通信</strong></p><p>相应的回包的时候，收到该数据包的 Pod 将响应，将源 IP 标记为自己的 IP，将目标 IP 标记为最初发送数据包的 Pod(1)。进入节点后，数据包流经 iptables，它使用 <code>conntrack</code> 记住它之前所做的选择，并将数据包的源重写为 Service 的 VIP 而不是现在 Pod 的 IP(2)。从这里开始，数据包通过网桥流向与 Pod 的命名空间配对的虚拟网络设备 (3)，然后流向我们之前看到的 Pod 的虚拟网络设备 (4)。</p><h2 id="外网到-Service-通信"><a href="#外网到-Service-通信" class="headerlink" title="外网到 Service 通信"></a>外网到 Service 通信</h2><p>到这里我们已经了解了 Kubernetes 集群内的流量是如何路由的，但是更多的时候我们需要将服务暴露到外部去。这个时候会涉及到两个主要的问题：</p><ul><li>将流量从 Kubernetes 服务路由到互联网上去</li><li>将流量从互联网传到你的 Kubernetes 服务</li></ul><p>接下来我们就来讨论这些问题。</p><h3 id="出流量"><a href="#出流量" class="headerlink" title="出流量"></a>出流量</h3><p>从节点到公共 Internet 的路由流量也是和特定的网络有关系的，这取决于你的网络如何配置来发布流量的。这里我们以 AWS VPC 为例来进行说明。</p><p>在 AWS 中，Kubernetes 集群在 VPC 中运行，每个节点都分配有一个私有 IP 地址，该地址可从 Kubernetes 集群内访问。要从集群外部访问服务，你可以在 VPC 上附加一个外网网关。外网网关有两个用途：在你的 VPC 路由表中为可路由到外网的流量提供目标，以及为已分配公共 IP 地址的实例执行网络地址转换 (NAT)。NAT 转换负责将集群节点的内部 IP 地址更改为公网中可用的外部 IP 地址。</p><p>有了外网网关，VM 就可以自由地将流量路由到外网。不过有一个小问题，Pod 有自己的 IP 地址，与运行 Pod 的节点 IP 地址不同，并且外网网关的 NAT 转换仅适用于 VM IP 地址，因为它不知道哪些 Pod 在哪些 VM 上运行 —— 网关不支持容器。让我们看看 Kubernetes 是如何使用 iptables 来解决这个问题的。</p><p>在下图中，数据包源自 Pod 的命名空间 (1)，并经过连接到根命名空间 (2) 的 veth 对。一旦进入根命名空间，数据包就会从网桥移动到默认设备，因为数据包上的 IP 与连接到网桥的任何网段都不匹配。在到达根命名空间的网络设备 (3) 之前，iptables 会破坏数据包 (3)。在这种情况下，数据包的源 IP 地址是 Pod，如果我们将源保留为 Pod，外网网关将拒绝它，因为网关 NAT 只了解连接到 VM 的 IP 地址。解决方案是<strong>让 iptables 执行源 NAT</strong> —— 更改数据包源，使数据包看起来来自 VM 而不是 Pod。有了正确的源 IP，数据包现在可以离开 VM (4) 并到达外网网关 (5) 了。外网网关将执行另一个 NAT，将源 IP 从 VM 内部 IP 重写为公网IP。最后，数据包将到达互联网上 (6)。在返回的路上，数据包遵循相同的路径，并且任何源 IP 的修改都会被取消，这样系统的每一层都会接收到它理解的 IP 地址：节点或 VM 级别的 VM 内部，以及 Pod 内的 Pod IP命名空间。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_gif/z9BgVMEm7YtugibqBH8vj7OvmDx0O2fwgGaUDVlu2VesbE999GjqtA1WthWLBRF47ZDQ6XttQMqkjq9fc1YE3kg/640?wx_fmt=gif&wxfrom=5&wx_lazy=1" alt="图片"></p><p>​                                                                                                                                       <strong>图10.从Pod到互联网通信</strong></p><h3 id="入流量"><a href="#入流量" class="headerlink" title="入流量"></a>入流量</h3><p>让流量进入你的集群是一个非常难以解决的问题。同样这也和特定的网络环境有关系，但是一般来说入流量可以分为两种解决方案：</p><ul><li>Service LoadBalancer</li><li>Ingress 控制器</li></ul><p><strong>LoadBalancer</strong></p><p>当你创建一个 Kubernetes Service时，你可以选择指定一个 LoadBalancer 来使用它。LoadBalancer 有为你提供服务的云供应商负责创建负载均衡器，创建服务后，它将暴露负载均衡器的 IP 地址。终端用户可以直接通过该 IP 地址与你的服务进行通信。</p><p><strong>LoadBalancer 到 Service</strong></p><p>在部署了 Service 后，你使用的云提供商将会为你创建一个新的 LoadBalancer（1）。因为 LoadBalancer 不支持容器，所以一旦流量到达 LoadBalancer，它就会分布在集群的各个节点上（2）。每个节点上的 iptables 规则会将来自 LoadBalancer 的传入流量路由到正确的 Pod 上（3）。从 Pod 到客户端的响应将返回 Pod 的 IP，但客户端需要有 LoadBalancer 的 IP 地址。正如我们之前看到的，iptables 和 conntrack 被用来在返回路径上正确重写 IP 地址。</p><p>下图展示的就是托管 Pod 的三个节点前面的负载均衡器。传入流量（1）指向 Service 的 LoadBalancer，一旦 LoadBalancer 接收到数据包（2），它就会随机选择一个节点。我们这里的示例中，我们选择了没有运行 Pod 的节点 VM2（3）。在这里，运行在节点上的 iptables 规则将使用 kube-proxy 安装到集群中的内部负载均衡规则，将数据包转发到正确的 Pod。iptables 执行正确的 NAT 并将数据包转发到正确的 Pod（4）。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_gif/z9BgVMEm7YtugibqBH8vj7OvmDx0O2fwgjicHYRZia3uYzyTenTbsnsCcUaKZYt1PeIj69MYh8uNic3oziaicIZeFKmQ/640?wx_fmt=gif&wxfrom=5&wx_lazy=1" alt="图片"></p><p>​                                                                                                                      <strong>图11.外网访问 Service</strong></p><p><strong>Ingress 控制器</strong></p><p>在七层网络上 Ingress 在 HTTP&#x2F;HTTPS 协议范围内运行，并建立在 Service 之上。启用 Ingress 的第一步是使用 Kubernetes 中的 NodePort 类型的 Service，如果你将 Service 设置成 NodePort 类型，Kubernetes master 将从你指定的范围内分配一个端口，并且每个节点都会将该端口代理到你的 Service，也就是说，任何指向节点端口的流量都将使用 iptables 规则转发到 Service。</p><p>将节点的端口暴露在外网，可以使用一个 Ingress 对象，Ingress 是一个更高级别的 HTTP 负载均衡器，它将 HTTP 请求映射到 Kubernetes Service。根据控制器的实现方式，Ingress 的使用方式会有所不同。HTTP 负载均衡器，和四层网络负载均衡器一样，只了解节点 IP（而不是 Pod IP），因此流量路由同样利用由 kube-proxy 安装在每个节点上的 iptables 规则提供的内部负载均衡。</p><p>在 AWS 环境中，ALB Ingress 控制器使用 AWS 的七层应用程序负载均衡器提供 Kubernetes 入口。下图详细介绍了此控制器创建的 AWS 组件，它还演示了 Ingress 流量从 ALB 到 Kubernetes 集群的路由。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtugibqBH8vj7OvmDx0O2fwgdr38KkXPhd9AKtGnrYhn2SxGDKy0fbjFWIrfrgzXaXCzjicEIjrWzRg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>​                                                                                                                   <strong>图12.Ingress 控制器</strong></p><p>创建后，(1) Ingress Controller 会 watch 来自 Kubernetes APIServer 的 Ingress 事件。当它找到满足其要求的 Ingress 资源时，它会开始创建 AWS 资源。AWS 将 Application Load Balancer (ALB) (2) 用于 Ingress 资源。负载均衡器与用于将请求路由到一个或多个注册节点的 TargetGroup一起工作。(3) 在 AWS 中为 Ingress 资源描述的每个唯一 Kubernetes Service 创建 TargetGroup。(4) Listener 是一个 ALB 进程，它使用你配置的协议和端口检查连接请求。Listener 由 Ingress 控制器为你的 Ingress 资源中描述的每个端口创建。最后，为 Ingress 资源中指定的每个路径创建 TargetGroup 规则。这可以保证到特定路径的流量被路由到正确的 Kubernetes 服务上 (5)。</p><p><strong>Ingress 到 Service</strong></p><p>流经 Ingress 的数据包的生命周期与 LoadBalancer 的生命周期非常相似。主要区别在于 Ingress 知道 URL 的路径（可以根据路径将流量路由到 Service）Ingress 和节点之间的初始连接是通过节点上为每个服务暴露的端口。</p><p>部署 Service 后，你使用的云提供商将为你创建一个新的 Ingress 负载均衡器 (1)。因为负载均衡器不支持容器，一旦流量到达负载均衡器，它就会通过为你的服务端口分布在组成集群 (2) 的整个节点中。每个节点上的 iptables 规则会将来自负载均衡器的传入流量路由到正确的 Pod (3)。Pod 到客户端的响应将返回 Pod 的 IP，但客户端需要有负载均衡器的 IP 地址。正如我们之前看到的，iptables 和 conntrack 用于在返回路径上正确重写 IP。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_gif/z9BgVMEm7YtugibqBH8vj7OvmDx0O2fwgB6f7ZsmsiamnMF10mxPp1NvlmMw5sHGfqAQ0MKnkYTxlMKpjkI6Gctg/640?wx_fmt=gif&wxfrom=5&wx_lazy=1" alt="图片"></p><p>​                                                                                                                                               <strong>图13.从 Ingress 到 Service</strong></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了 Kubernetes 网络模型以及如何实现常见网络任务。网络知识点既广泛又很深，所以我们这里不可能涵盖所有的内容，但是你可以以本文为起点，然后去深入了解你感兴趣的主题。</p><blockquote><p>原文链接：<a href="https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model">https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
      <tag>转载</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深信服内推</title>
    <link href="/2022/08/27/%E6%B7%B1%E4%BF%A1%E6%9C%8D%E5%86%85%E6%8E%A8/"/>
    <url>/2022/08/27/%E6%B7%B1%E4%BF%A1%E6%9C%8D%E5%86%85%E6%8E%A8/</url>
    
    <content type="html"><![CDATA[<h3 id="帮忙社招、校招、实习内推，可帮忙查询最新进展、面试结果-简历优先处理、可以帮忙看简历以及回答力所能及的问题，欢迎大家来投，想要帮忙看简历或者问问题可以私聊加微信或者发送邮箱，由于内推岗位选择数量有限制，存在部分岗位没有显示，可以私聊我直接投递或者上官网公众号投递。"><a href="#帮忙社招、校招、实习内推，可帮忙查询最新进展、面试结果-简历优先处理、可以帮忙看简历以及回答力所能及的问题，欢迎大家来投，想要帮忙看简历或者问问题可以私聊加微信或者发送邮箱，由于内推岗位选择数量有限制，存在部分岗位没有显示，可以私聊我直接投递或者上官网公众号投递。" class="headerlink" title="帮忙社招、校招、实习内推，可帮忙查询最新进展、面试结果 - 简历优先处理、可以帮忙看简历以及回答力所能及的问题，欢迎大家来投，想要帮忙看简历或者问问题可以私聊加微信或者发送邮箱，由于内推岗位选择数量有限制，存在部分岗位没有显示，可以私聊我直接投递或者上官网公众号投递。"></a>帮忙社招、校招、实习内推，可帮忙查询最新进展、面试结果 - 简历优先处理、可以帮忙看简历以及回答力所能及的问题，欢迎大家来投，想要帮忙看简历或者问问题可以私聊加微信或者发送邮箱，由于内推岗位选择数量有限制，存在部分岗位没有显示，可以私聊我直接投递或者上官网公众号投递。</h3><h3 id="招聘网址-https-app-mokahr-com-recommendation-apply-sangfor-5369-recommendCode-NTAEMbl-jobs-isCampusJob-1-amp-commitment-E5-85-A8-E8-81-8C"><a href="#招聘网址-https-app-mokahr-com-recommendation-apply-sangfor-5369-recommendCode-NTAEMbl-jobs-isCampusJob-1-amp-commitment-E5-85-A8-E8-81-8C" class="headerlink" title="招聘网址:   https://app.mokahr.com/recommendation-apply/sangfor/5369?recommendCode=NTAEMbl#/jobs?isCampusJob=1&amp;commitment=%E5%85%A8%E8%81%8C"></a>招聘网址:   <a href="https://app.mokahr.com/recommendation-apply/sangfor/5369?recommendCode=NTAEMbl#/jobs?isCampusJob=1&amp;commitment=%E5%85%A8%E8%81%8C">https://app.mokahr.com/recommendation-apply/sangfor/5369?recommendCode=NTAEMbl#/jobs?isCampusJob=1&amp;commitment=%E5%85%A8%E8%81%8C</a></h3><h3 id="内推码：NTAEMbl-投递时直接官网投递时填内推码即可，后续进展等问题可直接私信我！"><a href="#内推码：NTAEMbl-投递时直接官网投递时填内推码即可，后续进展等问题可直接私信我！" class="headerlink" title="内推码：NTAEMbl   投递时直接官网投递时填内推码即可，后续进展等问题可直接私信我！"></a>内推码：NTAEMbl   投递时直接官网投递时填内推码即可，后续进展等问题可直接私信我！</h3><p><img src="https://s2.loli.net/2022/08/27/aR4mNMEGDWwsHfT.jpg" alt="a3a7bad9b480bd23d69abd81723f366.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>内推</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
