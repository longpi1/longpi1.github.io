

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme="auto">



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/%E5%AD%A6%E6%A0%A1.png">
  <link rel="icon" href="/img/%E5%AD%A6%E6%A0%A1.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="lp">
  <meta name="keywords" content>
  
    <meta name="description" content="极客时间徐文浩-AI大模型之美课程笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="11.通过LangChain的链式调用简化多步提示语">
<meta property="og:url" content="https://blog.longpi1.com/2023/11/22/11-%E9%80%9A%E8%BF%87LangChain%E7%9A%84%E9%93%BE%E5%BC%8F%E8%B0%83%E7%94%A8%E7%AE%80%E5%8C%96%E5%A4%9A%E6%AD%A5%E6%8F%90%E7%A4%BA%E8%AF%AD/index.html">
<meta property="og:site_name" content="lp&#39;s blog">
<meta property="og:description" content="极客时间徐文浩-AI大模型之美课程笔记">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2023/11/21/ICihLrmyvRdWjse.png">
<meta property="og:image" content="https://s2.loli.net/2023/11/21/7kEJGKvgRibFjnt.png">
<meta property="og:image" content="https://s2.loli.net/2023/11/21/EALYg3RBJasQPzN.png">
<meta property="article:published_time" content="2023-11-22T03:11:47.000Z">
<meta property="article:modified_time" content="2023-11-22T03:12:15.599Z">
<meta property="article:author" content="lp">
<meta property="article:tag" content="原创">
<meta property="article:tag" content="笔记">
<meta property="article:tag" content="大模型">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://s2.loli.net/2023/11/21/ICihLrmyvRdWjse.png">
  
  
  
  <title>11.通过LangChain的链式调用简化多步提示语 - lp&#39;s blog</title>

  <link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css">



  <link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css">

  <link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css">

  <link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css">



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link rel="stylesheet" href="/css/main.css">


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css">
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css">
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"blog.longpi1.com","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":30,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"UvaCwj6C0pVj9XCWuMtLaBWJ-gzGzoHsz","app_key":"w2xUk9wycItSqrREmRMDYJHY","server_url":"https://uvacwj6c.lc-cn-n1-shared.com","path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script src="/js/utils.js"></script>
  <script src="/js/color-schema.js"></script>
  

  

  

  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>lp&#39;s blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                文章分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于我
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax="true" style="background: url('/img/bg.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="11.通过LangChain的链式调用简化多步提示语"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-11-22 11:11" pubdate>
          2023年11月22日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          14k 字
        
      </span>
    

  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>


    <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/longpi1"><img loading="lazy" width="149" height="149" src="https://github.blog/wp-content/uploads/2008/12/forkme_left_darkblue_121621.png?resize=149%2C149" srcset="/img/loading.gif" lazyload class="attachment-full size-full" alt="follow me on GitHub" data-recalc-dims="1"></a>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <aside class="sidebar" style="padding-left: 2rem; margin-right: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">11.通过LangChain的链式调用简化多步提示语</h1>
            
            
              <div class="markdown-body">
              <meta name="referrer" content="no-referrer">
                
                <h1 id="11-通过LangChain的链式调用简化多步提示语"><a href="#11-通过LangChain的链式调用简化多步提示语" class="headerlink" title="11.通过LangChain的链式调用简化多步提示语"></a>11.通过LangChain的链式调用简化多步提示语</h1><blockquote>
<p>大部分内容来自于极客时间<a target="_blank" rel="external nofollow noopener noreferrer" href="https://time.geekbang.org/column/intro/100541001">徐文浩-AI大模型之美</a></p>
</blockquote>
<h2 id="关于Langchain"><a href="#关于Langchain" class="headerlink" title="关于Langchain"></a>关于Langchain</h2><p>OpenAI的大语言模型，只是提供了简简单单的Completion和Embedding这样两个核心接口。但是通过合理使用这两个接口，我们完成了各种各样复杂的任务。</p>
<ul>
<li>通过提示语（Prompt）里包含历史的聊天记录，我们能够让AI根据上下文正确地回答问题。</li>
<li>通过将Embedding提前索引好存起来，我们能够让AI根据外部知识回答问题。</li>
<li>而通过多轮对话，将AI返回的答案放在新的问题里，我们能够让AI帮我们给自己的代码撰写单元测试。</li>
</ul>
<p>这些方法，也是一个实用的自然语言类应用里常见的模式。我之前也都通过代码为你演示过具体的做法。但是，如果我们每次写应用的时候，都需要自己再去OpenAI提供的原始API里做一遍，那就太麻烦了。于是，开源社区就有人将这些常见的需求和模式抽象了出来，开发了一个叫做Langchain的开源库。那么接下来，我们就来看看如何使用LangChain来快速实现之前我们利用大语言模型实现过的功能。以及我们如何进一步地，将Langchain和我们的业务系统整合，完成更复杂、更有实用价值的功能。</p>
<h2 id="使用Langchain的链式调用"><a href="#使用Langchain的链式调用" class="headerlink" title="使用Langchain的链式调用"></a>使用Langchain的链式调用</h2><p>我们先来看一个使用ChatGPT的例子，你就能理解为什么会有链式调用的需求了。我们知道，GPT-3的基础模型里面，中文的语料很少。用中文问它问题，很多时候它回答得不好。所以有时候，我会迂回处理一下，先把中文问题给AI，请它翻译成英文，然后再把英文问题贴进去提问，得到一个英文答案。最后，再请AI把英文答案翻译回中文。很多时候，问题的答案会更准确一点。比如，下面的截图里，我就请它简单介绍一下Stable Diffusion的原理是什么。</p>
<p>注：Stable Diffusion是一个热门的开源AI画图工具，后面我们在介绍用AI生成图片的时候会用到。</p>
<h3 id="人工链式调用"><a href="#人工链式调用" class="headerlink" title="人工链式调用"></a>人工链式调用</h3><p><img src="https://s2.loli.net/2023/11/21/ICihLrmyvRdWjse.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p><img src="https://s2.loli.net/2023/11/21/7kEJGKvgRibFjnt.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p>如果用API来实现这个过程，其实就是一个链式调用的过程。</p>
<ol>
<li>我们先调用OpenAI，把翻译请求和原始问题组合在一起发送给AI，完成问题的中译英。</li>
<li>然后再把拿到的翻译好的英文问题发送给OpenAI，得到英文答案。</li>
<li>最后再把英文答案，和对应要求AI翻译答案的请求组合在一起，完成答案的英译中。</li>
</ol>
<h3 id="使用LLMChain进行链式调用"><a href="#使用LLMChain进行链式调用" class="headerlink" title="使用LLMChain进行链式调用"></a>使用LLMChain进行链式调用</h3><p>如果我们用代码，可以像下面这样，一步步进行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> openai, os<br><span class="hljs-keyword">from</span> langchain.prompts <span class="hljs-keyword">import</span> PromptTemplate<br><span class="hljs-keyword">from</span> langchain.llms <span class="hljs-keyword">import</span> OpenAI<br><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> LLMChain<br><br>openai.api_key = os.environ.get(<span class="hljs-string">&quot;OPENAI_API_KEY&quot;</span>)<br><br>llm = OpenAI(model_name=<span class="hljs-string">&quot;text-davinci-003&quot;</span>, max_tokens=<span class="hljs-number">2048</span>, temperature=<span class="hljs-number">0.5</span>)<br><br>en_to_zh_prompt = PromptTemplate(<br>    template=<span class="hljs-string">&quot;请把下面这句话翻译成英文： \n\n &#123;question&#125;?&quot;</span>, input_variables=[<span class="hljs-string">&quot;question&quot;</span>]<br>)<br><br>question_prompt = PromptTemplate(<br>    template = <span class="hljs-string">&quot;&#123;english_question&#125;&quot;</span>, input_variables=[<span class="hljs-string">&quot;english_question&quot;</span>]<br>)<br><br>zh_to_cn_prompt = PromptTemplate(<br>    input_variables=[<span class="hljs-string">&quot;english_answer&quot;</span>],<br>    template=<span class="hljs-string">&quot;请把下面这一段翻译成中文： \n\n&#123;english_answer&#125;?&quot;</span>,<br>)<br><br>question_translate_chain = LLMChain(llm=llm, prompt=en_to_zh_prompt, output_key=<span class="hljs-string">&quot;english_question&quot;</span>)<br>english = question_translate_chain.run(question=<span class="hljs-string">&quot;请你作为一个机器学习的专家，介绍一下CNN的原理。&quot;</span>)<br><span class="hljs-built_in">print</span>(english)<br><br>qa_chain = LLMChain(llm=llm, prompt=question_prompt, output_key=<span class="hljs-string">&quot;english_answer&quot;</span>)<br>english_answer = qa_chain.run(english_question=english)<br><span class="hljs-built_in">print</span>(english_answer)<br><br>answer_translate_chain = LLMChain(llm=llm, prompt=zh_to_cn_prompt)<br>answer = answer_translate_chain.run(english_answer=english_answer)<br><span class="hljs-built_in">print</span>(answer)<br><br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs plain">Please explain the principles of CNN as an expert in Machine Learning.<br><br>A Convolutional Neural Network (CNN) is a type of deep learning algorithm that is used to analyze visual imagery. It is modeled after the structure of the human visual cortex and is composed of multiple layers of neurons that process and extract features from an image. The main principle behind a CNN is that it uses convolutional layers to detect patterns in an image. Each convolutional layer is comprised of a set of filters that detect specific features in an image. These filters are then used to extract features from the image and create a feature map. The feature map is then passed through a pooling layer which reduces the size of the feature map and helps to identify the most important features in the image. Finally, the feature map is passed through a fully-connected layer which classifies the image and outputs the result.<br><br>卷积神经网络（CNN）是一种深度学习算法，用于分析视觉图像。它模仿人类视觉皮层的结构，由多层神经元组成，可以处理和提取图像中的特征。CNN的主要原理是使用卷积层来检测图像中的模式。每个卷积层由一组滤波器组成，可以检测图像中的特定特征。然后使用这些滤波器从图像中提取特征，并创建特征图。然后，将特征图通过池化层传递，该层可以减小特征图的大小，并有助于识别图像中最重要的特征。最后，将特征图传递给完全连接的层，该层将对图像进行分类，并输出结果。<br><br></code></pre></td></tr></table></figure>

<p>这里的代码，<strong>我们使用了Langchain这个库，不过还没有动用它的链式调用过程。</strong>我们主要用了Langchain的三个包。</p>
<ol>
<li>LLM，也就是我们使用哪个大语言模型，来回答我们提出的问题。在这里，我们还是使用OpenAIChat，也就是最新放出来的 gpt-3.5-turbo 模型。</li>
<li>PromptTemplate，它可以定义一个提示语模版，里面能够定义一些可以动态替换的变量。比如，代码里的question_prompt这个模版里，我们就定义了一个叫做question的变量，因为我们每次问的问题都会不一样。事实上，llamd-index里面的PromptTemplate就是对Langchain的PromptTemplate做了一层简单的封装。</li>
<li>主角 LLMChain，它的构造函数接收一个LLM和一个PromptTemplate作为参数。构造完成之后，可以直接调用里面的run方法，将PromptTemplate需要的变量，用K&#x3D;&gt;V对的形式传入进去。返回的结果，就是LLM给我们的答案。</li>
</ol>
<p>不过如果看上面这段代码，我们似乎只是对OpenAI的API做了一层封装而已。我们构建了3个LLMChain，然后按照顺序调用，每次拿到答案之后，再作为输入，交给下一个LLM调用。感觉好像更麻烦了，没有减少什么工作量呀？</p>
<p>别着急，这是因为我们还没有真正用上LLMChain的“链式调用”功能，而用这个功能，只需要加上一行小小的代码。我们用一个叫做<strong>SimpleSequentialChain</strong>的LLMChain类，把我们要按照顺序依次调用的三个LLMChain放在一个数组里，传给这个类的构造函数。</p>
<p>然后对于这个对象，我们调用run方法，把我们用中文问的问题交给它。这个时候，这个SimpleSequentialChain，就会按照顺序开始调用chains这个数组参数里面包含的其他LLMChain。并且，每一次调用的结果，会存储在这个Chain构造时定义的output_key参数里。而下一个调用的LLMChain，里面模版内的变量如果有和之前的output_key名字相同的，就会用output_key里存入的内容替换掉模版内变量所在的占位符。</p>
<p>这次，我们只向这个SimpleSequentialChain调用一次run方法，把一开始的问题交给它就好了。后面根据答案去问新的问题，这个LLMChain会自动地链式搞定。我在这里把日志的Verbose模式打开了，你在输出的过程中，可以看到其实这个LLMChain是调用了三次，并且中间两次的返回结果你也可以一并看到。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> SimpleSequentialChain<br><br>chinese_qa_chain = SimpleSequentialChain(<br>    chains=[question_translate_chain, qa_chain, answer_translate_chain], input_key=<span class="hljs-string">&quot;question&quot;</span>,<br>    verbose=<span class="hljs-literal">True</span>)<br>answer = chinese_qa_chain.run(question=<span class="hljs-string">&quot;请你作为一个机器学习的专家，介绍一下CNN的原理。&quot;</span>)<br><span class="hljs-built_in">print</span>(answer)<br><br></code></pre></td></tr></table></figure>

<p>Verbose日志信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs plain">&gt; Entering new SimpleSequentialChain chain...<br><br>Please introduce the principle of CNN as a machine learning expert.<br><br>Convolutional Neural Networks (CNNs) are a type of artificial neural network that are commonly used in image recognition and classification tasks. They are inspired by the structure of the human brain and are composed of multiple layers of neurons connected in a specific pattern. The neurons in the first layer of a CNN are connected to the input image, and the neurons in the last layer are connected to the output. The neurons in between the input and output layers are called feature maps and are responsible for extracting features from the input image. CNNs use convolutional layers to detect patterns in the input image and pooling layers to reduce the size of the feature maps. This allows the CNN to learn the most important features in the image and use them to make predictions.<br><br>卷积神经网络（CNN）是一种常用于图像识别和分类任务的人工神经网络。它们受到人脑结构的启发，由多层神经元以特定模式连接而成。CNN的第一层神经元与输入图像连接，最后一层神经元与输出连接。输入和输出层之间的神经元称为特征映射，负责从输入图像中提取特征。CNN使用卷积层检测输入图像中的模式，使用池化层减小特征映射的大小。这使得CNN能够学习图像中最重要的特征，并利用它们进行预测。<br>&gt; Finished chain.<br><br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">卷积神经网络（CNN）是一种常用于图像识别和分类任务的人工神经网络。它们受到人脑结构的启发，由多层神经元以特定模式连接而成。CNN的第一层神经元与输入图像连接，最后一层神经元与输出连接。输入和输出层之间的神经元称为特征映射，负责从输入图像中提取特征。CNN使用卷积层检测输入图像中的模式，使用池化层减小特征映射的大小。这使得CNN能够学习图像中最重要的特征，并利用它们进行预测。<br><br></code></pre></td></tr></table></figure>

<p>在使用这样的链式调用的时候，有一点需要注意，就是一个LLMChain里，所使用的PromptTemplate里的输入参数， <strong>之前必须在LLMChain里，通过 output_key 定义过。</strong> 不然，这个变量没有值，程序就会报错。</p>
<h3 id="支持多个变量输入的链式调用"><a href="#支持多个变量输入的链式调用" class="headerlink" title="支持多个变量输入的链式调用"></a>支持多个变量输入的链式调用</h3><p>事实上，因为使用变量的输入输出，是用这些参数定义的。所以我们不是只能用前一个LLMChain的输出作为后一个LLMChain的输入。我们完全可以连续问多个问题，然后把这些问题的答案，作为后续问题的输入来继续处理。下面我就给你看一个例子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> SequentialChain<br><br>q1_prompt = PromptTemplate(<br>    input_variables=[<span class="hljs-string">&quot;year1&quot;</span>],<br>    template=<span class="hljs-string">&quot;&#123;year1&#125;年的欧冠联赛的冠军是哪支球队，只说球队名称。&quot;</span><br>)<br>q2_prompt = PromptTemplate(<br>    input_variables=[<span class="hljs-string">&quot;year2&quot;</span>],<br>    template=<span class="hljs-string">&quot;&#123;year2&#125;年的欧冠联赛的冠军是哪支球队，只说球队名称。&quot;</span><br>)<br>q3_prompt = PromptTemplate(<br>    input_variables=[<span class="hljs-string">&quot;team1&quot;</span>, <span class="hljs-string">&quot;team2&quot;</span>],<br>    template=<span class="hljs-string">&quot;&#123;team1&#125;和&#123;team2&#125;哪只球队获得欧冠的次数多一些？&quot;</span><br>)<br>chain1 = LLMChain(llm=llm, prompt=q1_prompt, output_key=<span class="hljs-string">&quot;team1&quot;</span>)<br>chain2 = LLMChain(llm=llm, prompt=q2_prompt, output_key=<span class="hljs-string">&quot;team2&quot;</span>)<br>chain3 = LLMChain(llm=llm, prompt=q3_prompt)<br><br>sequential_chain = SequentialChain(chains=[chain1, chain2, chain3], input_variables=[<span class="hljs-string">&quot;year1&quot;</span>, <span class="hljs-string">&quot;year2&quot;</span>], verbose=<span class="hljs-literal">True</span>)<br>answer = sequential_chain.run(year1=<span class="hljs-number">2000</span>, year2=<span class="hljs-number">2010</span>)<br><span class="hljs-built_in">print</span>(answer)<br><br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plain">&gt; Entering new SequentialChain chain...<br>&gt; Finished chain.<br><br>西班牙皇家马德里队获得欧冠的次数更多，共13次，而拜仁慕尼黑只有5次。<br><br></code></pre></td></tr></table></figure>

<p>在这个例子里，我们定义了两个PromptTemplate和对应的LLMChain，各自接收一个年份作为输入，回答这两个年份的欧冠冠军。然后将两个队名作为输入，放到第三个问题里，让AI告诉我们这两支球队哪一支获得欧冠的次数多一些。只需要在我们的SequentialChain里输入两个年份，就能通过三次回答得到答案。</p>
<h2 id="通过Langchain实现自动化撰写单元测试"><a href="#通过Langchain实现自动化撰写单元测试" class="headerlink" title="通过Langchain实现自动化撰写单元测试"></a>通过Langchain实现自动化撰写单元测试</h2><p>Langchain可以顺序地通过多个Prompt调用OpenAI的GPT模型。这个能力拿来实现上一讲的自动化测试的功能是再合适不过的了。下面，我就拿Langchain重新实现了一遍上一讲的这个功能，并且给它补上了AST语法解析失败之后自动重试的能力。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> SequentialChain<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">write_unit_test</span>(<span class="hljs-params">function_to_test, unit_test_package = <span class="hljs-string">&quot;pytest&quot;</span></span>):<br>    <span class="hljs-comment"># 解释源代码的步骤</span><br>    explain_code = <span class="hljs-string">&quot;&quot;&quot;&quot;# How to write great unit tests with &#123;unit_test_package&#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string">    In this advanced tutorial for experts, we&#x27;ll use Python 3.10 and `&#123;unit_test_package&#125;` to write a suite of unit tests to verify the behavior of the following function.</span><br><span class="hljs-string">    ```python</span><br><span class="hljs-string">    &#123;function_to_test&#125;</span><br><span class="hljs-string">    ```</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Before writing any unit tests, let&#x27;s review what each element of the function is doing exactly and what the author&#x27;s intentions may have been.</span><br><span class="hljs-string">    - First,&quot;&quot;&quot;</span><br><br>    explain_code_template = PromptTemplate(<br>        input_variables=[<span class="hljs-string">&quot;unit_test_package&quot;</span>, <span class="hljs-string">&quot;function_to_test&quot;</span>],<br>        template=explain_code<br>    )<br>    explain_code_llm = OpenAI(model_name=<span class="hljs-string">&quot;text-davinci-002&quot;</span>, temperature=<span class="hljs-number">0.4</span>, max_tokens=<span class="hljs-number">1000</span>,<br>            top_p=<span class="hljs-number">1</span>, stop=[<span class="hljs-string">&quot;\n\n&quot;</span>, <span class="hljs-string">&quot;\n\t\n&quot;</span>, <span class="hljs-string">&quot;\n    \n&quot;</span>])<br>    explain_code_step = LLMChain(llm=explain_code_llm, prompt=explain_code_template, output_key=<span class="hljs-string">&quot;code_explaination&quot;</span>)<br><br>    <span class="hljs-comment"># 创建测试计划示例的步骤</span><br>    test_plan = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string"></span><br><span class="hljs-string">    A good unit test suite should aim to:</span><br><span class="hljs-string">    - Test the function&#x27;s behavior for a wide range of possible inputs</span><br><span class="hljs-string">    - Test edge cases that the author may not have foreseen</span><br><span class="hljs-string">    - Take advantage of the features of `&#123;unit_test_package&#125;` to make the tests easy to write and maintain</span><br><span class="hljs-string">    - Be easy to read and understand, with clean code and descriptive names</span><br><span class="hljs-string">    - Be deterministic, so that the tests always pass or fail in the same way</span><br><span class="hljs-string"></span><br><span class="hljs-string">    `&#123;unit_test_package&#125;` has many convenient features that make it easy to write and maintain unit tests. We&#x27;ll use them to write unit tests for the function above.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    For this particular function, we&#x27;ll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):</span><br><span class="hljs-string">    -&quot;&quot;&quot;</span><br>    test_plan_template = PromptTemplate(<br>        input_variables=[<span class="hljs-string">&quot;unit_test_package&quot;</span>, <span class="hljs-string">&quot;function_to_test&quot;</span>, <span class="hljs-string">&quot;code_explaination&quot;</span>],<br>        template= explain_code + <span class="hljs-string">&quot;&#123;code_explaination&#125;&quot;</span> + test_plan<br>    )<br>    test_plan_llm = OpenAI(model_name=<span class="hljs-string">&quot;text-davinci-002&quot;</span>, temperature=<span class="hljs-number">0.4</span>, max_tokens=<span class="hljs-number">1000</span>,<br>            top_p=<span class="hljs-number">1</span>, stop=[<span class="hljs-string">&quot;\n\n&quot;</span>, <span class="hljs-string">&quot;\n\t\n&quot;</span>, <span class="hljs-string">&quot;\n    \n&quot;</span>])<br>    test_plan_step = LLMChain(llm=test_plan_llm, prompt=test_plan_template, output_key=<span class="hljs-string">&quot;test_plan&quot;</span>)<br><br>    <span class="hljs-comment"># 撰写测试代码的步骤</span><br>    starter_comment = <span class="hljs-string">&quot;Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator&quot;</span><br>    prompt_to_generate_the_unit_test = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string"></span><br><span class="hljs-string">Before going into the individual tests, let&#x27;s first look at the complete suite of unit tests as a cohesive whole. We&#x27;ve added helpful comments to explain what each line does.</span><br><span class="hljs-string">```python</span><br><span class="hljs-string">import &#123;unit_test_package&#125;  # used for our unit tests</span><br><span class="hljs-string"></span><br><span class="hljs-string">&#123;function_to_test&#125;</span><br><span class="hljs-string"></span><br><span class="hljs-string">#&#123;starter_comment&#125;&quot;&quot;&quot;</span><br><br>    unit_test_template = PromptTemplate(<br>        input_variables=[<span class="hljs-string">&quot;unit_test_package&quot;</span>, <span class="hljs-string">&quot;function_to_test&quot;</span>, <span class="hljs-string">&quot;code_explaination&quot;</span>, <span class="hljs-string">&quot;test_plan&quot;</span>, <span class="hljs-string">&quot;starter_comment&quot;</span>],<br>        template= explain_code + <span class="hljs-string">&quot;&#123;code_explaination&#125;&quot;</span> + test_plan + <span class="hljs-string">&quot;&#123;test_plan&#125;&quot;</span> + prompt_to_generate_the_unit_test<br>    )<br>    unit_test_llm = OpenAI(model_name=<span class="hljs-string">&quot;text-davinci-002&quot;</span>, temperature=<span class="hljs-number">0.4</span>, max_tokens=<span class="hljs-number">1000</span>, stop=<span class="hljs-string">&quot;```&quot;</span>)<br>    unit_test_step = LLMChain(llm=unit_test_llm, prompt=unit_test_template, output_key=<span class="hljs-string">&quot;unit_test&quot;</span>)<br><br>    sequential_chain = SequentialChain(chains=[explain_code_step, test_plan_step, unit_test_step],<br>                                    input_variables=[<span class="hljs-string">&quot;unit_test_package&quot;</span>, <span class="hljs-string">&quot;function_to_test&quot;</span>, <span class="hljs-string">&quot;starter_comment&quot;</span>], verbose=<span class="hljs-literal">True</span>)<br>    answer = sequential_chain.run(unit_test_package=unit_test_package, function_to_test=function_to_test, starter_comment=starter_comment)<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">f&quot;&quot;&quot;#<span class="hljs-subst">&#123;starter_comment&#125;</span>&quot;&quot;&quot;</span> + answer<br><br>code = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">def format_time(seconds):</span><br><span class="hljs-string">    minutes, seconds = divmod(seconds, 60)</span><br><span class="hljs-string">    hours, minutes = divmod(minutes, 60)</span><br><span class="hljs-string">    if hours &gt; 0:</span><br><span class="hljs-string">        return f&quot;&#123;hours&#125;h&#123;minutes&#125;min&#123;seconds&#125;s&quot;</span><br><span class="hljs-string">    elif minutes &gt; 0:</span><br><span class="hljs-string">        return f&quot;&#123;minutes&#125;min&#123;seconds&#125;s&quot;</span><br><span class="hljs-string">    else:</span><br><span class="hljs-string">        return f&quot;&#123;seconds&#125;s&quot;</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> ast<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">write_unit_test_automatically</span>(<span class="hljs-params">code, retry=<span class="hljs-number">3</span></span>):<br>    unit_test_code = write_unit_test(code)<br>    all_code = code + unit_test_code<br>    tried = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">while</span> tried &lt; retry:<br>        <span class="hljs-keyword">try</span>:<br>            ast.parse(all_code)<br>            <span class="hljs-keyword">return</span> all_code<br>        <span class="hljs-keyword">except</span> SyntaxError <span class="hljs-keyword">as</span> e:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Syntax error in generated code: <span class="hljs-subst">&#123;e&#125;</span>&quot;</span>)<br>            all_code = code + write_unit_test(code)<br>            tried += <span class="hljs-number">1</span><br><br><span class="hljs-built_in">print</span>(write_unit_test_automatically(code))<br><br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">format_time</span>(<span class="hljs-params">seconds</span>):<br>    minutes, seconds = <span class="hljs-built_in">divmod</span>(seconds, <span class="hljs-number">60</span>)<br>    hours, minutes = <span class="hljs-built_in">divmod</span>(minutes, <span class="hljs-number">60</span>)<br>    <span class="hljs-keyword">if</span> hours &gt; <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;hours&#125;</span>h<span class="hljs-subst">&#123;minutes&#125;</span>min<span class="hljs-subst">&#123;seconds&#125;</span>s&quot;</span><br>    <span class="hljs-keyword">elif</span> minutes &gt; <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;minutes&#125;</span>min<span class="hljs-subst">&#123;seconds&#125;</span>s&quot;</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;seconds&#125;</span>s&quot;</span><br><span class="hljs-comment">#Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator.</span><br><span class="hljs-comment">#The first element of the tuple is the name of the test case, and the second element is a list of tuples,</span><br><span class="hljs-comment">#where each tuple contains the input values for the format_time() function and the expected output.</span><br><span class="hljs-meta">@pytest.mark.parametrize(<span class="hljs-params"><span class="hljs-string">&quot;test_case, input_values, expected_output&quot;</span>, [</span></span><br><span class="hljs-params"><span class="hljs-meta">    <span class="hljs-comment"># Test cases for when the seconds parameter is an integer</span></span></span><br><span class="hljs-params"><span class="hljs-meta">    (<span class="hljs-params"><span class="hljs-string">&quot;seconds is positive&quot;</span>, (<span class="hljs-params"><span class="hljs-number">42</span>,</span>), <span class="hljs-string">&quot;42s&quot;</span></span>),</span></span><br><span class="hljs-params"><span class="hljs-meta">    (<span class="hljs-params"><span class="hljs-string">&quot;seconds is negative&quot;</span>, (<span class="hljs-params">-<span class="hljs-number">42</span>,</span>), <span class="hljs-string">&quot;-42s&quot;</span></span>),</span></span><br><span class="hljs-params"><span class="hljs-meta">    (<span class="hljs-params"><span class="hljs-string">&quot;seconds is 0&quot;</span>, (<span class="hljs-params"><span class="hljs-number">0</span>,</span>), <span class="hljs-string">&quot;0s&quot;</span></span>),</span></span><br><span class="hljs-params"><span class="hljs-meta">    <span class="hljs-comment"># Test cases for when the seconds parameter is not an integer</span></span></span><br><span class="hljs-params"><span class="hljs-meta">    (<span class="hljs-params"><span class="hljs-string">&quot;seconds is a float&quot;</span>, (<span class="hljs-params"><span class="hljs-number">42.0</span>,</span>), <span class="hljs-string">&quot;42.0s&quot;</span></span>),</span></span><br><span class="hljs-params"><span class="hljs-meta">    (<span class="hljs-params"><span class="hljs-string">&quot;seconds is a string&quot;</span>, (<span class="hljs-params"><span class="hljs-string">&quot;42&quot;</span>,</span>), <span class="hljs-string">&quot;42s&quot;</span></span>),</span></span><br><span class="hljs-params"><span class="hljs-meta">    (<span class="hljs-params"><span class="hljs-string">&quot;seconds is None&quot;</span>, (<span class="hljs-params"><span class="hljs-literal">None</span>,</span>), <span class="hljs-string">&quot;None&quot;</span></span>),</span></span><br><span class="hljs-params"><span class="hljs-meta">    <span class="hljs-comment"># Test cases for when the seconds parameter is an integer, but it is not in the range 0-3600</span></span></span><br><span class="hljs-params"><span class="hljs-meta">    (<span class="hljs-params"><span class="hljs-string">&quot;seconds is too small&quot;</span>, (<span class="hljs-params">-<span class="hljs-number">1</span>,</span>), <span class="hljs-string">&quot;-1s&quot;</span></span>),</span></span><br><span class="hljs-params"><span class="hljs-meta">    (<span class="hljs-params"><span class="hljs-string">&quot;seconds is too large&quot;</span>, (<span class="hljs-params"><span class="hljs-number">3601</span>,</span>), <span class="hljs-string">&quot;1h0min1s&quot;</span></span>),</span></span><br><span class="hljs-params"><span class="hljs-meta">]</span>)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">test_format_time</span>(<span class="hljs-params">test_case, input_values, expected_output</span>):<br>    <span class="hljs-comment"># We use the pytest.raises context manager to assert that the function raises a TypeError</span><br>    <span class="hljs-comment"># if the input is not an integer.</span><br>    <span class="hljs-keyword">with</span> pytest.raises(TypeError):<br>        format_time(input_values)<br>    <span class="hljs-comment"># We use the pytest.approx context manager to assert that the output is approximately equal</span><br>    <span class="hljs-comment"># to the expected output, within a certain tolerance.</span><br>    <span class="hljs-keyword">assert</span> format_time(input_values) == pytest.approx(expected_output)<br><br></code></pre></td></tr></table></figure>

<p>这个代码的具体功能，其实和上一讲是一模一样的，只是通过Langchain做了封装，使它更加容易维护了。我们把解释代码、生成测试计划，以及最终生成测试代码，变成了三个LLMChain。每一步的输入，都来自上一步的输出。这个输入既包括上一步的Prompt Template和这一步的Prompt Template的组合，也包括过程中的一些变量，这些变量是上一步执行的结果作为输入变量传递进来的。最终，我们可以使用SequentialChain来自动地按照这三个步骤，执行OpenAI的API调用。</p>
<p>这整个过程通过write_unit_test这个函数给封装起来了。对于重试，我们则是通过一个while循环来调用 write_unit_test。拿到的结果和输入的代码拼装在一起，交给AST库做解析。如果解析通不过，则重试整个单元测试生成的过程，直到达到我们最大的重试次数为止。</p>
<p>LangChain的这个分多个步骤调用OpenAI模型的能力，能够帮助我们通过AI完成复杂的任务，并且将整个任务的完成过程定义成了一个固定的流程模版。在下一讲里，我们还会进一步看到，通过这样一个链式组合多个LLMChain的方法，如何完成更复杂并且更具有现实意义的工作。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>好了，相信到这里，你脑子里应该有了更多可以利用大语言模型的好点子。这一讲，我带你学会了如何通过Langchain这个开源库，对大语言模型进行链式调用。想要通过大语言模型，完成一个复杂的任务，往往需要我们多次向AI提问，并且前面提问的答案，可能是后面问题输入的一部分。LangChain通过将多个LLMChain组合成一个SequantialChain并顺序执行，大大简化了这类任务的开发工作。</p>
<p><img src="https://s2.loli.net/2023/11/21/EALYg3RBJasQPzN.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p>Langchain还有很多更强大的功能，我们不仅能调用语言模型，还能调用外部系统，甚至我们还能直接让AI做决策，决定该让我们的系统做什么。在后面的几讲里，我们会覆盖这些内容，并最终给你一个完整的电商聊天机器人。</p>
<h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><p>和之前介绍过的llama-index这个项目一样，Langchain这个项目也在快速地发展和迭代过程中。推荐去看一看他们的 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://langchain.readthedocs.io/en/latest/">官方文档</a>，好知道他们提供的最新功能。此外，之前提到过的向量数据库公司Pinecone，也制作了一份 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.pinecone.io/learn/langchain/">Langchain AI Handbook</a>，也可以去看一看。</p>

                
              </div>
            
            <hr>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" class="category-chain-item">大模型</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E5%8E%9F%E5%88%9B/">#原创</a>
      
        <a href="/tags/%E7%AC%94%E8%AE%B0/">#笔记</a>
      
        <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">#大模型</a>
      
        <a href="/tags/AI/">#AI</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>11.通过LangChain的链式调用简化多步提示语</div>
      <div>https://blog.longpi1.com/2023/11/22/11-通过LangChain的链式调用简化多步提示语/</div>
    </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/11/22/HuggingFace%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/" title="HuggingFace介绍与使用">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">HuggingFace介绍与使用</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/11/22/10.%20%E9%80%9A%E8%BF%87%E8%B0%83%E7%94%A8GPT%20API%E5%B8%AE%E4%BD%A0%E5%AE%9E%E7%8E%B0%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%E7%9A%84%E7%94%9F%E6%88%90/" title="10. 通过调用GPT API帮你实现单元测试的生成">
                        <span class="hidden-mobile">10. 通过调用GPT API帮你实现单元测试的生成</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </article></div>

            
  <article id="comments" lazyload>
    
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.4.17/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"UvaCwj6C0pVj9XCWuMtLaBWJ-gzGzoHsz","appKey":"w2xUk9wycItSqrREmRMDYJHY","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


  </article>


          
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>






  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <div>lp的个人博客 | 记录成长的过程</div> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    

    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script>
  <link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css">

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script>
<script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script>
<script src="/js/events.js"></script>
<script src="/js/plugins.js"></script>


  <script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script src="/js/img-lazyload.js"></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src="https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js"></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script defer src="/js/leancloud.js"></script>

  <script src="/js/local-search.js"></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script src="/js/boot.js"></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
