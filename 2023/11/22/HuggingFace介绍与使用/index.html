

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme="auto">



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/%E5%AD%A6%E6%A0%A1.png">
  <link rel="icon" href="/img/%E5%AD%A6%E6%A0%A1.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="lp">
  <meta name="keywords" content>
  
    <meta name="description" content="极客时间徐文浩-AI大模型之美课程笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="HuggingFace介绍与使用">
<meta property="og:url" content="https://blog.longpi1.com/2023/11/22/HuggingFace%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/index.html">
<meta property="og:site_name" content="lp&#39;s blog">
<meta property="og:description" content="极客时间徐文浩-AI大模型之美课程笔记">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2023/11/22/vKlpb1kLUGjHOt6.png">
<meta property="og:image" content="https://s2.loli.net/2023/11/22/XmF3qy4sihNfUOC.png">
<meta property="og:image" content="https://s2.loli.net/2023/11/22/XZ16pKyWVtgTYeP.png">
<meta property="og:image" content="https://s2.loli.net/2023/11/22/DQjaStcBkgmAIyx.png">
<meta property="og:image" content="https://s2.loli.net/2023/11/22/5LZGWm1it2aRV6l.png">
<meta property="og:image" content="https://s2.loli.net/2023/11/22/W3pf8zkDaLXSTeZ.png">
<meta property="og:image" content="https://s2.loli.net/2023/11/22/WD3hn1OHBu7GT9P.png">
<meta property="og:image" content="https://s2.loli.net/2023/11/22/6cEQGW9Bavd7weI.png">
<meta property="og:image" content="https://s2.loli.net/2023/11/22/KLfPJtvWh8XpMlc.png">
<meta property="og:image" content="https://s2.loli.net/2023/11/22/dgzo3eTxXcKaLHU.png">
<meta property="article:published_time" content="2023-11-22T12:42:38.000Z">
<meta property="article:modified_time" content="2023-11-22T12:48:58.894Z">
<meta property="article:author" content="lp">
<meta property="article:tag" content="原创">
<meta property="article:tag" content="笔记">
<meta property="article:tag" content="大模型">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://s2.loli.net/2023/11/22/vKlpb1kLUGjHOt6.png">
  
  
  
  <title>HuggingFace介绍与使用 - lp&#39;s blog</title>

  <link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css">



  <link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css">

  <link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css">

  <link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css">



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link rel="stylesheet" href="/css/main.css">


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css">
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css">
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"blog.longpi1.com","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":30,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"UvaCwj6C0pVj9XCWuMtLaBWJ-gzGzoHsz","app_key":"w2xUk9wycItSqrREmRMDYJHY","server_url":"https://uvacwj6c.lc-cn-n1-shared.com","path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script src="/js/utils.js"></script>
  <script src="/js/color-schema.js"></script>
  

  

  

  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>lp&#39;s blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                文章分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于我
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax="true" style="background: url('/img/bg.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="HuggingFace介绍与使用"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-11-22 20:42" pubdate>
          2023年11月22日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          10k 字
        
      </span>
    

  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>


    <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/longpi1"><img loading="lazy" width="149" height="149" src="https://github.blog/wp-content/uploads/2008/12/forkme_left_darkblue_121621.png?resize=149%2C149" srcset="/img/loading.gif" lazyload class="attachment-full size-full" alt="follow me on GitHub" data-recalc-dims="1"></a>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <aside class="sidebar" style="padding-left: 2rem; margin-right: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">HuggingFace介绍与使用</h1>
            
            
              <div class="markdown-body">
              <meta name="referrer" content="no-referrer">
                
                <h1 id="HuggingFace介绍与使用"><a href="#HuggingFace介绍与使用" class="headerlink" title="HuggingFace介绍与使用"></a>HuggingFace介绍与使用</h1><blockquote>
<p>大部分内容来自于极客时间<a target="_blank" rel="external nofollow noopener noreferrer" href="https://time.geekbang.org/column/intro/100541001">徐文浩-AI大模型之美</a></p>
</blockquote>
<p>在这一系列文章中，我们介绍了AI的各种能力。而且相对于大语言模型，语音识别和语音合成都有完全可以用于商业应用的开源模型。事实上，Huggingface的火爆离不开他们开源的这个Transformers库。这个开源库里有数万个我们可以直接调用的模型。很多场景下，这个开源模型已经足够我们使用了。</p>
<p>不过，在使用这些开源模型的过程中，你会发现大部分模型都需要一块不错的显卡。而如果回到我们更早使用过的开源大语言模型，就更是这样了。</p>
<p>在课程里面，我们是通过用Colab免费的GPU资源来搞定的。但是如果我们想要投入生产环境使用，免费的Colab就远远不够用了。而且，Colab的GPU资源对于大语言模型来说还是太小了。我们在前面不得不使用小尺寸的T5-base和裁剪过的ChatGLM-6B-INT4，而不是FLAN-UL2或者ChatGLM-130B这样真正的大模型。</p>
<p>那么，这一讲我们就来看看，Transformers可以给我们提供哪些模型，以及如何在云端使用真正的大模型。而想要解决这两个问题啊，都少不了要使用HuggingFace这个目前最大的开源模型社区。</p>
<h2 id="Transformers-Pipeline"><a href="#Transformers-Pipeline" class="headerlink" title="Transformers Pipeline"></a>Transformers Pipeline</h2><h3 id="Pipeline的基本功能"><a href="#Pipeline的基本功能" class="headerlink" title="Pipeline的基本功能"></a>Pipeline的基本功能</h3><p>我们先来看看，Transformers这个开源库到底能干些什么。下面的代码都是直接使用开源模型，需要利用GPU的算力，所以你最好还是在Colab里运行，注意不要忘记把Runtime的类型修改为GPU。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br><br>classifier = pipeline(task=<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, device=<span class="hljs-number">0</span>)<br>preds = classifier(<span class="hljs-string">&quot;I am really happy today!&quot;</span>)<br><span class="hljs-built_in">print</span>(preds)<br><br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plain">No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).<br>Using a pipeline without specifying a model name and revision in production is not recommended.<br>[&#123;&#x27;label&#x27;: &#x27;POSITIVE&#x27;, &#x27;score&#x27;: 0.9998762607574463&#125;]<br><br></code></pre></td></tr></table></figure>

<p>这个代码非常简单，第一行代码，我们定义了一个task是sentimental-analysis的Pipeline，也就是一个情感分析的分类器。里面device&#x3D;0的意思是我们指定让Transformer使用GPU资源。如果你想要让它使用CPU，你可以设置device&#x3D;-1。然后，调用这个分类器对一段文本进行情感分析。从输出结果看，它给出了正确的Positive预测，也给出了具体的预测分数。因为我们在这里没有指定任何模型，所以Transformers自动选择了默认的模型，也就是日志里看到的 distilbert-base-uncased-finetuned-sst-2-english 这个模型。</p>
<p>看名字我们可以知道，这个模型是一个针对英语的模型。如果想要支持中文，我们也可以换一个模型来试试。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">classifier = pipeline(model=<span class="hljs-string">&quot;uer/roberta-base-finetuned-jd-binary-chinese&quot;</span>, task=<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, device=<span class="hljs-number">0</span>)<br>preds = classifier(<span class="hljs-string">&quot;这个餐馆太难吃了。&quot;</span>)<br><span class="hljs-built_in">print</span>(preds)<br><br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">[&#123;<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;negative (stars 1, 2 and 3)&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.934112012386322</span>&#125;]<br><br></code></pre></td></tr></table></figure>

<p>这里，我们指定模型的名称，就能换用另一个模型来进行情感分析了。这次我们选用的是roberta-base-finetuned-jd-binary-chinese 这个模型。RoBERTa这个模型是基于BERT做了一些设计上的修改而得来的。而后面的finetuned-jd-binary-chinese 是基于京东的数据进行微调过的一个模型。</p>
<p>Pipeline是Transformers库里面的一个核心功能，它封装了所有托管在HuggingFace上的模型推理预测的入口。你不需要关心具体每个模型的架构、输入数据格式是什么样子的。我们只要通过model参数指定使用的模型，通过task参数来指定任务类型，运行一下就能直接获得结果。</p>
<p>比如，我们现在不想做情感分析了，而是想要做英译中，我们只需要把task换成translation_en_to_zh，然后选用一个合适的模型就好了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">translation = pipeline(task=<span class="hljs-string">&quot;translation_en_to_zh&quot;</span>, model=<span class="hljs-string">&quot;Helsinki-NLP/opus-mt-en-zh&quot;</span>, device=<span class="hljs-number">0</span>)<br><br>text = <span class="hljs-string">&quot;I like to learn data science and AI.&quot;</span><br>translated_text = translation(text)<br><span class="hljs-built_in">print</span>(translated_text)<br><br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">[&#123;<span class="hljs-string">&#x27;translation_text&#x27;</span>: <span class="hljs-string">&#x27;我喜欢学习数据科学和人工智能&#x27;</span>&#125;]<br><br></code></pre></td></tr></table></figure>

<p>在这里，我们选用了赫尔辛基大学的opus-mt-en-zh这个模型来做英译中，运行一下就可以看到，我们输入的英文被翻译成了中文。不过，我们怎么知道应该选用哪个模型呢？这个如魔法一般的Helsinki-NLP&#x2F;opus-mt-en-zh模型名字从哪里可以找到呢？</p>
<h3 id="如何寻找自己需要的模型？"><a href="#如何寻找自己需要的模型？" class="headerlink" title="如何寻找自己需要的模型？"></a>如何寻找自己需要的模型？</h3><p>这个时候，我们就需要去HuggingFace的网站里找一找了。你点击网站的 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://huggingface.co/models">Models 板块</a>，就可以看到一个界面，左侧是一系列的筛选器，而右侧则是筛选出来的模型。比如刚才的英译中的模型，我们就是先在左侧的筛选器里，选中Task下的Translation这种任务类型。然后再在Languages里面选择Chinese，就能找到所有能够翻译中文的模型。默认的模型排序是按照用户下载数量从高到低排序的。一般来说，下载的数量越多，往往也意味着大家觉得这个模型可能更加靠谱。</p>
<p><img src="https://s2.loli.net/2023/11/22/vKlpb1kLUGjHOt6.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p><img src="https://s2.loli.net/2023/11/22/XmF3qy4sihNfUOC.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p>我们点击Helsinki-NLP&#x2F;opus-mt-en-zh进入这个模型的卡片页，就能看到更详细的介绍。并且很多模型，都在右侧提供了对应的示例。不使用代码，你也可以直接体验一下模型的能力和效果。</p>
<p><img src="https://s2.loli.net/2023/11/22/XZ16pKyWVtgTYeP.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<h3 id="Pipeline支持的自然语言处理任务"><a href="#Pipeline支持的自然语言处理任务" class="headerlink" title="Pipeline支持的自然语言处理任务"></a>Pipeline支持的自然语言处理任务</h3><p>Transformers的Pipeline模块，支持的task是非常丰富的。可以说大部分常见的自然语言处理任务都被囊括在内了，经常会用到的有这么几个。</p>
<ul>
<li>feature-extraction，其实它和OpenAI的Embedding差不多，也就是把文本变成一段向量。</li>
<li>fill-mask，也就是完形填空。你可以把一句话中的一部分遮盖掉，然后让模型预测遮盖掉的地方的词是什么。</li>
<li>ner，命名实体识别。我们常常用它来提取文本里面的时间、地点、人名、邮箱、电话号码、地址等信息，然后进一步用这些信息来处理其他任务。</li>
<li>question-answering和table-question-answering，专门针对问题进行自动问答，在客服的FAQ领域常常会用到这类任务。</li>
<li>sentiment-analysis和text-classification，也就是我们之前见过的情感分析，以及类目更自由的文本分类问题。</li>
<li>text-generation 和 text2text-generation，文本生成类型的任务。我们之前让AI写代码或者写故事，其实都是这一类的任务。</li>
</ul>
<p>剩下的还有 summarization文本摘要、translation机器翻译，以及zero-shot-classification，也就是我们课程一开始介绍的零样本分类。</p>
<p>看到这里，你有没有发现ChatGPT的强大之处？上面这些自然语言处理任务，常常需要切换使用不同的专有模型。但是 <strong>在ChatGPT里，我们只需要一个通用的模型，就能直接解决所有的问题</strong>。这也是很多人惊呼“通用人工智能”来了的原因。</p>
<h3 id="通过Pipeline进行语音识别"><a href="#通过Pipeline进行语音识别" class="headerlink" title="通过Pipeline进行语音识别"></a>通过Pipeline进行语音识别</h3><p>Pipeline不仅支持自然语言处理相关的任务，它还支持语音和视觉类的任务。比如，我们同样可以通过Pipeline使用OpenAI的Whisper模型来做语音识别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br><br>transcriber = pipeline(model=<span class="hljs-string">&quot;openai/whisper-medium&quot;</span>, device=<span class="hljs-number">0</span>)<br>result = transcriber(<span class="hljs-string">&quot;./data/podcast_clip.mp3&quot;</span>)<br><span class="hljs-built_in">print</span>(result)<br><br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">&#123;<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&quot; Welcome to OnBoard, a real first-line experience, a new investment thinking. I&#x27;m Monica. I&#x27;m Gao Ning. Let&#x27;s talk about how software changes the world.&quot;</span>&#125;<br><br></code></pre></td></tr></table></figure>

<p>不过，这里你会发现一个小小的问题。我们原本中文的内容，在通过Pipeline调用Whisper模型之后，输出就变成了英文。这个是因为Pipeline对整个数据处理进行了封装。在实际调用Whisper模型的时候，它会在最终生成文本的过程里面，加入一个&lt;|en|&gt;，导致文本生成的时候强行被指定成了英文。我们可以修改一下这个decoder生成文本时的设置，让输出的内容变成中文。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> WhisperProcessor, WhisperForConditionalGeneration<br>processor = WhisperProcessor.from_pretrained(<span class="hljs-string">&quot;openai/whisper-medium&quot;</span>)<br>forced_decoder_ids = processor.get_decoder_prompt_ids(language=<span class="hljs-string">&quot;zh&quot;</span>, task=<span class="hljs-string">&quot;transcribe&quot;</span>)<br><br>transcriber = pipeline(model=<span class="hljs-string">&quot;openai/whisper-medium&quot;</span>, device=<span class="hljs-number">0</span>,<br>                       generate_kwargs=&#123;<span class="hljs-string">&quot;forced_decoder_ids&quot;</span>: forced_decoder_ids&#125;)<br>result = transcriber(<span class="hljs-string">&quot;./data/podcast_clip.mp3&quot;</span>)<br><span class="hljs-built_in">print</span>(result)<br><br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">&#123;<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;欢迎来到Onboard真实的一线经验走新的投资思考我是Monica我是高宁我们一起聊聊软件如何改变世界&#x27;</span>&#125;<br><br></code></pre></td></tr></table></figure>

<p>不过，即使转录成了中文，也会有一些小小的问题。你会看到转录后的内容没有标点符号。目前，Transformers库的Pipeline还没有比较简单的方法给转录的内容加上Prompt。这也是Pipeline的抽象封装带来的一个缺点。如果你有兴趣，也可以看看是否可以为Transformers库贡献代码，让它能够为Whisper模型支持Prompt的功能。</p>
<h2 id="如何使用Inference-API？"><a href="#如何使用Inference-API？" class="headerlink" title="如何使用Inference API？"></a>如何使用Inference API？</h2><p>如果你实际运行了上面我们使用的Pipeline代码，你就会发现其实大量的时间，都被浪费在下载模型的过程里了。而且，因为Colab的内存和显存大小的限制，我们还没办法运行尺寸太大的模型。比如，flan-t5-xxl这样大尺寸的模型有110亿参数，Colab和一般的游戏显卡根本放不下。</p>
<p>但是这些模型的效果往往又比单机能够加载的小模型要好很多。那么这个时候，如果你想测试体验一下效果，就可以试试Inference API。它是HuggingFace免费提供的，让你可以通过API调用的方式先试用这些模型。</p>
<h3 id="尝试Inference-API"><a href="#尝试Inference-API" class="headerlink" title="尝试Inference API"></a>尝试Inference API</h3><p>首先，和其他的API Key一样，我们还是通过环境变量来设置一下Huggingface的Access Token。你可以在Huggingface的 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://huggingface.co/settings/tokens">个人设置</a> 里面拿到这个Key，然后通过export设置到环境变量里就好了。</p>
<p><img src="https://s2.loli.net/2023/11/22/DQjaStcBkgmAIyx.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p>设置环境变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">export HUGGINGFACE_API_KEY=YOUR_HUGGINGFACE_ACCESS_TOKEN<br><br></code></pre></td></tr></table></figure>

<p>然后，我们就可以通过简单的HTTP请求，调用托管在Huggingace里的模型了。比如，我们可以通过下面的代码，直接用flan-t5-xxl这个模型来进行问答。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os, requests, json<br><br>API_TOKEN = os.environ.get(<span class="hljs-string">&quot;HUGGINGFACE_API_KEY&quot;</span>)<br><br>model = <span class="hljs-string">&quot;google/flan-t5-xxl&quot;</span><br>API_URL = <span class="hljs-string">f&quot;https://api-inference.huggingface.co/models/<span class="hljs-subst">&#123;model&#125;</span>&quot;</span><br>headers = &#123;<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">&#123;API_TOKEN&#125;</span>&quot;</span>, <span class="hljs-string">&quot;Content-Type&quot;</span>: <span class="hljs-string">&quot;application/json&quot;</span>&#125;<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload, api_url=API_URL, headers=headers</span>):<br>    data = json.dumps(payload)<br>    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, api_url, headers=headers, data=data)<br>    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))<br><br>question = <span class="hljs-string">&quot;Please answer the following question. What is the capital of France?&quot;</span><br>data = query(&#123;<span class="hljs-string">&quot;inputs&quot;</span> : question&#125;)<br><br><span class="hljs-built_in">print</span>(data)<br><br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">[&#123;<span class="hljs-string">&#x27;generated_text&#x27;</span>: <span class="hljs-string">&#x27;paris&#x27;</span>&#125;]<br><br></code></pre></td></tr></table></figure>

<p>上面的演示代码也很简单，需要做到三点。</p>
<ol>
<li>我们向HuggingFace的api-inference这个域名发起一个请求，在对应的路径里跟上模型的名字。</li>
<li>在请求头里，带上我们拿到的ACCESS TOKEN，来通过权限的校验。</li>
<li>通过一个以inputs为key的JSON，作为请求体发送过去就好了。</li>
</ol>
<p>运行这个例子你就可以看到，flan-t5-xxl这样的模型也有一定的知识和问答能力。在这个例子里，它就准确地回答出了法国的首都是巴黎。</p>
<h3 id="等待模型加载完毕"><a href="#等待模型加载完毕" class="headerlink" title="等待模型加载完毕"></a>等待模型加载完毕</h3><p>同样的，Inference API也支持各种各样的任务。我们在模型页的卡片里，如果能够看到一个带着闪电标记⚡️的Hosted Inference API字样，就代表着这个模型可以通过Inference API调用。并且下面可以让你测试的示例，就是这个Inference API支持的任务。</p>
<p><img src="https://s2.loli.net/2023/11/22/5LZGWm1it2aRV6l.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p>比如上面截图里的 hfl&#x2F;chinese-pert-base 模型支持的就是 feature-extraction 的任务，它能够让你把自己的文本变成向量。我们不妨来试一试。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">model = <span class="hljs-string">&quot;hfl/chinese-pert-base&quot;</span><br>API_URL = <span class="hljs-string">f&quot;https://api-inference.huggingface.co/models/<span class="hljs-subst">&#123;model&#125;</span>&quot;</span><br><br>question = <span class="hljs-string">&quot;今天天气真不错！&quot;</span><br>data = query(&#123;<span class="hljs-string">&quot;inputs&quot;</span> : question&#125;, api_url=API_URL)<br><br><span class="hljs-built_in">print</span>(data)<br><br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">&#123;<span class="hljs-string">&#x27;error&#x27;</span>: <span class="hljs-string">&#x27;Model hfl/chinese-pert-base is currently loading&#x27;</span>, <span class="hljs-string">&#x27;estimated_time&#x27;</span>: <span class="hljs-number">20.0</span>&#125;<br><br></code></pre></td></tr></table></figure>

<p>第一次尝试去调用这个Inference API，我们得到了一个报错信息。这个消息说的是，模型还在加载，并且预计还需要20秒才会加载完。因为Inference API是Huggingface免费提供给大家的，它也没有足够的GPU资源把所有模型（约几万个）都随时加载到线上。所以实际上，很多模型在没有人调用的时候，就会把GPU资源释放出来。只有当我们调用的时候，它才会加载模型，运行对应的推理逻辑。</p>
<p>我们有两个选择，一个是等待一会儿，等模型加载完了再调用。或者，我们可以在调用的时候就直接加上一个参数 <strong>wait_for_model&#x3D;True</strong>。这个参数，会让服务端等待模型加载完成之后，再把结果返回给我们，而不是立刻返回一个模型正在加载的报错信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">data = query(&#123;<span class="hljs-string">&quot;inputs&quot;</span> : question, <span class="hljs-string">&quot;wait_for_model&quot;</span> : <span class="hljs-literal">True</span>&#125;, api_url=API_URL)<br><br><span class="hljs-built_in">print</span>(data)<br><br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">[[[-<span class="hljs-number">0.05410267040133476</span>, -<span class="hljs-number">0.0140887051820755</span>, <span class="hljs-number">0.017411280423402786</span>, <span class="hljs-number">0.10337194055318832</span>……<br><br></code></pre></td></tr></table></figure>

<p>我们在Pipeline里介绍的任务，基本都可以通过Inference API的方式来调用。如果你想深入了解每一个任务的API的参数，可以去看一下HuggingFace的 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://huggingface.co/docs/api-inference/detailed_parameters">官方文档</a>。</p>
<h2 id="如何部署自己的大模型"><a href="#如何部署自己的大模型" class="headerlink" title="如何部署自己的大模型?"></a>如何部署自己的大模型?</h2><p>不过，Inference API只能给你提供试用各个模型的接口。因为是免费的资源，<strong>自然不能无限使用，所以HuggingFace为它设置了限额（Rate Limit）</strong>。如果你觉得大模型真的好用，那么最好的办法，就是在云平台上找一些有GPU的机器，把自己需要的模型部署起来。</p>
<p><strong>HuggingFace自己就提供了一个非常简便的部署开源模型的产品，叫做Inference Endpoint。</strong>你不需要自己去云平台申请服务器，搭建各种环境。只需要选择想要部署的模型、使用的服务器资源，一键就能把自己需要的模型部署到云平台上。</p>
<h3 id="把模型部署到Endpoint上"><a href="#把模型部署到Endpoint上" class="headerlink" title="把模型部署到Endpoint上"></a>把模型部署到Endpoint上</h3><p>其实GPT2的论文里，已经体现了大语言模型不少潜力了。那么，下面我们就试着来部署一下GPT2这个模型。</p>
<ol>
<li>首先，进入 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://ui.endpoints.huggingface.co/new">创建 Endpoint 的界面</a>，你可以选择自己想要部署的模型，我们这里选择了GPT2这个模型。</li>
<li>Endpoint Name，你可以自己设置一个，也可以直接使用系统自动生成的。</li>
<li>系统默认会为你选择云服务商、对应的区域，以及需要的硬件资源。如果你选择的硬件资源不足以部署这个模型，页面上也会有对应的提示告诉你。GPT2的模型连GPU也不需要，有CPU就能运行起来。</li>
<li>最后你需要选择一下这个Endpoint的安全等级，一共有三种，分别是 Protected、Public和Private。</li>
</ol>
<ul>
<li>Public是指这个模型部署好之后，互联网上的任何人都能调用，不需要做任何权限验证。一般情况下，你不太会选择这一个安全等级。</li>
<li>Protected，需要HuggingFace的Access Token的验证。我们在这里就选用这个方式，这也是测试使用最常用的方式。</li>
<li>Private，不仅需要权限验证，还需要通过一个AWS或者Azure云里面的私有网络才能访问。如果你实际部署一个应用在线上，对应API访问都是通过自己在云上的服务器进行的，那么选择这个方式是最合理的。</li>
</ul>
<p><img src="https://s2.loli.net/2023/11/22/W3pf8zkDaLXSTeZ.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p>设置好了之后，你再点击最下面的 <strong>Create Endpoint</strong>，HuggingFace就会开始帮你创建机器资源，部署对应的模型了。</p>
<p><img src="https://s2.loli.net/2023/11/22/WD3hn1OHBu7GT9P.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p>我们只要等待几分钟，模型就能部署起来。当然，这是因为GPT2的模型比较小。如果你尝试部署一些大尺寸的模型，可能需要1-2个小时才能完成。因为HuggingFace要完成模型下载、Docker镜像打包等一系列的工作，单个模型又很大，所以需要更长时间。</p>
<h3 id="测试体验一下大模型"><a href="#测试体验一下大模型" class="headerlink" title="测试体验一下大模型"></a>测试体验一下大模型</h3><p>部署完成之后，我们会自动进入对应的Endpoint详情页里。上面的Endpoint URL就表示你可以像调用Inference API一样调用模型的API_URL。而下面，也给出了一个测试输入框，这个测试输入框我们在HuggingFace模型卡片页面里也能够看到。</p>
<p><img src="https://s2.loli.net/2023/11/22/6cEQGW9Bavd7weI.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p>我们可以用这样一段简单的代码来测试一下GPT2模型对应的效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">API_URL = <span class="hljs-string">&quot;https://abmlvcliaa98k9ct.us-east-1.aws.endpoints.huggingface.cloud&quot;</span><br><br>text = <span class="hljs-string">&quot;My name is Lewis and I like to&quot;</span><br>data = query(&#123;<span class="hljs-string">&quot;inputs&quot;</span> : text&#125;, api_url=API_URL)<br><br><span class="hljs-built_in">print</span>(data)<br><br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">[&#123;<span class="hljs-string">&#x27;generated_text&#x27;</span>: <span class="hljs-string">&#x27;My name is Lewis and I like to think I\&#x27;m a dog. It would mean my soul to you.&quot; The boy quickly gave up and returned to his studies, then began taking classes on the basics and the basics of English, then he\&#x27;d write&#x27;</span>&#125;]<br><br></code></pre></td></tr></table></figure>

<p>有了这样一个部署在线上的模型，你就可以完全根据自己的需求随时调用API来完成自己的任务了，唯一的限制就是你使用的硬件资源有多少。</p>
<h3 id="暂停、恢复以及删除Endpoint"><a href="#暂停、恢复以及删除Endpoint" class="headerlink" title="暂停、恢复以及删除Endpoint"></a>暂停、恢复以及删除Endpoint</h3><p>部署在Endpoint上的模型是按照在线的时长收费的。如果你暂时不用这个模型，可以选择 <strong>暂停</strong>（Pause）这个Endpoint。等到想使用的时候，再重新 <strong>恢复</strong>（Resume）这个Endpoint就好了。暂停期间的模型不会计费，这个功能的选项就在模型Overview标签页的右上角。</p>
<p><img src="https://s2.loli.net/2023/11/22/KLfPJtvWh8XpMlc.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p>如果你彻底不需要使用这个模型了，你可以把对应的Endpoint删掉，你只需要在对应Endpoint的Setting页面里输入Endpoint的名称，然后选择删除就好了。</p>
<p><img src="https://s2.loli.net/2023/11/22/dgzo3eTxXcKaLHU.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<p>HuggingFace将部署一个开源模型到线上的成本基本降低到了0。不过，目前它只支持海外的AWS、Azure以及Google Cloud，并不支持阿里云或者腾讯云，对国内的用户算是一个小小的遗憾。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天，解了如何利用HuggingFace以及开源模型，来实现各类大模型应用的推理任务。最简单的方式，是使用Transformers这个Python开源库里面的Pipeline模块，只需要指定Pipeline里的model和task，然后直接调用它们来处理我们给到的数据，就能拿到结果。我们不需要关心模型背后的结构、分词器，以及数据的处理方式，也能快速上手使用这些开源模型。Pipeline的任务，涵盖了常见的自然语言处理任务，同时也包括了音频和图像的功能。</p>
<p>而如果模型比较大，单个的GPU不足以加载这个模型，你可以尝试通过HuggingFace免费提供的Inference API来试用模型。只需要一个简单的HTTP请求，你就可以直接测试像 flan-t5-xxl 这样110亿参数的大模型。而如果你想要把这样的大模型应用到你的生产环境里，你就可以通过Inference Endpoint这个功能来把大模型部署到云端。当然，这需要花不少钱。</p>
<p>在了解了Pipeline、Inference API以及Inference Endpoint之后，相信你已经充分掌握利用Huggingface来完成各种常见的文本、音频任务的方法了。后面需要的就是多多实践。</p>
<h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><p>HuggingFace的 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://huggingface.co/docs/transformers/main_classes/pipelines">官方文档</a> 里，给出了通过Pipeline完成各种任务的详细示例。你可以对照着自己的需求看一下这个文档，相信能解决你90%以上的问题。</p>

                
              </div>
            
            <hr>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" class="category-chain-item">大模型</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E5%8E%9F%E5%88%9B/">#原创</a>
      
        <a href="/tags/%E7%AC%94%E8%AE%B0/">#笔记</a>
      
        <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">#大模型</a>
      
        <a href="/tags/AI/">#AI</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>HuggingFace介绍与使用</div>
      <div>https://blog.longpi1.com/2023/11/22/HuggingFace介绍与使用/</div>
    </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/11/23/12.%E6%B7%B1%E5%85%A5%E4%BD%BF%E7%94%A8LLMChain%EF%BC%8C%E8%BF%9E%E6%8E%A5%E5%A4%96%E9%83%A8%E5%B7%A5%E5%85%B7Google%E7%AD%89/" title="12.深入使用LLMChain，连接外部工具Google等">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">12.深入使用LLMChain，连接外部工具Google等</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/11/22/11-%E9%80%9A%E8%BF%87LangChain%E7%9A%84%E9%93%BE%E5%BC%8F%E8%B0%83%E7%94%A8%E7%AE%80%E5%8C%96%E5%A4%9A%E6%AD%A5%E6%8F%90%E7%A4%BA%E8%AF%AD/" title="11.通过LangChain的链式调用简化多步提示语">
                        <span class="hidden-mobile">11.通过LangChain的链式调用简化多步提示语</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </article></div>

            
  <article id="comments" lazyload>
    
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.4.17/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"UvaCwj6C0pVj9XCWuMtLaBWJ-gzGzoHsz","appKey":"w2xUk9wycItSqrREmRMDYJHY","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


  </article>


          
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>






  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <div>lp的个人博客 | 记录成长的过程</div> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    

    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script>
  <link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css">

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script>
<script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script>
<script src="/js/events.js"></script>
<script src="/js/plugins.js"></script>


  <script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script src="/js/img-lazyload.js"></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src="https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js"></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script defer src="/js/leancloud.js"></script>

  <script src="/js/local-search.js"></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script src="/js/boot.js"></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
